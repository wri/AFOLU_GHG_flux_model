{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30181827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# dask/parallelization libraries\n",
    "import coiled\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.distributed import print as dask_print\n",
    "import dask.config\n",
    "import distributed\n",
    "\n",
    "# scipy basics\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import rasterio.transform\n",
    "import rasterio.windows\n",
    "\n",
    "from numba import jit\n",
    "import concurrent.futures\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "import math\n",
    "import ctypes\n",
    "import pandas as pd\n",
    "\n",
    "%run utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe44e76",
   "metadata": {},
   "source": [
    "<font size=\"6\">Making cloud and local clusters</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4591aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cluster\n",
    "coiled_cluster = coiled.Cluster(\n",
    "    n_workers=20,\n",
    "    use_best_zone=True, \n",
    "    compute_purchase_option=\"spot_with_fallback\",\n",
    "    idle_timeout=\"10 minutes\",\n",
    "    region=\"us-east-1\",\n",
    "    name=\"AFOLU_flux_model\", \n",
    "    account='jterry64', # Necessary to use the AWS environment that Justin set up in Coiled\n",
    "    worker_memory = \"32GiB\" \n",
    ")\n",
    "\n",
    "# Coiled cluster (cloud run)\n",
    "coiled_client = coiled_cluster.get_client()\n",
    "coiled_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c6fda-132e-401a-a83f-97036cac559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cluster\n",
    "coiled_cluster = coiled.Cluster(\n",
    "    n_workers=1,\n",
    "    use_best_zone=True, \n",
    "    compute_purchase_option=\"spot_with_fallback\",\n",
    "    idle_timeout=\"10 minutes\",\n",
    "    region=\"us-east-1\",\n",
    "    name=\"AFOLU_flux_model\", \n",
    "    account='jterry64', # Necessary to use the AWS environment that Justin set up in Coiled\n",
    "    worker_memory = \"64GiB\" \n",
    ")\n",
    "\n",
    "# Coiled cluster (cloud run)\n",
    "coiled_client = coiled_cluster.get_client()\n",
    "coiled_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d54a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local single-process cluster (local run). Will run .compute() on just one process, not a whole cluster.\n",
    "local_client = Client(processes=False)\n",
    "local_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53da8ca-0c69-415f-a247-1dab6e7140f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_client = Client()\n",
    "local_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8387c-c700-4137-af80-cc75e689d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local cluster with multiple workers\n",
    "local_cluster = LocalCluster()  \n",
    "local_client = Client(local_cluster)\n",
    "local_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768fe323",
   "metadata": {},
   "source": [
    "<font size=\"6\">Shutting down cloud and local clusters</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "coiled_cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f376b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44f117",
   "metadata": {},
   "source": [
    "<font size=\"6\">Analysis</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ce237",
   "metadata": {},
   "source": [
    "<font size=\"4\">Paths and functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General paths and constants\n",
    "\n",
    "general_uri = 's3://gfw2-data/landcover/composite/'\n",
    "\n",
    "s3_out_dir = 'climate/AFOLU_flux_model/LULUCF/outputs'\n",
    "\n",
    "IPCC_class_max_val = 6\n",
    "\n",
    "# IPCC codes\n",
    "forest = 1\n",
    "cropland = 2\n",
    "settlement = 3\n",
    "wetland = 4\n",
    "grassland = 5\n",
    "otherland = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8f958-8527-4a33-ac10-3a0243bdee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestr():\n",
    "    return time.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "\n",
    "def boundstr(bounds):\n",
    "    bounds_str = \"_\".join([str(round(x)) for x in bounds])\n",
    "    return bounds_str\n",
    "\n",
    "def calc_chunk_length_pixels(bounds):\n",
    "    chunk_length_pixels = int((bounds[3]-bounds[1]) * (40000/10))\n",
    "    return chunk_length_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee3796-717c-4b93-895d-2010d933a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of all chunk boundaries within a bounding box for chunks of a given size\n",
    "def get_chunk_bounds(chunk_params):\n",
    "\n",
    "    min_x = chunk_params[0]\n",
    "    min_y = chunk_params[1]\n",
    "    max_x = chunk_params[2]\n",
    "    max_y = chunk_params[3]\n",
    "    chunk_size = chunk_params[4]\n",
    "    \n",
    "    x, y = (min_x, min_y)\n",
    "    chunks = []\n",
    "\n",
    "    # Polygon Size\n",
    "    while y < max_y:\n",
    "        while x < max_x:\n",
    "            bounds = [\n",
    "                x,\n",
    "                y,\n",
    "                x + chunk_size,\n",
    "                y + chunk_size,\n",
    "            ]\n",
    "            chunks.append(bounds)\n",
    "            x += chunk_size\n",
    "        x = min_x\n",
    "        y += chunk_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Returns the encompassing tile_id string in the form YYN/S_XXXE/W based on a coordinate\n",
    "def xy_to_tile_id(top_left_x, top_left_y):\n",
    "\n",
    "    lat_ceil = math.ceil(top_left_y/10.0) * 10\n",
    "    lng_floor = math.floor(top_left_x/10.0) * 10\n",
    "    \n",
    "    lng: str = f\"{str(lng_floor).zfill(3)}E\" if (lng_floor >= 0) else f\"{str(-lng_floor).zfill(3)}W\"\n",
    "    lat: str = f\"{str(lat_ceil).zfill(2)}N\" if (lat_ceil >= 0) else f\"{str(-lat_ceil).zfill(2)}S\"\n",
    "\n",
    "    return f\"{lat}_{lng}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ecf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazily opens tile within provided bounds (i.e. one chunk) and returns as a numpy array\n",
    "# If it can't open the chunk (no data in it), it returns an array of all 0s\n",
    "def get_tile_dataset_rio(uri, bounds, chunk_length):\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(uri) as ds:\n",
    "            window = rasterio.windows.from_bounds(*bounds, ds.transform)\n",
    "            data = ds.read(1, window=window)\n",
    "    except:\n",
    "        data = np.zeros((chunk_length, chunk_length))\n",
    "\n",
    "    if data.size==0:\n",
    "        # dask_print(\"No data in chunk\")\n",
    "        return np.zeros((chunk_length, chunk_length))\n",
    "    else:\n",
    "        # dask_print(\"Data in chunk\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74856567-cecf-4e37-a40b-cbd5ea11feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares list of chunks to download.\n",
    "# Chunks are defined by a bounding box.\n",
    "def prepare_to_download_chunk(bounds, start_year, download_dict):\n",
    " \n",
    "    futures = {}\n",
    "\n",
    "    bounds_str = boundstr(bounds)\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)\n",
    "\n",
    "    # Submit requests to S3 for input chunks but don't actually download them yet. This queueing of the requests before downloading them speeds up the downloading\n",
    "    # Approach is to download all the input chunks up front for every year to make downloading more efficient, even though it means storing more upfront\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        dask_print(f\"Requesting data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "\n",
    "        for key, value in download_dict.items():\n",
    "            futures[executor.submit(get_tile_dataset_rio, value, bounds, chunk_length_pixels)] = key\n",
    "\n",
    "    return futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb116818-8790-4f8f-8a3a-473a2c82e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_upload(bounds, chunk_length_pixels, tile_id, bounds_str, output_dict):\n",
    "\n",
    "    transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "\n",
    "    file_info = f'{tile_id}__{bounds_str}'\n",
    "\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    # For every output file, saves from array to local raster, then to s3.\n",
    "    # Can't save directly to s3, unfortunately, so need to save locally first.\n",
    "    for key, value in output_dict.items():\n",
    "\n",
    "        data_meaning = value[2]\n",
    "        year_out = value[3]\n",
    "\n",
    "        dask_print(f\"Saving {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "        file_name = f\"{file_info}__{key}__{timestr()}\"\n",
    "\n",
    "        with rasterio.open(f\"/tmp/{file_name}.tif\", 'w', driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels, count=1, dtype='uint8', crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "            dst.write(value[0].astype(rasterio.uint8), 1)\n",
    "\n",
    "        dask_print(f\"Uploading {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "        s3_client.upload_file(f\"/tmp/{file_name}.tif\", \"gfw2-data\", Key=f\"{s3_out_dir}/{data_meaning}/{year_out}/{chunk_length_pixels}_pixels/{time.strftime('%Y%m%d')}/{file_name}.tif\")\n",
    "\n",
    "        # Deletes the local raster. It won't be used again.\n",
    "        os.remove(f\"/tmp/{file_name}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4b896",
   "metadata": {},
   "source": [
    "<font size=\"4\">Model steps</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720e810-b37b-4b20-b8a5-e501b526e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reclassify GLCLU classes to basic IPCC reporting classes.\n",
    "# Operates on the array/chunk. \n",
    "# Classification comes from https://onewri-my.sharepoint.com/:p:/g/personal/david_gibbs_wri_org/EWwyxRfgdeVJi4ezwX7LrfcBjCoqAcjL2jRAZjb_8RU9LQ?e=YUsQiU\n",
    "def reclassify_to_IPCC(GLCLU_block):\n",
    "\n",
    "    # Outputs\n",
    "    IPCC_classes = np.zeros(GLCLU_block.shape)\n",
    "\n",
    "    IPCC_classes[np.where(GLCLU_block <= 1)] = otherland                                 \n",
    "    IPCC_classes[np.where((GLCLU_block >= 2) & (GLCLU_block <= 26))] = grassland          \n",
    "    IPCC_classes[np.where((GLCLU_block >= 27) & (GLCLU_block <= 48))] = forest         \n",
    "    IPCC_classes[np.where((GLCLU_block >= 100) & (GLCLU_block <= 101))] = wetland       \n",
    "    IPCC_classes[np.where((GLCLU_block >= 102) & (GLCLU_block <= 126))] = grassland       \n",
    "    IPCC_classes[np.where((GLCLU_block >= 127) & (GLCLU_block <= 148))] = forest       \n",
    "    IPCC_classes[np.where((GLCLU_block >= 200) & (GLCLU_block <= 204))] = wetland       \n",
    "    IPCC_classes[np.where((GLCLU_block >= 205) & (GLCLU_block <= 207))] = otherland       \n",
    "    IPCC_classes[np.where(GLCLU_block == 241)] = otherland                                \n",
    "    IPCC_classes[np.where(GLCLU_block == 244)] = cropland                                \n",
    "    IPCC_classes[np.where(GLCLU_block == 250)] = settlement                               \n",
    "    IPCC_classes[np.where(GLCLU_block == 254)] = otherland                              \n",
    "    \n",
    "    return IPCC_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070fd86-a857-445e-92e5-e9ba3dc99caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map basic IPCC change classes.\n",
    "# Operates pixel by pixel, so uses numba (Python compiled to C++).\n",
    "@jit(nopython=True)\n",
    "def change_classes_IPCC(IPCC_previous_block, IPCC_current_block):\n",
    "\n",
    "    # Output array of 0s\n",
    "    IPCC_change_block = np.zeros(IPCC_previous_block.shape)\n",
    "\n",
    "    # Iterates through all pixels in the chunk\n",
    "    for row in range(IPCC_previous_block.shape[0]):\n",
    "        for col in range(IPCC_previous_block.shape[1]):\n",
    "\n",
    "            IPCC_previous = IPCC_previous_block[row, col]\n",
    "            IPCC_current = IPCC_current_block[row, col]\n",
    "\n",
    "            # Equation to calculate the IPCC change code\n",
    "            IPCC_change_block[row, col] = ((IPCC_previous - 1) * IPCC_class_max_val) + IPCC_current\n",
    "\n",
    "    return IPCC_change_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7027b3-48f8-4f73-8da4-5065bfcc60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads input chunks, reclassifies GLCLU classes into IPCC land use reporting classes for each year, and maps changes between classes for consecutive years.\n",
    "# Chunks are defined by a bounding box and a starting year for iteration\n",
    "def reclassify_and_map_change_chunk(bounds, start_year):\n",
    "\n",
    "    bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])    # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees)\n",
    "\n",
    "    \n",
    "    ### Part 1: download tiles\n",
    "\n",
    "    # Dictionary of downloaded layers\n",
    "    layers = {}\n",
    "\n",
    "    download_dict = {\n",
    "        \n",
    "        # \"iso\": f\"s3://gfw2-data/gadm_administrative_boundaries/v3.6/raster/epsg-4326/10/40000/adm0/gdal-geotiff/{tile_id}.tif\",  # Originally from gfw-data-lake, so it's in 400x400 windows,\n",
    "\n",
    "        \"land_cover_2000\": f\"s3://gfw2-data/landcover/composite/2000/raw/{tile_id}.tif\",\n",
    "        \"land_cover_2005\": f\"s3://gfw2-data/landcover/composite/2005/raw/{tile_id}.tif\",\n",
    "        \"land_cover_2010\": f\"s3://gfw2-data/landcover/composite/2010/raw/{tile_id}.tif\",\n",
    "        \"land_cover_2015\": f\"s3://gfw2-data/landcover/composite/2015/raw/{tile_id}.tif\",\n",
    "        \"land_cover_2020\": f\"s3://gfw2-data/landcover/composite/2020/raw/{tile_id}.tif\"   \n",
    "    }\n",
    "    \n",
    "    futures = prepare_to_download_chunk(bounds, start_year, download_dict)\n",
    "    dask_print(f\"Waiting for requests for data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "    # Waits for requests to come back with data from S3\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        layer = futures[future]\n",
    "        layers[layer] = future.result()\n",
    "\n",
    "    \n",
    "    ### Part 2: reclassify GLCLU classes into IPCC reporting classes \n",
    "    IPCC_class_dict = {}\n",
    "\n",
    "    # Iterates through model years\n",
    "    for year in list(range(2000, 2021, 5)):\n",
    "        \n",
    "        dask_print(f\"Reclassifying {bounds_str} in {tile_id} for {year}: {timestr()}\")\n",
    "\n",
    "        # Reclassifies GLCLU to 6 IPCC classes \n",
    "        IPCC_classes = reclassify_to_IPCC(\n",
    "            layers[f\"land_cover_{year}\"]   \n",
    "        )\n",
    "\n",
    "        # Output files to upload to s3\n",
    "        IPCC_class_dict[f\"IPCC_classes_{year}\"] = [IPCC_classes, \"uint8\", \"IPCC_basic_classes\", year]                 \n",
    "\n",
    "        # Clear memory of unneeded arrays\n",
    "        del IPCC_classes\n",
    "\n",
    "    save_and_upload(bounds, chunk_length_pixels, tile_id, bounds_str, IPCC_class_dict)\n",
    "\n",
    "    \n",
    "    ### Part 3\n",
    "    IPCC_change_dict = {}\n",
    "\n",
    "    # Iterates through model years in a way that change can be calculated\n",
    "    for year in list(range(2005, 2021, 5)):\n",
    "        \n",
    "        dask_print(f\"Getting IPCC class change in {bounds_str} in {tile_id} for {year}: {timestr()}\")\n",
    "\n",
    "        # Maps change between IPCC classes\n",
    "        IPCC_change = change_classes_IPCC(\n",
    "            IPCC_class_dict[f\"IPCC_classes_{year-5}\"][0], # first [0] needed because results_download is a tuple with a dictionary inside it. Second [0] to isolate the array.\n",
    "            IPCC_class_dict[f\"IPCC_classes_{year}\"][0]    # first [0] needed because results_download is a tuple with a dictionary inside it. Second [0] to isolate the array.\n",
    "        )\n",
    "\n",
    "        # Output files to upload to s3\n",
    "        IPCC_change_dict[f\"IPCC_change_{year-5}_{year}\"] = [IPCC_change, \"uint8\", \"IPCC_basic_change\", f'{year-5}_{year}']  \n",
    "\n",
    "        # Clear memory of unneeded arrays\n",
    "        del IPCC_change\n",
    "\n",
    "    save_and_upload(bounds, chunk_length_pixels, tile_id, bounds_str, IPCC_change_dict)\n",
    "\n",
    "    return f\"success for {bounds_str}: {timestr()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Year to start the analysis\n",
    "# start_year = 2000   # full run\n",
    "start_year = 2020  # final year\n",
    "\n",
    "# Area to analyze\n",
    "# chunk_params arguments: W, S, E, N, chunk size (degrees)\n",
    "# chunk_params = [-12, 34, 32, 72, 1]  # all of Europe\n",
    "# chunk_params = [-10, 40, 20, 70, 1]    # 30x30 deg (70N_010W), 900 chunks\n",
    "# chunk_params = [10, 40, 20, 50, 1]    # 10x10 deg (50N_010E), 100 chunks\n",
    "# chunk_params = [10, 40, 20, 50, 10]    # 10x10 deg (50N_010E), 1 chunk\n",
    "# chunk_params = [10, 46, 14, 50, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [10, 48, 12, 50, 1]   # 2x2 deg, 4 chunks\n",
    "# chunk_params = [10, 49, 11, 50, 1]   # 1x1 deg, 1 chunk\n",
    "chunk_params = [10, 49, 11, 50, 0.5] # 1x1 deg, 4 chunks\n",
    "# chunk_params = [10, 49.5, 10.5, 50, 0.25] # 0.5x0.5 deg, 4 chunks\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data)\n",
    "# chunk_params = [0, 79.75, 0.25, 80, 0.25] # 0.25x0.25 deg, 1 chunk (no data)\n",
    "\n",
    "# Makes list of chunks to analyze\n",
    "chunks = get_chunk_bounds(chunk_params)  \n",
    "print(\"Processing\", len(chunks), \"chunks\")\n",
    "\n",
    "# Creates list of tasks to run (1 task = 1 chunk for all years)\n",
    "delayed_result = [dask.delayed(reclassify_and_map_change_chunk)(chunk, start_year) for chunk in chunks]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c322828-5e41-4706-a9df-09c9e5cafcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d12370-fe37-4d06-8a66-9210a4628988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1114ac-9c2c-4cfb-9212-1d96f1aada06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run without dask at all\n",
    "process_chunk([10, 49, 11, 50], 1, start_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5f88b-9a9d-45ce-ad03-235510e6798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test-- checks that uri is found and recognized\n",
    "tile_id = \"50N_010E\"\n",
    "# uri = f\"s3://gfw2-data/climate/carbon_model/BGB_AGB_ratio/processed/20230216/{tile_id}_BGB_AGB_ratio.tif\"\n",
    "uri = f\"s3://gfw2-data/gadm_administrative_boundaries/v3.6/raster/epsg-4326/10/40000/adm0/gdal-geotiff/{tile_id}.tif\"  # Originally from gfw-data-lake, so it's in 400x400 windows\n",
    "# uri = f\"s3://gfw2-data/fao_ecozones/v2000/raster/epsg-4326/10/40000/class/gdal-geotiff/{tile_id}.tif\"   # Originally from gfw-data-lake, so it's in 400x400 windows \n",
    "bounds = [10, 49.75, 10.25, 50]\n",
    "\n",
    "get_tile_dataset_rio(uri, bounds, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c41e5-b135-4933-8bce-68d2acfcb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coiled_client.restart() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53347e4f-aaa0-440d-9080-dc95a31bb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(future) # per https://github.com/dask/distributed/issues/3898#issuecomment-645590511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2de194-d396-4ab8-bd77-4641e4997a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws s3 cp s3://gfw2-data/climate/European_height_carbon_model/outputs/ . --recursive --exclude \"*\" --include \"*10_49_11_50*\"\n",
    "# aws s3 cp s3://gfw2-data/climate/European_height_carbon_model/outputs/ . --recursive --exclude \"*\" --include \"*2002*10_49_11_50*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5045c-a010-4762-88c3-a42a5d3edbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
