{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e10cac-fb30-475f-ba4c-8595ad81ff19",
   "metadata": {},
   "source": [
    "<font size=\"6\">Create starting non-soil carbon density rasters from WHRC AGB 2000: aboveground carbon, belowground carbon, deadwood carbonn, litter carbon</font> \n",
    "\n",
    "<font size=\"4\">Must be run using the utilities_and_variables.ipynb kernel</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "26d2c3b6-0578-4f82-9e7f-d03f34168400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create initial non-soil carbon pool densities\n",
    "# Operates pixel by pixel, so uses numba (Python compiled to C++).\n",
    "@jit(nopython=True)\n",
    "def create_starting_C_densities(in_dict_uint8, in_dict_int16, in_dict_int32, in_dict_float32):\n",
    "\n",
    "    # Separate dictionaries for output numpy arrays of each datatype, named by output data type.\n",
    "    # This is because a dictionary in a Numba function cannot have arrays with multiple data types, so each dictionary has to store only one data type,\n",
    "    # just like inputs to the function.\n",
    "    out_dict_float32 = {}\n",
    "\n",
    "    agb_dens_curr_block = in_dict_int16[agb_2000]\n",
    "    elevation_block = in_dict_int16[elevation]\n",
    "    climate_domain_block = in_dict_int16[climate_domain]\n",
    "    precipitation_block = in_dict_int32[precipitation]\n",
    "\n",
    "    # # Iterates through all pixels in the chunk\n",
    "    # for row in range(IPCC_previous_block.shape[0]):\n",
    "    #     for col in range(IPCC_previous_block.shape[1]):\n",
    "\n",
    "    #         IPCC_previous = IPCC_previous_block[row, col]\n",
    "    #         IPCC_current = IPCC_current_block[row, col]\n",
    "\n",
    "    #         # When land cover chunks have \"no data\"\n",
    "    #         if (IPCC_previous == 0) and (IPCC_current == 0):\n",
    "    #             IPCC_change_block[row, col] = 0\n",
    "\n",
    "    #         else:\n",
    "    #             # Equation to calculate the IPCC change code\n",
    "    #             IPCC_change_block[row, col] = ((IPCC_previous - 1) * IPCC_class_max_val) + IPCC_current\n",
    "\n",
    "    # return IPCC_change_block\n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80c387b9-3466-42ee-ae25-b4cc2da5e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def create_and_upload_starting_C_densities(bounds, is_final):\n",
    "\n",
    "    bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])    # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees)\n",
    "\n",
    "    no_data_val = 255\n",
    "\n",
    "    \n",
    "    ### Part 1: download chunks and check for data\n",
    "\n",
    "    # Dictionary of downloaded layers\n",
    "    layers = {}\n",
    "\n",
    "    download_dict = {\n",
    "        \n",
    "        agb_2000: f\"s3://gfw2-data/climate/WHRC_biomass/WHRC_V4/Processed/{tile_id}_t_aboveground_biomass_ha_2000.tif\",\n",
    "        elevation: f\"s3://gfw2-data/climate/carbon_model/inputs_for_carbon_pools/processed/elevation/20190418/{tile_id}_elevation.tif\",\n",
    "        climate_domain: f\"s3://gfw2-data/climate/carbon_model/inputs_for_carbon_pools/processed/fao_ecozones_bor_tem_tro/20190418/{tile_id}_fao_ecozones_bor_tem_tro_processed.tif\",\n",
    "        precipitation: f\"s3://gfw2-data/climate/carbon_model/inputs_for_carbon_pools/processed/precip/20190418/{tile_id}_precip_mm_annual.tif\"\n",
    "    }\n",
    "\n",
    "    # Checks whether tile exists at all. Doesn't try to download chunk if the tile doesn't exist.\n",
    "    tile_exists = check_for_tile(download_dict, is_final)\n",
    "\n",
    "    if tile_exists == 0:\n",
    "        return\n",
    "\n",
    "    futures = prepare_to_download_chunk(bounds, download_dict, no_data_val)\n",
    "\n",
    "    if not is_final:\n",
    "        print(f\"Waiting for requests for data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "    \n",
    "    # Waits for requests to come back with data from S3\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        layer = futures[future]\n",
    "        layers[layer] = future.result()\n",
    "\n",
    "    # Checks chunk for data. Skips the chunk if it has no data in it.\n",
    "    data_in_chunk = check_chunk_for_data(layers, agb_2000, bounds_str, tile_id, no_data_val, is_final)\n",
    "\n",
    "    if data_in_chunk == 0:\n",
    "        return\n",
    "        \n",
    "        \n",
    "    ### Part 2: Create a separate dictionary for each chunk datatype so that they can be passed to Numba as separate arguments.\n",
    "    ### Numba functions can accept (and return) dictionaries of arrays as long as each dictionary only has arrays of one data type (e.g., uint8, float32)\n",
    "    ### Note: need to add new code if inputs with other data types are added\n",
    "\n",
    "    typed_dict_uint8, typed_dict_int16, typed_dict_int32, typed_dict_float32 = create_typed_dicts(layers)\n",
    "    \n",
    "    \n",
    "    # ### Part 3: Create starting carbon pool densities and upload them to s3\n",
    "\n",
    "    print(f\"Creating starting C densities for {bounds_str} in {tile_id}: {timestr()}\")\n",
    "\n",
    "    # Create AGC, BGC, deadwood C and litter C\n",
    "    out_dict_float32 = create_starting_C_densities(\n",
    "        typed_dict_uint8, typed_dict_int16, typed_dict_int32, typed_dict_float32  \n",
    "    )\n",
    "\n",
    "    # # # Output arrays to upload to s3. Adds new array to dictionary for each year\n",
    "    # # IPCC_class_dict[f\"{IPCC_class_pattern}_{year}\"] = [IPCC_classes, IPCC_classes.dtype.name, IPCC_class_path, year]                 \n",
    "    \n",
    "    # # print(IPCC_class_dict)\n",
    "    \n",
    "    # # save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, IPCC_class_dict, is_final)\n",
    "\n",
    "    \n",
    "    # # # Clear memory of unneeded arrays\n",
    "    # # del IPCC_classes\n",
    "    # # del IPCC_class_dict\n",
    "    # # del IPCC_change\n",
    "    # # del IPCC_change_dict\n",
    "\n",
    "    # return f\"Success for {bounds_str}: {timestr()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0ad8aa54-9be9-45f3-9627-259e60433ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 chunks\n",
      "Tile id _ha_2000 exists. Proceeding.\n",
      "Requesting data in chunk 15_42_15_42 in 50N_010E: 20240712_21_36_42\n",
      "Waiting for requests for data in chunk 15_42_15_42 in 50N_010E: 20240712_21_36_45\n",
      "Data in chunk 15_42_15_42. Proceeding.\n",
      "uint8 datasets: dict_keys([])\n",
      "int16 datasets: dict_keys(['agb_2000', 'elevation', 'fao_ecozone_climate_domain'])\n",
      "int32 datasets: dict_keys(['precipitation_annual'])\n",
      "float32 datasets: dict_keys([])\n",
      "Creating starting C densities for 15_42_15_42 in 50N_010E: 20240712_21_36_45\n"
     ]
    },
    {
     "ename": "TypingError",
     "evalue": "\u001b[1mFailed in nopython mode pipeline (step: nopython frontend)\n\u001b[1mCannot infer the type of variable 'out_dict_float32', have imprecise type: DictType[undefined,undefined]<iv={}>. \n\u001b[1m\nFile \"ipykernel_5674/32138510.py\", line 9:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:51\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dask_notebook/lib/python3.10/site-packages/dask/base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "Cell \u001b[0;32mIn[122], line 59\u001b[0m, in \u001b[0;36mcreate_and_upload_starting_C_densities\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating starting C densities for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbounds_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestr()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Create AGC, BGC, deadwood C and litter C\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m out_dict_float32 \u001b[38;5;241m=\u001b[39m create_starting_C_densities(\n\u001b[1;32m     60\u001b[0m     typed_dict_uint8, typed_dict_int16, typed_dict_int32, typed_dict_float32  \n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.10/site-packages/numba/core/dispatcher.py:468\u001b[0m, in \u001b[0;36m_compile_for_args\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.10/site-packages/numba/core/dispatcher.py:409\u001b[0m, in \u001b[0;36merror_rewrite\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypingError\u001b[0m: \u001b[1mFailed in nopython mode pipeline (step: nopython frontend)\n\u001b[1mCannot infer the type of variable 'out_dict_float32', have imprecise type: DictType[undefined,undefined]<iv={}>. \n\u001b[1m\nFile \"ipykernel_5674/32138510.py\", line 9:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Create LULUCF flux and carbon stock 2x2 deg rasters \n",
    "\n",
    "## Area to analyze\n",
    "## chunk_params arguments: W, S, E, N, chunk size (degrees)\n",
    "# chunk_params = [-180, -60, 180, 80, 2]  # entire world\n",
    "# chunk_params = [-10, 40, 20, 70, 1]    # 30x30 deg (70N_010W), 900 chunks\n",
    "\n",
    "# chunk_params = [-10, 60, 0, 70, 1]    # 10x10 deg (70N_010W), 100 chunks\n",
    "# chunk_params = [-10, 65, -5, 70, 1]    # 5x5 deg (70N_010W), 25 chunks\n",
    "# chunk_params = [-10, 68, -8, 70, 1]    # 2x2 deg (70N_010W), 4 chunks\n",
    "# chunk_params = [-10, 69, -9, 70, 1]    # 1x1 deg (70N_010W), 1 chunk\n",
    "\n",
    "# chunk_params = [10, 40, 20, 50, 2]    # 10x10 deg (50N_010E), 25 chunks\n",
    "# chunk_params = [10, 40, 20, 50, 10]    # 10x10 deg (50N_010E), 1 chunk\n",
    "# chunk_params = [10, 46, 14, 50, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [110, -10, 114, -6, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [10, 48, 12, 50, 1]   # 2x2 deg, 4 chunks\n",
    "# chunk_params = [10, 49, 11, 50, 1]   # 1x1 deg, 1 chunk\n",
    "# chunk_params = [10, 49, 11, 50, 0.5] # 1x1 deg, 4 chunks\n",
    "# chunk_params = [10, 49.5, 10.5, 50, 0.25] # 0.5x0.5 deg, 4 chunks\n",
    "# chunk_params = [10, 42, 11, 43, 0.5] # 1x1 deg, 4 chunks (some GLCLU code=254 for ocean and some land, so data should be output)\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data, no fire)\n",
    "chunk_params = [15, 41.75, 15.25, 42, 0.25] # 0.25x0.25 deg, 1 chunk (has data with fire)\n",
    "\n",
    "# # Range of no-data cases for testing\n",
    "# chunk_params = [20, 69.75, 20.25, 70, 0.25] # 0.25x0.25 deg, 1 chunk (tile exists for GLCLU but not all other inputs, e.g., fire)\n",
    "# chunk_params = [110, -10, 120, 0, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [110, -20, 120, -10, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [0, 79.75, 0.25, 80, 0.25] # 0.25x0.25 deg, 1 chunk (no 80N_000E tile-- no data)\n",
    "# chunk_params = [112, -12, 116, -8, 2]   # 2x2 deg, 1 chunk (bottom of Java, has data but mostly ocean)\n",
    "# chunk_params = [10.875, 41.75, 11, 42, 0.25] # 0.25x0.25 deg, 1 chunk (entirely GLCLU code=255 for ocean, so no actual data-- nothing should be be output)\n",
    "# chunk_params = [-10, 21.75, -9.75, 22, 0.25] # 0.25x0.25 deg, 1 chunk (has data but entirely desert (fully GLCLU code=0))\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data)\n",
    "\n",
    "\n",
    "# Makes list of chunks to analyze\n",
    "chunks = get_chunk_bounds(chunk_params)  \n",
    "print(\"Processing\", len(chunks), \"chunks\")\n",
    "# print(chunks)\n",
    "\n",
    "# Determines if the output file names for final versions of outputs should be used\n",
    "is_final = False\n",
    "if len(chunks) > 90:\n",
    "    is_final = True\n",
    "    print(\"Running as final model.\")\n",
    "\n",
    "# Creates list of tasks to run (1 task = 1 chunk for all years)\n",
    "delayed_result = [dask.delayed(create_and_upload_starting_C_densities)(chunk, is_final) for chunk in chunks]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
