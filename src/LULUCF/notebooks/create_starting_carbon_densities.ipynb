{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e10cac-fb30-475f-ba4c-8595ad81ff19",
   "metadata": {},
   "source": [
    "<font size=\"6\">Create starting non-soil carbon density rasters from WHRC AGB 2000: aboveground carbon, belowground carbon, deadwood carbon, litter carbon</font> \n",
    "\n",
    "<font size=\"4\">Must be run using the utilities_and_variables.ipynb kernel</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2c3b6-0578-4f82-9e7f-d03f34168400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create initial (year 2000) non-soil carbon pool densities\n",
    "# Operates pixel by pixel, so uses numba (Python compiled to C++).\n",
    "@jit(nopython=True)\n",
    "def create_starting_C_densities(in_dict_uint8, in_dict_int16, in_dict_int32, in_dict_float32, mangrove_C_ratio_array):\n",
    "    \n",
    "    # Separate dictionaries for output numpy arrays of each datatype, named by output data type.\n",
    "    # This is because a dictionary in a Numba function cannot have arrays with multiple data types, so each dictionary has to store only one data type,\n",
    "    # just like inputs to the function.\n",
    "    out_dict_float32 = {}\n",
    "\n",
    "    whrc_agb_2000_block = in_dict_int16[\"agb_2000\"]\n",
    "    mangrove_agb_2000_block = in_dict_float32[\"mangrove_agb_2000\"]\n",
    "    elevation_block = in_dict_int16[\"elevation\"]\n",
    "    climate_domain_block = in_dict_int16[\"climate_domain\"]\n",
    "    precipitation_block = in_dict_int32[\"precipitation\"]\n",
    "    r_s_ratio_block = in_dict_float32[\"r_s_ratio\"]\n",
    "    continent_ecozone_block = in_dict_int16[\"continent_ecozone\"]\n",
    "\n",
    "    agc_2000_out_block = np.zeros(in_dict_float32[\"r_s_ratio\"].shape).astype('float32')  # Need to specify the output datatype or it will default to float32\n",
    "    bgc_2000_out_block = np.zeros(in_dict_float32[\"r_s_ratio\"].shape).astype('float32')\n",
    "    deadwood_c_2000_out_block = np.zeros(in_dict_float32[\"r_s_ratio\"].shape).astype('float32')\n",
    "    litter_c_2000_out_block = np.zeros(in_dict_float32[\"r_s_ratio\"].shape).astype('float32')\n",
    "\n",
    "    mangrove_in_chunk = True   # Flag for whether chunk has mangrove in it (defaults to True)\n",
    "\n",
    "    # Checks if chunk has mangrove data in it by comparing the min and max values of the chunk with the NoData value.\n",
    "    # I originally compared the mangrove chunk to NoData using chunk's np.mean. That identified non-mangrove chunks \n",
    "    # for 1000x1000 pixel chunks but not for 2000x2000 pixel chunks when I was using numba. \n",
    "    # That is, np.mean(mangrove_agb_2000_block) kept equalling ~67 rather than the expected 255 in 2000x2000 pixel chunks, specifically.\n",
    "    # I don't know why np.mean and np.average gave incorrect chunk averages for these larger chunks,\n",
    "    # but comparing the mangrove NoData value to the min and max of the chunk seems to correctly identify chunks without mangroves.\n",
    "    if ((np.min(mangrove_agb_2000_block) == mang_no_data_val) and (np.max(mangrove_agb_2000_block) == mang_no_data_val)):\n",
    "        mangrove_in_chunk = False\n",
    "\n",
    "    # Iterates through all pixels in the chunk\n",
    "    for row in range(whrc_agb_2000_block.shape[0]):\n",
    "        for col in range(whrc_agb_2000_block.shape[1]):\n",
    "\n",
    "            # Input values for this specific cell\n",
    "            whrc_agb_2000 = whrc_agb_2000_block[row, col]    \n",
    "            mangrove_agb_2000 = mangrove_agb_2000_block[row, col]   # Would be 255 if pixel is NoData\n",
    "            elevation = elevation_block[row, col]\n",
    "            climate_domain = climate_domain_block[row, col]\n",
    "            precipitation = precipitation_block[row, col]\n",
    "            r_s_ratio = r_s_ratio_block[row, col]\n",
    "            continent_ecozone = continent_ecozone_block[row, col]\n",
    "\n",
    "            # If mangrove AGB is present, it replaces the non-mangrove AGB\n",
    "            if mangrove_in_chunk and (mangrove_agb_2000 > 0):   # Only replaces WHRC AGB if mangrove chunk exists and if there is a mangrove value in that pixel\n",
    "                whrc_agb_2000 = mangrove_agb_2000\n",
    "\n",
    "            # Calculates AGC from AGB. Same calculation for mangroves and non-mangroves.\n",
    "            agc_2000_out_block[row, col] = whrc_agb_2000 * biomass_to_carbon\n",
    "\n",
    "            # Ridiculous default BGC, deadwood C, and litter C ratios that will make it very clear if they are being used instead of \n",
    "            # something being assigned in the decision tree below\n",
    "            bgc_ratio = -5\n",
    "            deadwood_c_ratio = -10\n",
    "            litter_c_ratio = -20\n",
    "\n",
    "            # Separate branches for assigning BGC, deadwood C, and litter C ratios depending on whether the pixel has mangroves.\n",
    "            # Calculation of BGC, deadwood C, and litter C are done after the decision tree assigns the ratios.\n",
    "            \n",
    "            # Mangrove carbon pool ratio branch\n",
    "            # From IPCC 2013 Wetland Supplement\n",
    "            if (mangrove_in_chunk) and (mangrove_agb_2000 > 0):    # Only replaces WHRC AGB if mangrove chunk exists and if mangrove value in that pixel                                                         \n",
    "                bgc_ratio = mangrove_C_ratio_array[np.where(mangrove_C_ratio_array[:,0] == continent_ecozone)][0,1]\n",
    "                deadwood_c_ratio = mangrove_C_ratio_array[np.where(mangrove_C_ratio_array[:,0] == continent_ecozone)][0,2]\n",
    "                litter_c_ratio = mangrove_C_ratio_array[np.where(mangrove_C_ratio_array[:,0] == continent_ecozone)][0,3]\n",
    "\n",
    "            # Non-mangrove carbon pool ratio branch\n",
    "            # Deadwood and litter carbon as fractions of AGC are from\n",
    "            # https://cdm.unfccc.int/methodologies/ARmethodologies/tools/ar-am-tool-12-v3.0.pdf\n",
    "            # \"Clean Development Mechanism A/R Methodological Tool: \n",
    "            # Estimation of carbon stocks and change in carbon stocks in dead wood and litter in A/R CDM project activities version 03.0\"\n",
    "            # Tables on pages 18 (deadwood) and 19 (litter).\n",
    "            # They depend on the climate domain, elevation, and precipitation. \n",
    "            else:                                                                                       # Non-mangrove\n",
    "                \n",
    "                # If no mapped R:S, uses the global default value instead\n",
    "                if r_s_ratio == 0:\n",
    "                    r_s_ratio = default_r_s\n",
    "                bgc_ratio = r_s_ratio                                                                   # Uses R:S for BGC\n",
    "\n",
    "                if climate_domain == 1:                                                                 # Tropical\n",
    "                    if elevation <= 2000:                                                               # Low elevation\n",
    "                        if precipitation <= 1000:                                                       # Low precipitation or no precip raster\n",
    "                            deadwood_c_ratio = tropical_low_elev_low_precip_deadwood_c_ratio\n",
    "                            litter_c_ratio = tropical_low_elev_low_precip_litter_c_ratio\n",
    "                        elif ((precipitation > 1000) and (precipitation <= 1600)):                      # Medium precipitation\n",
    "                            deadwood_c_ratio = tropical_low_elev_med_precip_deadwood_c_ratio\n",
    "                            litter_c_ratio = tropical_low_elev_med_precip_litter_c_ratio\n",
    "                        else:                                                                           # High precipitation\n",
    "                            deadwood_c_ratio = tropical_low_elev_high_precip_deadwood_c_ratio\n",
    "                            litter_c_ratio = tropical_low_elev_high_precip_litter_c_ratio\n",
    "                    else:                                                                               # High elevation\n",
    "                        deadwood_c_ratio = tropical_high_elev_deadwood_c_ratio\n",
    "                        litter_c_ratio = tropical_high_elev_litter_c_ratio\n",
    "                else:                                                                                   # Non-tropical (temperate/boreal)\n",
    "                    deadwood_c_ratio = non_tropical_deadwood_c_ratio\n",
    "                    litter_c_ratio = non_tropical_litter_c_ratio  \n",
    "\n",
    "            # Actually calculates BGC, deadwood C, and litter C using the ratios assigned in the above decision tree\n",
    "            bgc_2000_out_block[row, col] = agc_2000_out_block[row, col] * bgc_ratio\n",
    "            deadwood_c_2000_out_block[row, col] = agc_2000_out_block[row, col] * deadwood_c_ratio\n",
    "            litter_c_2000_out_block[row, col] = agc_2000_out_block[row, col] * litter_c_ratio\n",
    "    \n",
    "    # Adds the output arrays to the dictionary with the appropriate data type\n",
    "    # Outputs need .copy() so that previous intervals' arrays in dicationary aren't overwritten because arrays in dictionaries are mutable (courtesy of ChatGPT).        \n",
    "    out_dict_float32[f\"{agc_dens_pattern}_{first_year}\"] = agc_2000_out_block.copy()\n",
    "    out_dict_float32[f\"{bgc_dens_pattern}_{first_year}\"] = bgc_2000_out_block.copy()\n",
    "    out_dict_float32[f\"{deadwood_c_dens_pattern}_{first_year}\"] = deadwood_c_2000_out_block.copy()\n",
    "    out_dict_float32[f\"{litter_c_dens_pattern}_{first_year}\"] = litter_c_2000_out_block.copy()\n",
    "\n",
    "    # return IPCC_change_block\n",
    "    return out_dict_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c387b9-3466-42ee-ae25-b4cc2da5e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All steps for creating starting non-soil carbon pools in a chunk: download chunks, calculate carbon densities, upload to s3\n",
    "def create_and_upload_starting_C_densities(bounds, is_final, mangrove_C_ratio_array):\n",
    "\n",
    "    logger = setup_logging()\n",
    "\n",
    "    bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])    # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees)   \n",
    "    \n",
    "    ### Part 1: download chunks and check for data\n",
    "\n",
    "    mang_no_data_val = 255   # NoData value in mangrove AGB raster. For checking input chunks.\n",
    "\n",
    "    # Dictionary of downloaded layers\n",
    "    layers = {}\n",
    "\n",
    "    download_dict = {\n",
    "        \n",
    "        agb_2000: f\"{agb_2000_path}{tile_id}_{agb_2000_pattern}.tif\",\n",
    "        mangrove_agb_2000: f\"{mangrove_agb_2000_path}{tile_id}_{mangrove_agb_2000_pattern}.tif\",\n",
    "        elevation: f\"{elevation_path}{tile_id}_{elevation_pattern}.tif\",\n",
    "        climate_domain: f\"{climate_domain_path}{tile_id}_{climate_domain_pattern}.tif\",\n",
    "        precipitation: f\"{precipitation_path}{tile_id}_{precipitation_pattern}.tif\",\n",
    "        r_s_ratio: f\"{r_s_ratio_path}{tile_id}_{r_s_ratio_pattern}.tif\",\n",
    "        continent_ecozone: f\"{continent_ecozone_path}{tile_id}_{continent_ecozone_pattern}.tif\"\n",
    "    }\n",
    "\n",
    "    # Checks whether tile exists at all. Doesn't try to download chunk if the tile doesn't exist.\n",
    "    tile_exists = check_for_tile(download_dict, is_final, logger)\n",
    "\n",
    "    if tile_exists == 0:\n",
    "        return\n",
    "\n",
    "    futures = prepare_to_download_chunk(bounds, download_dict, mang_no_data_val, is_final, logger)\n",
    "\n",
    "    print_and_log(f\"Waiting for requests for data in chunk {bounds_str} in {tile_id}: {timestr()}\", is_final, logger)\n",
    "\n",
    "    # if not is_final:\n",
    "    #     logger.info(f\"flm: Waiting for requests for data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "    #     # print(f\"flm: Waiting for requests for data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "    \n",
    "    # Waits for requests to come back with data from S3\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        layer = futures[future]\n",
    "        layers[layer] = future.result()\n",
    "    \n",
    "    layers_to_check_for_data = {'agb_2000': layers['agb_2000'],\n",
    "                                'mangrove_agb_2000': layers['mangrove_agb_2000']}\n",
    "    \n",
    "    # Checks chunk for data. Skips the chunk if it has no data in it.\n",
    "    data_in_chunk = check_chunk_for_data(layers_to_check_for_data, agb_2000, bounds_str, tile_id, is_final, logger)\n",
    "\n",
    "    if data_in_chunk == 0:\n",
    "        return\n",
    "        \n",
    "        \n",
    "    ### Part 2: Create a separate dictionary for each chunk datatype so that they can be passed to Numba as separate arguments.\n",
    "    ### Numba functions can accept (and return) dictionaries of arrays as long as each dictionary only has arrays of one data type (e.g., uint8, float32)\n",
    "    ### Note: need to add new code if inputs with other data types are added\n",
    "\n",
    "    typed_dict_uint8, typed_dict_int16, typed_dict_int32, typed_dict_float32 = create_typed_dicts(layers)\n",
    "    \n",
    "    \n",
    "    ### Part 3: Create starting carbon pool densities and upload them to s3\n",
    "\n",
    "    print_and_log(f\"Creating starting C densities for {bounds_str} in {tile_id}: {timestr()}\", is_final, logger)\n",
    "    \n",
    "    # if not is_final:\n",
    "    #     logger.info(f\"flm: Creating starting C densities for {bounds_str} in {tile_id}: {timestr()}\")\n",
    "    #     # print(f\"flm: Creating starting C densities for {bounds_str} in {tile_id}: {timestr()}\")\n",
    "\n",
    "    # Create AGC, BGC, deadwood C and litter C\n",
    "    out_dict_float32 = create_starting_C_densities(\n",
    "        typed_dict_uint8, typed_dict_int16, typed_dict_int32, typed_dict_float32, mangrove_C_ratio_array  \n",
    "    )\n",
    "\n",
    "    # Fresh non-Numba-constrained dictionary that stores all numpy arrays.\n",
    "    # The dictionaries by datatype that are returned from the numba function have limitations on them, \n",
    "    # e.g., they can't be combined with other datatypes. This prevents the addition of attributes needed for uploading to s3.\n",
    "    # So the trick here is to copy the numba-exported arrays into normal Python arrays to which we can do anything in Python.\n",
    "    \n",
    "    out_dict_all_dtypes = {}\n",
    "\n",
    "    # Transfers the dictionaries of numpy arrays for each data type to a new, Pythonic array\n",
    "    for key, value in out_dict_float32.items():\n",
    "        out_dict_all_dtypes[key] = value\n",
    "\n",
    "    # Clear memory of unneeded arrays\n",
    "    del out_dict_float32\n",
    "\n",
    "    \n",
    "    ### Part 4: Save numpy arrays as rasters and upload to s3\n",
    "\n",
    "    out_no_data_val = 0   # NoData value for output raster (optional)\n",
    "\n",
    "    # Adds metadata used for uploading outputs to s3 to the dictionary\n",
    "    for key, value in out_dict_all_dtypes.items():\n",
    "\n",
    "        data_type = value.dtype.name\n",
    "        out_pattern = key[:-5]    # Drops the year (2000) from the end of the string \n",
    "\n",
    "        # Dictionary with metadata for each array\n",
    "        out_dict_all_dtypes[key] = [value, data_type, out_pattern, first_year]\n",
    "\n",
    "    save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, out_dict_all_dtypes, is_final, logger, out_no_data_val)\n",
    "    \n",
    "    # Clear memory of unneeded arrays\n",
    "    del out_dict_all_dtypes\n",
    "\n",
    "    return f\"Success for {bounds_str}: {timestr()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8aa54-9be9-45f3-9627-259e60433ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Coiled cluster config notes:\n",
    "At most points, at least a few workers exceeded 16GB, with most between 8 and 16GB. \n",
    "This makes me think that 32GB workers are a good size. \n",
    "Running with 40x 32GB r6i.2xlarge workers had 200 tasks running simultaneously. \n",
    "\"\"\"\n",
    "\n",
    "## Create carbon density year 2000 2x2 deg rasters \n",
    "\n",
    "## Area to analyze\n",
    "## chunk_params arguments: W, S, E, N, chunk size (degrees)\n",
    "chunk_params = [-180, -60, 180, 80, 2]  # entire world (12600 chunks) (40x 32GB r6i.2xlarge workers= 32 minutes; around 11 Coiled credits and a few dollars of AWS costs)\n",
    "# chunk_params = [110, -10, 140, 20, 2]    # 30x30 deg (20N_110E, mangroves), 225 chunks (40x 32GB r6i.2xlarge workers=2.5 minutes)\n",
    "# chunk_params = [-10, 40, 20, 70, 2]    # 30x30 deg (70N_010W, no mangroves), 225 chunks (40x 32GB r6i.2xlarge workers=2.25 minutes)\n",
    "# chunk_params = [10, 40, 20, 50, 2]   # 10x10 deg (50N_010E), 25 chunks\n",
    "# chunk_params = [10, 46, 14, 50, 2]   # 4x4 deg (50N_010E), 4 chunks\n",
    "# chunk_params = [10, 48, 12, 50, 2]   # 2x2 deg (50N_010E), 1 chunk\n",
    "# chunk_params = [10, 48, 12, 50, 1]   # 2x2 deg (50N_010E), 4 chunks\n",
    "# chunk_params = [10, 49, 11, 50, 1]   # 1x1 deg (50N_010E), 1 chunk\n",
    "\n",
    "\n",
    "# chunk_params = [10, 40, 20, 50, 10]    # 10x10 deg (50N_010E), 1 chunk   \n",
    "# chunk_params = [10, 46, 14, 50, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [110, -10, 114, -6, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [10, 48, 12, 50, 1]   # 2x2 deg, 4 chunks\n",
    "# chunk_params = [10, 49, 11, 50, 1]   # 1x1 deg, 1 chunk\n",
    "# chunk_params = [10, 49, 11, 50, 0.5] # 1x1 deg, 4 chunks\n",
    "# chunk_params = [10, 49.5, 10.5, 50, 0.25] # 0.5x0.5 deg, 4 chunks\n",
    "# chunk_params = [10, 42, 11, 43, 0.5] # 1x1 deg, 4 chunks (some GLCLU code=254 for ocean and some land, so data should be output)\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data, no fire)\n",
    "# chunk_params = [15, 41.75, 15.25, 42, 0.25] # 0.25x0.25 deg, 1 chunk (has data with fire)\n",
    "# chunk_params = [116, -3, 116.25, -2.75, 0.25] # 0.25x0.25 deg, 1 chunk (has mangroves)\n",
    "\n",
    "\n",
    "# # Range of no-data cases for testing\n",
    "# chunk_params = [20, -70, 20.25, -69.75, 0.25] # 0.25x0.25 deg, 1 chunk (tile does not exist)\n",
    "# chunk_params = [20, 69.75, 20.25, 70, 0.25] # 0.25x0.25 deg, 1 chunk (tile exists for GLCLU but not all other inputs, e.g., fire)\n",
    "# chunk_params = [110, -10, 120, 0, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [110, -20, 120, -10, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [0, 79.75, 0.25, 80, 0.25] # 0.25x0.25 deg, 1 chunk (no 80N_000E tile-- no data)\n",
    "# chunk_params = [112, -12, 116, -8, 2]   # 2x2 deg, 1 chunk (bottom of Java, has data but mostly ocean)\n",
    "# chunk_params = [10.875, 41.75, 11, 42, 0.25] # 0.25x0.25 deg, 1 chunk (entirely GLCLU code=255 for ocean, so no actual data-- nothing should be be output)\n",
    "# chunk_params = [-10, 21.75, -9.75, 22, 0.25] # 0.25x0.25 deg, 1 chunk (has data but entirely desert (fully GLCLU code=0))\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data)\n",
    "\n",
    "# Creates numpy array of ratios of BGC, deadwood C, and litter C relative to AGC. Relevant columns must be specified. \n",
    "mangrove_C_ratio_array = convert_lookup_table_to_array(rate_ratio_spreadsheet, mangrove_rate_ratio_tab, ['gainEcoCon', 'BGC_AGC', 'deadwood_AGC', 'litter_AGC'])\n",
    "\n",
    "# Makes list of chunks to analyze\n",
    "chunks = get_chunk_bounds(chunk_params)  \n",
    "print(\"Processing\", len(chunks), \"chunks\")\n",
    "# print(chunks)\n",
    "\n",
    "# Determines if the output file names for final versions of outputs should be used\n",
    "is_final = False\n",
    "if len(chunks) > 90:\n",
    "    is_final = True\n",
    "    print(\"Running as final model.\")\n",
    "\n",
    "# Creates list of tasks to run (1 task = 1 chunk)\n",
    "delayed_result = [dask.delayed(create_and_upload_starting_C_densities)(chunk, is_final, mangrove_C_ratio_array) for chunk in chunks]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac33020-1fa7-4062-961a-ebc625d69e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coiled_cluster.get_logs()\n",
    "\n",
    "# Get the logs for all workers\n",
    "logs = coiled_cluster.get_logs()\n",
    "\n",
    "compile_and_upload_log(logs, \"C_pools_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512334ac-252a-4ea4-8802-20174efc3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Create raster footprint shapefiles from listed rasters\n",
    "## Doesn't use memory. Can be done on 4 GB workers. Only need as many workers as there are folders. \n",
    "## 12 minutes for global run\n",
    "\n",
    "# Folders to process and the corresponding output shapefile names\n",
    "input_dicts = [\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/8000_pixels/20240729/\": \"AGC_2000_global\"},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/8000_pixels/20240729/\": \"BGC_2000_global\"},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/8000_pixels/20240729/\": \"deadwood_C_2000_global\"},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/8000_pixels/20240729/\": \"litter_C_2000_global\"}\n",
    "          ]\n",
    "\n",
    "# Make raster footprint shapefiles from output rasters\n",
    "delayed_result = [dask.delayed(make_tile_footprint_shp)(input_dict) for input_dict in input_dicts]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25167e-dd7d-4712-8139-fe669dda85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Create 10x10 degree rasters aggregated from 2x2 degree rasters\n",
    "## Doesn't use much memory. Can be done on 30x 8 GB workers (1 hour). \n",
    "## In this case, it's aggregation of the carbon pool 2000 rasters\n",
    "## 1144 chunks to process took 65 minutes\n",
    "\n",
    "# Folders to process and the corresponding nodata values for the output rasters\n",
    "s3_in_folder_dicts = [\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/8000_pixels/20240729/\": 0},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/8000_pixels/20240729/\": 0},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/8000_pixels/20240729/\": 0},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/8000_pixels/20240729/\": 0}\n",
    "          ]\n",
    "\n",
    "# Creates the list of aggregated 10x10 rasters that will be created (list of dictionaries of input s3 folder and output aggregated raster name.\n",
    "# These are the basis for the tasks.\n",
    "list_of_s3_name_dicts_total = create_list_for_aggregation(s3_in_folder_dicts)\n",
    "\n",
    "# # For testing. Limits the number of output rasters\n",
    "# list_of_s3_name_dicts_total = list_of_s3_name_dicts_total[0:3]  # First 3 tiles\n",
    "# list_of_s3_name_dicts_total = list_of_s3_name_dicts_total[40:41] # 10N_130E; Internal chunks missing and padding needed on right; FID40\n",
    "# list_of_s3_name_dicts_total = list_of_s3_name_dicts_total[0:1]  # 00N_000E\n",
    "# list_of_s3_name_dicts_total = list_of_s3_name_dicts_total[16:17] # 00N_110E \n",
    "# list_of_s3_name_dicts_total = list_of_s3_name_dicts_total[41:42]  # 10S_010E; No padding needed; FID41\n",
    "\n",
    "delayed_result = [dask.delayed(merge_small_tiles_gdal)(s3_name_no_data_dict) for s3_name_no_data_dict in list_of_s3_name_dicts_total]\n",
    "\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c6156-1bf1-4615-b623-4b9833ecb8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Create raster footprint shapefiles from listed rasters\n",
    "## Doesn't use memory. Can be done on 4 GB workers. Only need as many workers as there are folders. \n",
    "# Took 1.2 minutes\n",
    "\n",
    "# Folders to process and the corresponding output shapefile names\n",
    "input_dicts = [\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/\": \"AGC_2000__global__10x10\"},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/\": \"BGC_2000__global__10x10\"},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/\": \"deadwood_C_2000__global__10x10\"},\n",
    "           {\"gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/\": \"litter_C_2000__global__10x10\"}\n",
    "          ]\n",
    "\n",
    "# Make raster footprint shapefiles from output rasters\n",
    "delayed_result = [dask.delayed(make_tile_footprint_shp)(input_dict) for input_dict in input_dicts]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
