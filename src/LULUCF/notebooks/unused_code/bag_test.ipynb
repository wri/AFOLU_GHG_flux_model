{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b792e3c1-cfb5-4e8f-b911-6faaac54ef12",
   "metadata": {},
   "source": [
    "Alternative approach to running model that uses Dask bags for tasks and maps the operation onto them.\n",
    "Doesn't seem to be any faster than the dask.delayed approch I'd been working with, and indeed seemed slower.\n",
    "Many things about this were different, including how data were downloaded. \n",
    "It comes from this very long exchange with ChatGPT: https://chatgpt.com/share/e/484b70f8-2179-4a5a-b034-8cd6076a3830.\n",
    "My original reason for trying this was that the global run using dask.delayed was failing even with 1x1 chunks and I was seeing lots of \n",
    "messages in the Coiled dashboard log about GIL being held too long.\n",
    "So I thought I'd try something different, although it doesn't solve the fundamental issue of the numba function just being\n",
    "giant and taking a lot of time no matter how the rest of the code works. \n",
    "Basically, this helped me conclude that the bottleneck in performance really is the numba function,\n",
    "although something else about the downloading system could be huring performance.\n",
    "Still, when I did head-to-head comparisons of just downloading 1x1 chunks for all inputs so far a 10x10 tile (50N_010E or 00N_110E), \n",
    "the dask.delayed route I'd been working with was generally faster. \n",
    "So, I'm not pursuing this route but I want to keep the code in case something about it useful eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf9f4f-5065-4e14-869e-3cac9880afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First pass at reading chunks from rasters using Dask bags and executing on the Dask bags with a lambda function.\n",
    "### Just reads and prints the chunks.\n",
    "### The number of bags comes from the number of input rasters\n",
    "\n",
    "%%time\n",
    "\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Initialize the S3 filesystem\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            print(f\"Reading from {s3_uri}\")\n",
    "            print(f\"Raster bounds: {src.bounds}\")\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # print(f\"Window: {window}\")\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "            print(f\"Data shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# List of S3 URIs to download\n",
    "s3_uris = [\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__AGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__BGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__litter_C_density_MgC_ha_2000.tif\"\n",
    "    # Add more URIs as needed\n",
    "]\n",
    "\n",
    "# Bounds of the chunk to download\n",
    "# bounds = [50, -10, 120, 60]\n",
    "# bounds = [110, -10, 120, 0]\n",
    "bounds = [10, 49, 11, 50]    # 1x1 deg (50N_010E)\n",
    "\n",
    "# Create a Dask bag from the list of S3 URIs\n",
    "s3_bag = db.from_sequence(s3_uris, npartitions=len(s3_uris))\n",
    "\n",
    "# Apply the download function to each URI in parallel\n",
    "chunks = s3_bag.map(lambda uri: download_chunk(uri, bounds)).compute()\n",
    "\n",
    "# `chunks` now contains the downloaded data for each raster\n",
    "print(chunks)\n",
    "# print(chunks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d17072-6292-4f4a-ac4d-15cb92913488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868d08f-f3d9-4cad-9992-a455b8946281",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experimenting with performing some operation on the chunks: getting the min and max of each chunk\n",
    "\n",
    "%time\n",
    "\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the S3 filesystem\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "    return data\n",
    "\n",
    "# Function to calculate min and max values of the chunk\n",
    "def calculate_min_max(data):\n",
    "    return np.min(data), np.max(data)\n",
    "\n",
    "# Function to download and calculate min and max values\n",
    "def download_and_calculate_min_max(s3_uri, bounds):\n",
    "    data = download_chunk(s3_uri, bounds)\n",
    "    min_val, max_val = calculate_min_max(data)\n",
    "    return s3_uri, min_val, max_val\n",
    "\n",
    "# List of S3 URIs to download\n",
    "s3_uris = [\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__AGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__BGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__litter_C_density_MgC_ha_2000.tif\"\n",
    "    # Add more URIs as needed\n",
    "]\n",
    "\n",
    "# Bounds of the chunk to download (left, bottom, right, top)\n",
    "bounds = [10, 48, 12, 50]    # 1x1 deg (50N_010E)\n",
    "\n",
    "# Create a Dask bag from the list of S3 URIs\n",
    "s3_bag = db.from_sequence(s3_uris, npartitions=len(s3_uris))\n",
    "\n",
    "# Apply the download_and_calculate_min_max function to each URI in parallel\n",
    "results = s3_bag.map(lambda uri: download_and_calculate_min_max(uri, bounds)).compute()\n",
    "\n",
    "# `results` now contains the URI, min, and max values for each raster\n",
    "for uri, min_val, max_val in results:\n",
    "    print(f\"URI: {uri}, Min: {min_val}, Max: {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ced5d-26a2-40dc-8de3-a6169e0bc296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039750bf-6e4c-4f60-82cc-bc5065fe80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of performing a simple arithmetic operation on the chunks in a numba jit-decorated function (not pixel by pixel but on an entire array).\n",
    "### Still uses Dask bags (determined by the number of input files) and a lambda function to iterate on them.\n",
    "\n",
    "%%time\n",
    "\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the S3 filesystem\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "    return data\n",
    "\n",
    "# Function to process each chunk with LULUCF_fluxes\n",
    "@jit(nopython=True)\n",
    "def LULUCF_fluxes(arr):\n",
    "    # Example processing function, simplified\n",
    "    processed_arr = arr * 1.1  # Example operation\n",
    "    return processed_arr\n",
    "\n",
    "def download_and_process_chunk(s3_uri, bounds):\n",
    "    # Download the chunk\n",
    "    data = download_chunk(s3_uri, bounds)\n",
    "    \n",
    "    # Assuming data is float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    # Process the chunk\n",
    "    processed_data = LULUCF_fluxes(data)\n",
    "    \n",
    "    return s3_uri, processed_data\n",
    "\n",
    "# List of S3 URIs to download\n",
    "s3_uris = [\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__AGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__BGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__litter_C_density_MgC_ha_2000.tif\"\n",
    "    # Add more URIs as needed\n",
    "]\n",
    "\n",
    "# Bounds of the chunk to download (left, bottom, right, top)\n",
    "bounds = [10, 49, 11, 50]  # Example bounds\n",
    "\n",
    "# Create a Dask bag from the list of S3 URIs\n",
    "s3_bag = db.from_sequence(s3_uris, npartitions=len(s3_uris))\n",
    "\n",
    "# Apply the download_and_process_chunk function to each URI in parallel\n",
    "processed_chunks = s3_bag.map(lambda uri: download_and_process_chunk(uri, bounds)).compute()\n",
    "\n",
    "# `processed_chunks` now contains the processed data for each raster\n",
    "print(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b22bc-c079-4694-bb54-c06ea9b049f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d2635-0394-484a-a1eb-b2a901ef3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uploads the output rasters to s3 after saving them locally first.\n",
    "### Could not get numpy arrays saved directly to s3-- had to save them locally as rasters first\n",
    "\n",
    "%%time\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "# Initialize the S3 filesystem with appropriate credentials\n",
    "s3 = s3fs.S3FileSystem(anon=False)  # Set anon=False to use AWS credentials\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "            transform = src.window_transform(window)\n",
    "            metadata = src.meta.copy()\n",
    "            metadata.update({\n",
    "                \"height\": window.height,\n",
    "                \"width\": window.width,\n",
    "                \"transform\": transform,\n",
    "                \"compress\": \"lzw\"  # Add compression to reduce file size\n",
    "            })\n",
    "    return data, metadata\n",
    "\n",
    "# Function to process each chunk with LULUCF_fluxes\n",
    "@jit(nopython=True)\n",
    "def LULUCF_fluxes(arr):\n",
    "    # Example processing function, simplified\n",
    "    processed_arr = arr * 1.1  # Example operation\n",
    "    return processed_arr\n",
    "\n",
    "# Function to upload processed data to S3\n",
    "def upload_to_s3(local_path, s3_uri):\n",
    "\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    output_uri = s3_uri.replace(\"outputs\", \"test_out\")\n",
    "    bucket, key = output_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    print(local_path, bucket, key)\n",
    "    s3_client.upload_file(local_path, bucket, key)\n",
    "    os.remove(local_path)  # Clean up the local file\n",
    "\n",
    "# Wrapper function to download, process, and upload a chunk\n",
    "def download_process_upload_chunk(s3_uri, bounds):\n",
    "   \n",
    "    # Download the chunk\n",
    "    data, metadata = download_chunk(s3_uri, bounds)\n",
    "    \n",
    "    # Assuming data is float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    # Process the chunk\n",
    "    processed_data = LULUCF_fluxes(data)\n",
    "    \n",
    "    # Save to a local file\n",
    "    local_output_path = f\"/tmp/{uuid.uuid4()}.tif\"\n",
    "    with rasterio.open(local_output_path, \"w\", **metadata) as dst:\n",
    "        dst.write(processed_data)\n",
    "    \n",
    "    # Upload the processed data\n",
    "    upload_to_s3(local_output_path, s3_uri)\n",
    "\n",
    "    return s3_uri, processed_data.shape\n",
    "\n",
    "# List of S3 URIs to download\n",
    "s3_uris = [\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__AGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__BGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__litter_C_density_MgC_ha_2000.tif\"\n",
    "    # Add more URIs as needed\n",
    "]\n",
    "\n",
    "# Bounds of the chunk to download (left, bottom, right, top)\n",
    "bounds = [10, 49, 11, 50]  # Example bounds\n",
    "\n",
    "# Create a Dask bag from the list of S3 URIs\n",
    "s3_bag = db.from_sequence(s3_uris, npartitions=len(s3_uris))\n",
    "\n",
    "# Apply the download_process_upload_chunk function to each URI in parallel\n",
    "processed_chunks = s3_bag.map(lambda uri: download_process_upload_chunk(uri, bounds)).compute()\n",
    "\n",
    "# `processed_chunks` now contains the URI and the shape of the processed data for each raster\n",
    "for uri, shape in processed_chunks:\n",
    "    print(f\"URI: {uri}, Processed data shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2ce13-dc05-4a17-a6e0-cf699ef22405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de4151-1cda-453d-ad33-e47c70ffb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Changed the tasks in the Dask bag to use chunks within a bounding box rather than the number of input files.\n",
    "### So now you supply a bounding box and chunk size within and that creates the items in the Dask bag. \n",
    "### Also, no longer uses a lambda function (though I don't know why ChatGPT stopped doing that).\n",
    "\n",
    "%%time\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the S3 filesystem with appropriate credentials\n",
    "s3 = s3fs.S3FileSystem(anon=False)  # Set anon=False to use AWS credentials\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "            transform = src.window_transform(window)\n",
    "            metadata = src.meta.copy()\n",
    "            metadata.update({\n",
    "                \"height\": window.height,\n",
    "                \"width\": window.width,\n",
    "                \"transform\": transform,\n",
    "                \"compress\": \"lzw\"  # Add compression to reduce file size\n",
    "            })\n",
    "    return data, metadata\n",
    "\n",
    "# Function to process each chunk with LULUCF_fluxes\n",
    "@jit(nopython=True)\n",
    "def LULUCF_fluxes(arr):\n",
    "    # Example processing function, simplified\n",
    "    processed_arr = arr * 2  # Updated operation\n",
    "    return processed_arr\n",
    "\n",
    "# Function to upload processed data to S3\n",
    "def upload_to_s3(local_path, s3_uri, bounds):\n",
    "    s3_client = boto3.client('s3')  # Ensure client is created in the worker process\n",
    "    \n",
    "    # Extract carbon pool from the URI\n",
    "    carbon_pool = s3_uri.split('/')[-1].split('__')[1]\n",
    "    \n",
    "    # Get today's date\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the output URI\n",
    "    west, south, east, north = bounds\n",
    "    output_uri = s3_uri.replace(\n",
    "        \"outputs\",\n",
    "        \"test_outputs\"\n",
    "        # f\"test_outputs/{carbon_pool}/2000/{today}/{west}_{south}_{east}_{north}__{carbon_pool}_MgC_ha_2000\"\n",
    "    )\n",
    "    bucket, key = output_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    # Upload the file\n",
    "    s3_client.upload_file(local_path, bucket, key)\n",
    "    os.remove(local_path)  # Clean up the local file\n",
    "\n",
    "# Wrapper function to download, process, and upload a chunk\n",
    "def download_process_upload_chunk(s3_uri, bounds):\n",
    "    # Download the chunk\n",
    "    data, metadata = download_chunk(s3_uri, bounds)\n",
    "    \n",
    "    # Assuming data is float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    # Process the chunk\n",
    "    processed_data = LULUCF_fluxes(data)\n",
    "    \n",
    "    # Construct the local output file path with bounding box in the name\n",
    "    west, south, east, north = bounds\n",
    "    local_output_path = f\"/tmp/{west}_{south}_{east}_{north}.tif\"\n",
    "    \n",
    "    # Save to a local file\n",
    "    with rasterio.open(local_output_path, \"w\", **metadata) as dst:\n",
    "        dst.write(processed_data)\n",
    "    \n",
    "    # Upload the processed data\n",
    "    upload_to_s3(local_output_path, s3_uri, bounds)\n",
    "\n",
    "    return s3_uri, processed_data.shape\n",
    "\n",
    "# Function to generate bounding boxes within a specified bounding box with a specified chunk size\n",
    "def generate_chunks_within_bounds(west, south, east, north, chunk_size):\n",
    "    chunks = []\n",
    "    lat = south\n",
    "    while lat < north:\n",
    "        lon = west\n",
    "        while lon < east:\n",
    "            chunk_west = lon\n",
    "            chunk_south = lat\n",
    "            chunk_east = min(lon + chunk_size, east)\n",
    "            chunk_north = min(lat + chunk_size, north)\n",
    "            chunks.append((chunk_west, chunk_south, chunk_east, chunk_north))\n",
    "            lon += chunk_size\n",
    "        lat += chunk_size\n",
    "    return chunks\n",
    "\n",
    "# Specify the bounding box and chunk size\n",
    "# bounding_box = [10, 45 15, 50]  # Updated bounding box\n",
    "bounding_box = [10, 49, 11, 50] \n",
    "chunk_size = 1  # 1x1 degree chunks\n",
    "\n",
    "# Generate chunks within the specified bounding box\n",
    "chunks = generate_chunks_within_bounds(*bounding_box, chunk_size)\n",
    "\n",
    "# List of S3 URIs to download\n",
    "s3_uris = [\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__AGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__BGC_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "    f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/50N_010E__litter_C_density_MgC_ha_2000.tif\"\n",
    "]\n",
    "\n",
    "# Define the function to be used with Dask for each chunk and URI\n",
    "def process_uri_chunk(args):\n",
    "    uri, bounds = args\n",
    "    return download_process_upload_chunk(uri, bounds)\n",
    "\n",
    "# Create a list of (uri, bounds) tuples for each chunk\n",
    "tasks = [(uri, bounds) for uri in s3_uris for bounds in chunks]\n",
    "\n",
    "# Create a Dask bag from the list of tasks\n",
    "s3_bag = db.from_sequence(tasks, npartitions=len(tasks))\n",
    "\n",
    "# Apply the process_uri_chunk function to each task in parallel\n",
    "processed_chunks = s3_bag.map(process_uri_chunk).compute()\n",
    "\n",
    "# `processed_chunks` now contains the URI and the shape of the processed data for each raster\n",
    "for uri, shape in processed_chunks:\n",
    "    print(f\"URI: {uri}, Processed data shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017787f8-2cf3-4a81-bbd6-0d87473510e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56113c4-a17e-41b0-9edc-38eb43cbb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The list of uris is now specified inside download_process_upload_chunk, so all the tiles for each input dataset can be accessed.\n",
    "### This means that each chunk has to identify and access the right tile_id.\n",
    "### Bags are now entirely dependent on the chunks.\n",
    "### The LULUCF_fluxes function operates pixel by pixel using nested for loops, not on input arrays. \n",
    "### Inputs to LULUCF_fluxes are kwargs instead of a fixed list. \n",
    "\n",
    "%%time\n",
    "\n",
    "import os\n",
    "import math\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Initialize the S3 filesystem with appropriate credentials\n",
    "s3 = s3fs.S3FileSystem(anon=False)  # Set anon=False to use AWS credentials\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "            transform = src.window_transform(window)\n",
    "            metadata = src.meta.copy()\n",
    "            metadata.update({\n",
    "                \"height\": window.height,\n",
    "                \"width\": window.width,\n",
    "                \"transform\": transform,\n",
    "                \"compress\": \"lzw\"  # Add compression to reduce file size\n",
    "            })\n",
    "    return data, metadata\n",
    "\n",
    "# Function to process each chunk with LULUCF_fluxes\n",
    "@jit(nopython=True)\n",
    "def LULUCF_fluxes(*arrays):\n",
    "\n",
    "    processed_arr = np.zeros_like(arrays[0])\n",
    "\n",
    "    for row in range(arrays[0].shape[0]):\n",
    "        for col in range(arrays[0].shape[1]):\n",
    "\n",
    "            agc_cell = arrays[0][row, col]\n",
    "            bgc_cell = arrays[1][row, col]\n",
    "            deadwood_cell = arrays[2][row, col]\n",
    "            litter_cell = arrays[3][row, col]\n",
    "\n",
    "            total_c = agc_cell + bgc_cell + deadwood_cell + litter_cell\n",
    "            processed_arr[row, col] = total_c\n",
    "    \n",
    "    return processed_arr\n",
    "\n",
    "# Function to upload processed data to S3\n",
    "def upload_to_s3(local_path, s3_uri, bounds, tile_id):\n",
    "    s3_client = boto3.client('s3')  # Ensure client is created in the worker process\n",
    "    \n",
    "    # Extract carbon pool from the URI\n",
    "    carbon_pool = s3_uri.split('/')[-1].split('__')[1]\n",
    "    \n",
    "    # Get today's date\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the output URI\n",
    "    west, south, east, north = bounds\n",
    "    output_uri = f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/test_outputs/total_C/2000/40000_pixels/{today}/{tile_id}_{west}_{south}_{east}_{north}__{carbon_pool}\"\n",
    "    bucket, key = output_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    # Upload the file\n",
    "    s3_client.upload_file(local_path, bucket, key)\n",
    "    os.remove(local_path)  # Clean up the local file\n",
    "\n",
    "# Returns the encompassing tile_id string in the form YYN/S_XXXE/W based on a coordinate\n",
    "def xy_to_tile_id(top_left_x, top_left_y):\n",
    "    lat_ceil = math.ceil(top_left_y / 10.0) * 10\n",
    "    lng_floor = math.floor(top_left_x / 10.0) * 10\n",
    "    \n",
    "    lng = f\"{str(abs(lng_floor)).zfill(3)}E\" if lng_floor >= 0 else f\"{str(abs(lng_floor)).zfill(3)}W\"\n",
    "    lat = f\"{str(abs(lat_ceil)).zfill(2)}N\" if lat_ceil >= 0 else f\"{str(abs(lat_ceil)).zfill(2)}S\"\n",
    "\n",
    "    return f\"{lat}_{lng}\"\n",
    "\n",
    "# Wrapper function to download, process, and upload a chunk\n",
    "def download_process_upload_chunk(bounds):\n",
    "    west, south, east, north = bounds\n",
    "\n",
    "    tile_id = xy_to_tile_id(west, north)\n",
    "    \n",
    "    s3_uris = [\n",
    "        f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__AGC_density_MgC_ha_2000.tif\",\n",
    "        f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__BGC_density_MgC_ha_2000.tif\",\n",
    "        f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "        f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__litter_C_density_MgC_ha_2000.tif\"\n",
    "    ]\n",
    "    \n",
    "    # Download each chunk and store them\n",
    "    data_list = []\n",
    "    for s3_uri in s3_uris:\n",
    "        data, metadata = download_chunk(s3_uri, bounds)\n",
    "        data_list.append(data.astype(np.float32))\n",
    "\n",
    "    # Sum the arrays using LULUCF_fluxes\n",
    "    processed_data = LULUCF_fluxes(*data_list)\n",
    "    \n",
    "    # Construct the local output file path with bounding box in the name\n",
    "    local_output_path = f\"/tmp/{tile_id}_{west}_{south}_{east}_{north}.tif\"\n",
    "    \n",
    "    # Save to a local file\n",
    "    with rasterio.open(local_output_path, \"w\", **metadata) as dst:\n",
    "        dst.write(processed_data)\n",
    "    \n",
    "    # Upload the processed data\n",
    "    upload_to_s3(local_output_path, s3_uris[0], bounds, tile_id)  # Use the first URI for constructing the output path\n",
    "\n",
    "    return bounds, processed_data.shape\n",
    "\n",
    "# Function to generate bounding boxes within a specified bounding box with a specified chunk size\n",
    "def generate_chunks_within_bounds(west, south, east, north, chunk_size):\n",
    "    chunks = []\n",
    "    lat = south\n",
    "    while lat < north:\n",
    "        lon = west\n",
    "        while lon < east:\n",
    "            chunk_west = lon\n",
    "            chunk_south = lat\n",
    "            chunk_east = min(lon + chunk_size, east)\n",
    "            chunk_north = min(lat + chunk_size, north)\n",
    "            chunks.append((chunk_west, chunk_south, chunk_east, chunk_north))\n",
    "            lon += chunk_size\n",
    "        lat += chunk_size\n",
    "    return chunks\n",
    "\n",
    "# Specify the bounding box and chunk size: west, south, east, north\n",
    "bounding_box = [10, 40, 20, 50]  # 50N_010E\n",
    "bounding_box = [110, -10, 120, 0]  # 00N_110E\n",
    "chunk_size = 1  # 1x1 degree chunks\n",
    "\n",
    "# Generate chunks within the specified bounding box\n",
    "chunks = generate_chunks_within_bounds(*bounding_box, chunk_size)\n",
    "\n",
    "# Define the function to be used with Dask for each chunk\n",
    "def process_chunk(bounds):\n",
    "    return download_process_upload_chunk(bounds)\n",
    "\n",
    "# Create a Dask bag from the list of chunks\n",
    "s3_bag = db.from_sequence(chunks, npartitions=len(chunks))\n",
    "\n",
    "# Apply the process_chunk function to each chunk in parallel\n",
    "processed_chunks = s3_bag.map(process_chunk).compute()\n",
    "\n",
    "# `processed_chunks` now contains the bounds and the shape of the processed data for each raster\n",
    "for bounds, shape in processed_chunks:\n",
    "    print(f\"Bounds: {bounds}, Processed data shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c33e5-bc05-4c46-b0a7-0a9629e135f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419565d5-b839-4c72-86d2-eebfe8d01013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c4863-7d89-4e80-b601-bf19290ffbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Turns downloads into a dictionary where the keys are the inputs' names. \n",
    "### Converts the download dictionary to typed dictionaries that Numba will understand and uses them in the Numba function.\n",
    "### Returns typed dictionaries, which are then saved to raster locally and uploaded to s3\n",
    "\n",
    "import os\n",
    "import math\n",
    "import uuid\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Initialize the S3 filesystem with appropriate credentials\n",
    "s3 = s3fs.S3FileSystem(anon=False)  # Set anon=False to use AWS credentials\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "            transform = src.window_transform(window)\n",
    "            metadata = src.meta.copy()\n",
    "            metadata.update({\n",
    "                \"height\": window.height,\n",
    "                \"width\": window.width,\n",
    "                \"transform\": transform,\n",
    "                \"compress\": \"lzw\"  # Add compression to reduce file size\n",
    "            })\n",
    "    return data, metadata\n",
    "\n",
    "# Function to process each chunk with LULUCF_fluxes\n",
    "@jit(nopython=True)\n",
    "def LULUCF_fluxes_bag(in_dict_uint8, in_dict_int16, in_dict_float32):\n",
    "\n",
    "    # Separate dictionaries for output numpy arrays of each datatype, named by output data type).\n",
    "    # This is because a dictionary in a Numba function cannot have arrays with multiple data types, so each dictionary has to store only one data type,\n",
    "    # just like inputs to the function.\n",
    "    out_dict_float32 = {}\n",
    "\n",
    "    agc_dens_curr_block = in_dict_float32[agc_2000].astype('float32')\n",
    "    bgc_dens_curr_block = in_dict_float32[bgc_2000].astype('float32')\n",
    "    deadwood_c_dens_curr_block = in_dict_float32[deadwood_c_2000].astype('float32')\n",
    "    litter_c_dens_curr_block = in_dict_float32[litter_c_2000].astype('float32')\n",
    "    \n",
    "    processed_arr = np.zeros_like(agc_dens_curr_block).astype('float32')\n",
    "\n",
    "    for row in range(agc_dens_curr_block.shape[0]):\n",
    "        for col in range(agc_dens_curr_block.shape[1]):\n",
    "\n",
    "            agc_cell = agc_dens_curr_block[row, col]\n",
    "            bgc_cell = bgc_dens_curr_block[row, col]\n",
    "            deadwood_cell = deadwood_c_dens_curr_block[row, col]\n",
    "            litter_cell = litter_c_dens_curr_block[row, col]\n",
    "\n",
    "            total_c = agc_cell + bgc_cell + deadwood_cell + litter_cell\n",
    "            processed_arr[row, col] = total_c\n",
    "\n",
    "    out_dict_float32[\"total_C\"] = processed_arr.copy()\n",
    "    \n",
    "    return out_dict_float32\n",
    "\n",
    "# Function to upload processed data to S3\n",
    "def upload_to_s3(local_path, s3_uri, bounds, tile_id):\n",
    "    s3_client = boto3.client('s3')  # Ensure client is created in the worker process\n",
    "    \n",
    "    # Extract carbon pool from the URI\n",
    "    carbon_pool = s3_uri.split('/')[-1].split('__')[1]\n",
    "    \n",
    "    # Get today's date\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the output URI\n",
    "    west, south, east, north = bounds\n",
    "    output_uri = f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/test_outputs/total_C/2000/40000_pixels/{today}/{tile_id}_{west}_{south}_{east}_{north}__{carbon_pool}\"\n",
    "    bucket, key = output_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    # Upload the file\n",
    "    s3_client.upload_file(local_path, bucket, key)\n",
    "    os.remove(local_path)  # Clean up the local file\n",
    "\n",
    "# Returns the encompassing tile_id string in the form YYN/S_XXXE/W based on a coordinate\n",
    "def xy_to_tile_id(top_left_x, top_left_y):\n",
    "    lat_ceil = math.ceil(top_left_y / 10.0) * 10\n",
    "    lng_floor = math.floor(top_left_x / 10.0) * 10\n",
    "    \n",
    "    lng = f\"{str(abs(lng_floor)).zfill(3)}E\" if lng_floor >= 0 else f\"{str(abs(lng_floor)).zfill(3)}W\"\n",
    "    lat = f\"{str(abs(lat_ceil)).zfill(2)}N\" if lat_ceil >= 0 else f\"{str(abs(lat_ceil)).zfill(2)}S\"\n",
    "\n",
    "    return f\"{lat}_{lng}\"\n",
    "\n",
    "# Wrapper function to download, process, and upload a chunk\n",
    "def download_process_upload_chunk(bounds):\n",
    "    west, south, east, north = bounds\n",
    "\n",
    "    bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(west, north)    # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees)\n",
    "    \n",
    "    s3_uris = {\n",
    "        'agc_2000': f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__AGC_density_MgC_ha_2000.tif\",\n",
    "        'bgc_2000': f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__BGC_density_MgC_ha_2000.tif\",\n",
    "        'deadwood_c_2000': f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "        'litter_c_2000': f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__litter_C_density_MgC_ha_2000.tif\"\n",
    "    }\n",
    "\n",
    "    def download_and_process(name, uri):\n",
    "        data, metadata = download_chunk(uri, bounds)\n",
    "        return name, np.squeeze(data.astype(np.float32)), metadata\n",
    "\n",
    "    # Download each chunk in parallel and store them\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(lambda item: download_and_process(*item), s3_uris.items()))\n",
    "\n",
    "    data_dict = {name: data for name, data, metadata in results}\n",
    "    metadata = results[0][2]\n",
    "\n",
    "    typed_dict_uint8, typed_dict_int16, typed_dict_int32, typed_dict_float32 = create_typed_dicts(data_dict)\n",
    "    \n",
    "    # Sum the arrays using LULUCF_fluxes\n",
    "    out_dict_float32 = LULUCF_fluxes_bag(\n",
    "        typed_dict_uint8, typed_dict_int16, typed_dict_float32 \n",
    "    )\n",
    "\n",
    "    out_dict_all_dtypes = {}\n",
    "\n",
    "    # Transfers the dictionaries of numpy arrays for each data type to a new, Pythonic array\n",
    "    # for key, value in out_dict_uint32.items():\n",
    "    #     out_dict_all_dtypes[key] = value\n",
    "\n",
    "    for key, value in out_dict_float32.items():\n",
    "        out_dict_all_dtypes[key] = value\n",
    "\n",
    "    # Clear memory of unneeded arrays\n",
    "    # del out_dict_uint32\n",
    "    del out_dict_float32\n",
    "  \n",
    "    # Construct the local output file path with bounding box in the name\n",
    "    local_output_path = f\"/tmp/{tile_id}_{west}_{south}_{east}_{north}.tif\"\n",
    "    \n",
    "    # Save to a local file\n",
    "    transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "    with rasterio.open(local_output_path, \"w\", driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels, count=1, \n",
    "                               dtype='float32', crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "        dst.write(out_dict_all_dtypes[\"total_C\"][np.newaxis, :, :])\n",
    "    \n",
    "    # Upload the processed data\n",
    "    upload_to_s3(local_output_path, s3_uris['agc_2000'], bounds, tile_id)\n",
    "\n",
    "    return bounds, out_dict_all_dtypes[\"total_C\"].shape\n",
    "\n",
    "# Function to generate bounding boxes within a specified bounding box with a specified chunk size\n",
    "def generate_chunks_within_bounds(west, south, east, north, chunk_size):\n",
    "    chunks = []\n",
    "    lat = south\n",
    "    while lat < north:\n",
    "        lon = west\n",
    "        while lon < east:\n",
    "            chunk_west = lon\n",
    "            chunk_south = lat\n",
    "            chunk_east = min(lon + chunk_size, east)\n",
    "            chunk_north = min(lat + chunk_size, north)\n",
    "            chunks.append((chunk_west, chunk_south, chunk_east, chunk_north))\n",
    "            lon += chunk_size\n",
    "        lat += chunk_size\n",
    "    return chunks\n",
    "\n",
    "# Specify the bounding box and chunk size: west, south, east, north\n",
    "# bounding_box = [10, 40, 20, 50]  # 50N_010E\n",
    "bounding_box = [10, 49, 11, 50]\n",
    "# bounding_box = [10, 49.75, 10.25, 50]\n",
    "chunk_size = 1  # 1x1 degree chunks\n",
    "\n",
    "# Generate chunks within the specified bounding box\n",
    "chunks = generate_chunks_within_bounds(*bounding_box, chunk_size)\n",
    "\n",
    "# Define the function to be used with Dask for each chunk\n",
    "def process_chunk(bounds):\n",
    "    return download_process_upload_chunk(bounds)\n",
    "\n",
    "# Create a Dask bag from the list of chunks\n",
    "s3_bag = db.from_sequence(chunks, npartitions=len(chunks))\n",
    "\n",
    "# Apply the process_chunk function to each chunk in parallel\n",
    "processed_chunks = s3_bag.map(process_chunk).compute()\n",
    "\n",
    "# `processed_chunks` now contains the bounds and the shape of the processed data for each raster\n",
    "for bounds, shape in processed_chunks:\n",
    "    print(f\"Bounds: {bounds}, Processed data shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f507b25-c024-4161-938b-6ff3caddcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate LULUCF fluxes and carbon densities\n",
    "# Operates pixel by pixel, so uses numba (Python compiled to C++).\n",
    "@jit(nopython=True)\n",
    "def LULUCF_fluxes_bag(in_dict_uint8, in_dict_int16, in_dict_float32):\n",
    "\n",
    "    # Separate dictionaries for output numpy arrays of each datatype, named by output data type).\n",
    "    # This is because a dictionary in a Numba function cannot have arrays with multiple data types, so each dictionary has to store only one data type,\n",
    "    # just like inputs to the function.\n",
    "    out_dict_uint32 = {}\n",
    "    out_dict_float32 = {}\n",
    "\n",
    "    end_years = list(range(first_year, last_year+1, interval_years))[1:]\n",
    "    # end_years = [2005, 2010]\n",
    "\n",
    "    agc_dens_curr_block = in_dict_float32[agc_2000].astype('float32')\n",
    "    bgc_dens_curr_block = in_dict_float32[bgc_2000].astype('float32')\n",
    "    deadwood_c_dens_curr_block = in_dict_float32[deadwood_c_2000].astype('float32')\n",
    "    litter_c_dens_curr_block = in_dict_float32[litter_c_2000].astype('float32')\n",
    "    soil_c_dens_curr_block = in_dict_int16[soil_c_2000].astype('float32')\n",
    "\n",
    "    r_s_ratio_block = in_dict_float32[r_s_ratio]\n",
    "\n",
    "    for year in end_years:\n",
    "\n",
    "        # print(year)\n",
    "\n",
    "        # Creates array for each input\n",
    "        LC_prev_block = in_dict_uint8[f\"{land_cover}_{year-interval_years}\"]\n",
    "        LC_curr_block = in_dict_uint8[f\"{land_cover}_{year}\"]\n",
    "        veg_h_prev_block = in_dict_uint8[f\"{vegetation_height}_{year-interval_years}\"]\n",
    "        veg_h_curr_block = in_dict_uint8[f\"{vegetation_height}_{year}\"]\n",
    "        planted_forest_type_block = in_dict_uint8[planted_forest_type_layer]\n",
    "        planted_forest_tree_crop_block = in_dict_uint8[planted_forest_tree_crop_layer]\n",
    "\n",
    "        burned_area_t_4_block = in_dict_uint8[f\"{burned_area}_{year-4}\"]\n",
    "        burned_area_t_3_block = in_dict_uint8[f\"{burned_area}_{year-3}\"]\n",
    "        burned_area_t_2_block = in_dict_uint8[f\"{burned_area}_{year-2}\"]\n",
    "        burned_area_t_1_block = in_dict_uint8[f\"{burned_area}_{year-1}\"]\n",
    "        burned_area_t_block = in_dict_uint8[f\"{burned_area}_{year}\"]\n",
    "\n",
    "        forest_dist_t_4_block = in_dict_uint8[f\"{forest_disturbance}_{year-4}\"]\n",
    "        forest_dist_t_3_block = in_dict_uint8[f\"{forest_disturbance}_{year-3}\"]\n",
    "        forest_dist_t_2_block = in_dict_uint8[f\"{forest_disturbance}_{year-2}\"]\n",
    "        forest_dist_t_1_block = in_dict_uint8[f\"{forest_disturbance}_{year-1}\"]\n",
    "        forest_dist_t_block = in_dict_uint8[f\"{forest_disturbance}_{year}\"]\n",
    "\n",
    "        # Numpy arrays for outputs that don't depend on previous values\n",
    "        state_out = np.zeros(in_dict_float32[agc_2000].shape).astype('uint32') \n",
    "        agc_flux_out_block = np.zeros(in_dict_float32[agc_2000].shape).astype('float32')\n",
    "        bgc_flux_out_block = np.zeros(in_dict_float32[agc_2000].shape).astype('float32')\n",
    "        deadwood_c_flux_out_block = np.zeros(in_dict_float32[agc_2000].shape).astype('float32')\n",
    "        litter_c_flux_out_block = np.zeros(in_dict_float32[agc_2000].shape).astype('float32')\n",
    "\n",
    "        \n",
    "        # Iterates through all pixels in the chunk\n",
    "        for row in range(LC_curr_block.shape[0]):\n",
    "            for col in range(LC_curr_block.shape[1]):\n",
    "                \n",
    "                LC_prev = LC_prev_block[row, col]\n",
    "                LC_curr = LC_curr_block[row, col]\n",
    "                veg_h_prev = veg_h_prev_block[row, col]\n",
    "                veg_h_curr = veg_h_curr_block[row, col]\n",
    "                planted_forest_type = planted_forest_type_block[row, col]\n",
    "                planted_forest_tree_crop = planted_forest_tree_crop_block[row, col]\n",
    "\n",
    "                # Note: Stacking the burned area rasters using ndstack outside the pixel iteration did not work with numba.\n",
    "                # So just reading each burned area raster separately.\n",
    "                burned_area_t_4 = burned_area_t_4_block[row, col]\n",
    "                burned_area_t_3 = burned_area_t_3_block[row, col]\n",
    "                burned_area_t_2 = burned_area_t_2_block[row, col]\n",
    "                burned_area_t_1 = burned_area_t_1_block[row, col]\n",
    "                burned_area_t = burned_area_t_block[row, col]\n",
    "                burned_area_last = max([burned_area_t_4, burned_area_t_3, burned_area_t_2, burned_area_t_1, burned_area_t])  # Most recent year with burned area during the interval\n",
    "\n",
    "                forest_dist_t_4 = forest_dist_t_4_block[row, col]\n",
    "                forest_dist_t_3 = forest_dist_t_3_block[row, col]\n",
    "                forest_dist_t_2 = forest_dist_t_2_block[row, col]\n",
    "                forest_dist_t_1 = forest_dist_t_1_block[row, col]\n",
    "                forest_dist_t = forest_dist_t_block[row, col]\n",
    "                forest_dist_last = max([forest_dist_t_4, forest_dist_t_3, forest_dist_t_2, forest_dist_t_1, forest_dist_t])  # Most recent year with forest disturbance during the interval   \n",
    "\n",
    "                agc_dens_curr = agc_dens_curr_block[row, col]\n",
    "                bgc_dens_curr = bgc_dens_curr_block[row, col]\n",
    "                deadwood_c_dens_curr = deadwood_c_dens_curr_block[row, col]\n",
    "                litter_c_dens_curr = litter_c_dens_curr_block[row, col]\n",
    "                soil_c_dens_curr = soil_c_dens_curr_block[row, col]\n",
    "\n",
    "                r_s_ratio_cell = r_s_ratio_block[row, col]\n",
    "\n",
    "                tree_prev = (veg_h_prev >= tree_threshold)\n",
    "                tree_curr = (veg_h_curr >=  tree_threshold)\n",
    "                tall_veg_prev = (((LC_prev >= tree_dry_min_height_code) and (LC_prev <= tree_dry_max_height_code)) or\n",
    "                                        ((LC_prev >= tree_wet_min_height_code) and (LC_prev <= tree_wet_max_height_code)))\n",
    "                tall_veg_curr = (((LC_curr >= tree_dry_min_height_code) and (LC_curr <= tree_dry_max_height_code)) or\n",
    "                                       ((LC_curr >= tree_wet_min_height_code) and (LC_curr <= tree_wet_max_height_code)))\n",
    "                short_med_veg_prev = (((LC_prev >= 2) and (LC_prev <= 26)) or\n",
    "                                       ((LC_prev >= 102) and (LC_prev <= 126)))\n",
    "                short_med_veg_curr = (((LC_curr >= 2) and (LC_curr <= 26)) or\n",
    "                                       ((LC_curr >= 102) and (LC_curr <= 126)))\n",
    "\n",
    "                sig_height_loss_prev_curr = (veg_h_prev-veg_h_curr >= sig_height_loss_threshold) \n",
    "\n",
    "                node = 0\n",
    "                \n",
    "                ### Tree gain\n",
    "                if (not tree_prev) and (tree_curr):                  # Non-tree converted to tree (1)    ##TODO: Include mangrove exception.\n",
    "                    node = accrete_node(node, 1)\n",
    "                    if planted_forest_type == 0:                     # New non-SDPT trees (11)\n",
    "                        node = accrete_node(node, 1)\n",
    "                        if not tall_veg_curr:                        # New trees outside forests (111)\n",
    "                            node = accrete_node(node, 1)\n",
    "                            state_out[row, col] = node\n",
    "                            agc_rf = 2.8\n",
    "                            agc_flux_out_block[row, col] = (agc_rf*interval_years)*-1\n",
    "                            agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]   \n",
    "                            bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                            bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col]   \n",
    "                        else:                                        # New terrestrial natural forest (112)\n",
    "                            node = accrete_node(node, 2)\n",
    "                            state_out[row, col] = node\n",
    "                            agc_rf = 5.6\n",
    "                            agc_flux_out_block[row, col] = (agc_rf*interval_years)*-1\n",
    "                            agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                            bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                            bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                    else:                                            # New SDPT trees (12)\n",
    "                        node = accrete_node(node, 2)\n",
    "                        state_out[row, col] = node\n",
    "                        agc_rf = 10\n",
    "                        agc_flux_out_block[row, col] = (agc_rf*interval_years)*-1\n",
    "                        agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]   \n",
    "                        bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                        bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                \n",
    "                ### Tree loss\n",
    "                elif (tree_prev) and (not tree_curr):                # Tree converted to non-tree (2)    ##TODO: Include forest disturbance condition.  ##TODO: Include mangrove exception.\n",
    "                    node = 2\n",
    "                    if planted_forest_type == 0:                     # Full loss of non-SDPT trees (21)\n",
    "                        node = accrete_node(node, 1)\n",
    "                        if not tall_veg_prev:                        # Full loss of trees outside forests (211)\n",
    "                            node = accrete_node(node, 1)\n",
    "                            if burned_area_last == 0:                # Full loss of trees outside forests without fire (2111)\n",
    "                                node = accrete_node(node, 1)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.8\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            else:                                    # Full loss of trees outside forests with fire (2112)\n",
    "                                node = accrete_node(node, 2)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.6\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                        else:                                        # Full loss of natural forest (212)\n",
    "                            node = accrete_node(node, 2)\n",
    "                            if LC_curr == cropland:                  # Full loss of natural forest converted to cropland (2121)\n",
    "                                node = accrete_node(node, 1)\n",
    "                                if burned_area_last == 0:            # Full loss of natural forest converted to cropland, not burned (21211)\n",
    "                                    node = accrete_node(node, 1)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                                else:                                # Full loss of natural forest converted to cropland, burned (21212)\n",
    "                                    node = accrete_node(node, 2)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            elif short_med_veg_curr:                 # Full loss of natural forest converted to short or medium vegetation (2122)\n",
    "                                node = accrete_node(node, 2)\n",
    "                                if burned_area_last == 0:            # Full loss of natural forest converted to short or medium vegetation, not burned (21221)\n",
    "                                    node = accrete_node(node, 1)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.9\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                                else:                                # Full loss of natural forest converted to short or medium vegetation, burned (21222)\n",
    "                                    node = accrete_node(node, 2)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            elif LC_curr == builtup:                 # Full loss of natural forest converted to builtup (2123)\n",
    "                                node = accrete_node(node, 3)\n",
    "                                if burned_area_last == 0:            # Full loss of natural forest converted to builtup, not burned (21231)\n",
    "                                    node = accrete_node(node, 1)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                                else:                                # Full loss of natural forest converted to builtup, burned (21232)\n",
    "                                    node = accrete_node(node, 2)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            else:                                    # Full loss of natural forest converted to anything else (2124)\n",
    "                                node = accrete_node(node, 4)\n",
    "                                if burned_area_last == 0:            # Full loss of natural forest converted to anything else, not burned (21241)\n",
    "                                    node = accrete_node(node, 1)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                                else:                                # Full loss of natural forest converted to anything else, burned (21242)\n",
    "                                    node = accrete_node(node, 2)\n",
    "                                    state_out[row, col] = node       \n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                    else:                                            # Full loss of SDPT trees (22)\n",
    "                        node = accrete_node(node, 2)\n",
    "                        if LC_curr == cropland:                      # Full loss of SDPT converted to cropland (221)\n",
    "                            node = accrete_node(node, 1)\n",
    "                            if burned_area_last == 0:                # Full loss of SDPT converted to cropland, not burned (2211)\n",
    "                                node = accrete_node(node, 1)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.3\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            else:                                    # Full loss of SPDPT converted to cropland, burned (2212)\n",
    "                                node = accrete_node(node, 2)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.3\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                        elif short_med_veg_curr:                     # Full loss of SDPT converted to short or medium vegetation (222)\n",
    "                            node = accrete_node(node, 2)\n",
    "                            if burned_area_last == 0:                # Full loss of SDPT converted to short or medium vegetation, not burned (2221)\n",
    "                                node = accrete_node(node, 1)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.3\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            else:                                    # Full loss of SDPT converted to short or medium vegetation, burned (2222)\n",
    "                                node = accrete_node(node, 2)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.3\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                        elif LC_curr == builtup:                     # Full loss of SDPT converted to builtup (223)\n",
    "                            node = accrete_node(node, 3)\n",
    "                            if planted_forest_tree_crop == 1:        # Full loss of SDPT planted forest to builtup (2231)\n",
    "                                node = accrete_node(node, 1)\n",
    "                                if burned_area_last == 0:            # Full loss of SDPT planted forest converted to builtup, not burned (22311)\n",
    "                                    node = accrete_node(node, 1)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                                else:                                # Full loss of SDPT planted forest converted to builtup, burned (22312)\n",
    "                                    node = accrete_node(node, 2)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            else:                                    # Full loss of SDPT tree crop to builtup (2232)\n",
    "                                node = accrete_node(node, 2)\n",
    "                                if burned_area_last == 0:            # Full loss of SDPT tree crop converted to builtup, not burned (22321)\n",
    "                                    node = accrete_node(node, 1)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                                else:                                # Full loss of SDPT tree crop converted to builtup, burned (22322)\n",
    "                                    node = accrete_node(node, 2)\n",
    "                                    state_out[row, col] = node         \n",
    "                                    agc_ef = 0.3\n",
    "                                    agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                    agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                    bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                    bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                        else:                                        # Full loss of SDPT converted to anything else (224)\n",
    "                            node = accrete_node(node, 4)\n",
    "                            if burned_area_last == 0:                # Full loss of SDPT converted to builtup, not burned (2241)\n",
    "                                node = accrete_node(node, 1)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.3\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                            else:                                    # Full loss of SDPT converted to builtup, burned (2242)\n",
    "                                node = accrete_node(node, 2)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_ef = 0.3\n",
    "                                agc_flux_out_block[row, col] = (agc_dens_curr * agc_ef)\n",
    "                                agc_dens_curr_block[row, col] = agc_dens_curr - agc_flux_out_block[row, col]\n",
    "                                bgc_flux_out_block[row, col] = float(agc_flux_out_block[row, col]) * r_s_ratio_cell\n",
    "                                bgc_dens_curr_block[row, col] = bgc_dens_curr - bgc_flux_out_block[row, col] \n",
    "                \n",
    "                ### Trees remaining trees\n",
    "                elif (tree_prev) and (tree_curr):                    # Trees remaining trees (3)    ##TODO: Include mangrove exception.\n",
    "                    node = accrete_node(node, 3)\n",
    "                    if forest_dist_last == 0:                        # Trees without stand-replacing disturbances in the last interval (31)\n",
    "                        node = accrete_node(node, 1)\n",
    "                        if planted_forest_type == 0:                 # Non-planted trees without stand-replacing disturbance in the last interval (311)\n",
    "                            node = accrete_node(node, 1)\n",
    "                            if not tall_veg_curr:                    # Trees outside forests without stand-replacing disturbance in the last interval (3111)\n",
    "                                node = accrete_node(node, 1)\n",
    "                                if not sig_height_loss_prev_curr:    # Stable trees outside forests (31111)\n",
    "                                    node = accrete_node(node, 1)\n",
    "                                    state_out[row, col] = node\n",
    "                                    agc_flux_out_block[row, col] = 5.54\n",
    "                                    agc_dens_curr_block[row, col] = 13.59\n",
    "                                    bgc_flux_out_block[row, col] = 2.83\n",
    "                                    bgc_dens_curr_block[row, col] = 7.34\n",
    "                                else:                                # Partially disturbed trees outside forests (31112)\n",
    "                                    node = accrete_node(node, 2)\n",
    "                                    if burned_area_last == 0:        # Partially disturbed trees outside forests without fire (311121)\n",
    "                                        node = accrete_node(node, 1)\n",
    "                                        state_out[row, col] = node\n",
    "                                        agc_flux_out_block[row, col] = 5.54\n",
    "                                        agc_dens_curr_block[row, col] = 13.59\n",
    "                                        bgc_flux_out_block[row, col] = 2.83\n",
    "                                        bgc_dens_curr_block[row, col] = 7.34\n",
    "                                    else:\n",
    "                                        node = accrete_node(node, 2)\n",
    "                                        state_out[row, col] = node\n",
    "                                        agc_flux_out_block[row, col] = 5.54\n",
    "                                        agc_dens_curr_block[row, col] = 13.59\n",
    "                                        bgc_flux_out_block[row, col] = 2.83\n",
    "                                        bgc_dens_curr_block[row, col] = 7.34\n",
    "                            else:                                    # Natural forest without stand-replacing disturbance in the last interval (3112)\n",
    "                                node = accrete_node(node, 2)\n",
    "                                state_out[row, col] = node\n",
    "                                agc_flux_out_block[row, col] = 5.54\n",
    "                                agc_dens_curr_block[row, col] = 13.59\n",
    "                                bgc_flux_out_block[row, col] = 2.83\n",
    "                                bgc_dens_curr_block[row, col] = 7.34\n",
    "                                # if not sig_height_loss_prev_curr:    # Stable natural forest (31121)\n",
    "                                    \n",
    "                                \n",
    "                            \n",
    "                    else:\n",
    "                        state_out[row, col] = 32\n",
    "                        agc_flux_out_block[row, col] = 5.54\n",
    "                        agc_dens_curr_block[row, col] = 13.59\n",
    "                        bgc_flux_out_block[row, col] = 2.83\n",
    "                        bgc_dens_curr_block[row, col] = 7.34\n",
    "                    \n",
    "                else:                                                # Not covered in above branches\n",
    "                    state_out[row, col] = 4000000000                 # High value for uint32\n",
    "        \n",
    "        # Adds the output arrays to the dictionary with the appropriate data type\n",
    "        # Outputs need .copy() so that previous intervals' arrays in dicationary aren't overwritten because arrays in dictionaries are mutable (courtesy of ChatGPT).\n",
    "        year_range = f\"{year-interval_years}_{year}\"\n",
    "        out_dict_uint32[f\"{land_state_pattern}_{year_range}\"] = state_out.copy()  \n",
    "        out_dict_float32[f\"{agc_dens_pattern}_{year_range}\"] = agc_dens_curr_block.copy()\n",
    "        out_dict_float32[f\"{bgc_dens_pattern}_{year_range}\"] = bgc_dens_curr_block.copy()\n",
    "        out_dict_float32[f\"{deadwood_c_dens_pattern}_{year_range}\"] = deadwood_c_dens_curr_block.copy()\n",
    "        out_dict_float32[f\"{litter_c_dens_pattern}_{year_range}\"] = litter_c_dens_curr_block.copy()\n",
    "        \n",
    "        out_dict_float32[f\"{agc_flux_pattern}_{year_range}\"] = agc_flux_out_block.copy()\n",
    "        out_dict_float32[f\"{bgc_flux_pattern}_{year_range}\"] = bgc_flux_out_block.copy()\n",
    "        out_dict_float32[f\"{deadwood_c_flux_pattern}_{year_range}\"] = deadwood_c_flux_out_block.copy()\n",
    "        out_dict_float32[f\"{litter_c_flux_pattern}_{year_range}\"] = litter_c_flux_out_block.copy()\n",
    "\n",
    "    return out_dict_uint32, out_dict_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eaa970-6140-4b7d-9c8f-85c5b66b5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Turns downloads into a dictionary where the keys are the inputs' names. \n",
    "### Converts the download dictionary to typed dictionaries that Numba will understand and uses them in the Numba function.\n",
    "### Returns typed dictionaries, which are then saved to raster locally and uploaded to s3\n",
    "\n",
    "import os\n",
    "import math\n",
    "import uuid\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import s3fs\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Initialize the S3 filesystem with appropriate credentials\n",
    "s3 = s3fs.S3FileSystem(anon=False)  # Set anon=False to use AWS credentials\n",
    "\n",
    "# Function to download a chunk from S3 based on bounds\n",
    "def download_chunk(s3_uri, bounds):\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(s3_uri) as src:\n",
    "            # Calculate window from bounds\n",
    "            window = from_bounds(*bounds, transform=src.transform)\n",
    "            # Read the windowed data\n",
    "            data = src.read(window=window)\n",
    "            transform = src.window_transform(window)\n",
    "            metadata = src.meta.copy()\n",
    "            metadata.update({\n",
    "                \"height\": window.height,\n",
    "                \"width\": window.width,\n",
    "                \"transform\": transform,\n",
    "                \"compress\": \"lzw\"  # Add compression to reduce file size\n",
    "            })\n",
    "    return data, metadata\n",
    "\n",
    "# # Function to process each chunk with LULUCF_fluxes\n",
    "# @jit(nopython=True)\n",
    "# def LULUCF_fluxes_bag(in_dict_uint8, in_dict_int16, in_dict_float32):\n",
    "\n",
    "#     # Separate dictionaries for output numpy arrays of each datatype, named by output data type).\n",
    "#     # This is because a dictionary in a Numba function cannot have arrays with multiple data types, so each dictionary has to store only one data type,\n",
    "#     # just like inputs to the function.\n",
    "#     out_dict_float32 = {}\n",
    "\n",
    "#     agc_dens_curr_block = in_dict_float32[agc_2000].astype('float32')\n",
    "#     bgc_dens_curr_block = in_dict_float32[bgc_2000].astype('float32')\n",
    "#     deadwood_c_dens_curr_block = in_dict_float32[deadwood_c_2000].astype('float32')\n",
    "#     litter_c_dens_curr_block = in_dict_float32[litter_c_2000].astype('float32')\n",
    "    \n",
    "#     processed_arr = np.zeros_like(agc_dens_curr_block).astype('float32')\n",
    "\n",
    "#     for row in range(agc_dens_curr_block.shape[0]):\n",
    "#         for col in range(agc_dens_curr_block.shape[1]):\n",
    "\n",
    "#             agc_cell = agc_dens_curr_block[row, col]\n",
    "#             bgc_cell = bgc_dens_curr_block[row, col]\n",
    "#             deadwood_cell = deadwood_c_dens_curr_block[row, col]\n",
    "#             litter_cell = litter_c_dens_curr_block[row, col]\n",
    "\n",
    "#             total_c = agc_cell + bgc_cell + deadwood_cell + litter_cell\n",
    "#             processed_arr[row, col] = total_c\n",
    "\n",
    "#     out_dict_float32[\"total_C\"] = processed_arr.copy()\n",
    "    \n",
    "#     return out_dict_float32\n",
    "\n",
    "# Function to upload processed data to S3\n",
    "def upload_to_s3(local_path, s3_uri, bounds, tile_id):\n",
    "    s3_client = boto3.client('s3')  # Ensure client is created in the worker process\n",
    "    \n",
    "    # Extract carbon pool from the URI\n",
    "    carbon_pool = s3_uri.split('/')[-1].split('__')[1]\n",
    "    \n",
    "    # Get today's date\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    \n",
    "    # Construct the output URI\n",
    "    west, south, east, north = bounds\n",
    "    output_uri = f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/test_outputs/total_C/2000/40000_pixels/{today}/{tile_id}_{west}_{south}_{east}_{north}__{carbon_pool}\"\n",
    "    bucket, key = output_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    # Upload the file\n",
    "    s3_client.upload_file(local_path, bucket, key)\n",
    "    os.remove(local_path)  # Clean up the local file\n",
    "\n",
    "# Returns the encompassing tile_id string in the form YYN/S_XXXE/W based on a coordinate\n",
    "def xy_to_tile_id(top_left_x, top_left_y):\n",
    "    lat_ceil = math.ceil(top_left_y / 10.0) * 10\n",
    "    lng_floor = math.floor(top_left_x / 10.0) * 10\n",
    "    \n",
    "    lng = f\"{str(abs(lng_floor)).zfill(3)}E\" if lng_floor >= 0 else f\"{str(abs(lng_floor)).zfill(3)}W\"\n",
    "    lat = f\"{str(abs(lat_ceil)).zfill(2)}N\" if lat_ceil >= 0 else f\"{str(abs(lat_ceil)).zfill(2)}S\"\n",
    "\n",
    "    return f\"{lat}_{lng}\"\n",
    "\n",
    "# Wrapper function to download, process, and upload a chunk\n",
    "def download_process_upload_chunk(bounds):\n",
    "    west, south, east, north = bounds\n",
    "\n",
    "    bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(west, north)    # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees)\n",
    "    \n",
    "    s3_uris = {\n",
    "        f\"{land_cover}_2000\": f\"{LC_uri}/composite/2000/raw/{tile_id}.tif\",\n",
    "        f\"{land_cover}_2005\": f\"{LC_uri}/composite/2005/raw/{tile_id}.tif\",\n",
    "        f\"{land_cover}_2010\": f\"{LC_uri}/composite/2010/raw/{tile_id}.tif\",\n",
    "        f\"{land_cover}_2015\": f\"{LC_uri}/composite/2015/raw/{tile_id}.tif\",\n",
    "        f\"{land_cover}_2020\": f\"{LC_uri}/composite/2020/raw/{tile_id}.tif\",  \n",
    "\n",
    "        f\"{vegetation_height}_2000\": f\"{LC_uri}/vegetation_height/2000/{tile_id}_vegetation_height_2000.tif\",\n",
    "        f\"{vegetation_height}_2005\": f\"{LC_uri}/vegetation_height/2005/{tile_id}_vegetation_height_2005.tif\",\n",
    "        f\"{vegetation_height}_2010\": f\"{LC_uri}/vegetation_height/2010/{tile_id}_vegetation_height_2010.tif\",\n",
    "        f\"{vegetation_height}_2015\": f\"{LC_uri}/vegetation_height/2015/{tile_id}_vegetation_height_2015.tif\",\n",
    "        f\"{vegetation_height}_2020\": f\"{LC_uri}/vegetation_height/2020/{tile_id}_vegetation_height_2020.tif\",  \n",
    "\n",
    "        agc_2000: f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/AGC_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__AGC_density_MgC_ha_2000.tif\",\n",
    "        bgc_2000: f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/BGC_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__BGC_density_MgC_ha_2000.tif\",\n",
    "        deadwood_c_2000: f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/deadwood_C_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__deadwood_C_density_MgC_ha_2000.tif\",\n",
    "        litter_c_2000: f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/litter_C_density_MgC_ha/2000/40000_pixels/20240729/{tile_id}__litter_C_density_MgC_ha_2000.tif\",\n",
    "        soil_c_2000: f\"s3://gfw2-data/climate/carbon_model/carbon_pools/soil_carbon/intermediate_full_extent/standard/20231108/{tile_id}_soil_C_full_extent_2000_Mg_C_ha.tif\",\n",
    "\n",
    "        r_s_ratio: f\"{r_s_ratio_path}{tile_id}_{r_s_ratio_pattern}.tif\",\n",
    "\n",
    "        # \"drivers\": f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/tree_cover_loss_drivers/processed/drivers_2022/20230407/{tile_id}_tree_cover_loss_driver_processed.tif\",\n",
    "        planted_forest_type_layer: f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/plantation_type/SDPTv2/20230911/{tile_id}_plantation_type_oilpalm_woodfiber_other.tif\", # Originally from gfw-data-lake, so it's in 400x400 windows\n",
    "        planted_forest_tree_crop_layer: f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/plantation_simpleType__planted_forest_tree_crop/SDPTv2/20230911/{tile_id}.tif\"  # Originally from gfw-data-lake, so it's in 400x400 windows\n",
    "        # \"peat\": f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/peatlands/processed/20230315/{tile_id}_peat_mask_processed.tif\",\n",
    "        # \"ecozone\": f\"s3://gfw2-data/fao_ecozones/v2000/raster/epsg-4326/10/40000/class/gdal-geotiff/{tile_id}.tif\",   # Originally from gfw-data-lake, so it's in 400x400 windows \n",
    "        # \"iso\": f\"s3://gfw2-data/gadm_administrative_boundaries/v3.6/raster/epsg-4326/10/40000/adm0/gdal-geotiff/{tile_id}.tif\",  # Originally from gfw-data-lake, so it's in 400x400 windows\n",
    "        # \"ifl_primary\": f\"s3://gfw2-data/climate/carbon_model/ifl_primary_merged/processed/20200724/{tile_id}_ifl_2000_primary_2001_merged.tif\"\n",
    "    }\n",
    "\n",
    "    # for year in range(first_year, last_year+1):     # Annual burned area maps start in 2000\n",
    "    for year in range(2000, 2021):     # Annual burned area maps start in 2000\n",
    "        s3_uris[f\"{burned_area}_{year}\"] = f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/burn_year/burn_year_10x10_clip/ba_{year}_{tile_id}.tif\"    \n",
    "\n",
    "    # for year in range(first_year+1, last_year+1):     # Annual forest disturbance maps start in 2001 and ends in 2020\n",
    "    for year in range(2001, 2021):     # Annual forest disturbance maps start in 2001 and ends in 2020\n",
    "        s3_uris[f\"{forest_disturbance}_{year}\"] = f\"{LC_uri}/annual_forest_disturbance/raw/{year}_{tile_id}.tif\"  \n",
    "\n",
    "    print(\"Downloading data\")\n",
    "    \n",
    "    def download_and_process(name, uri):\n",
    "        data, metadata = download_chunk(uri, bounds)\n",
    "        return name, np.squeeze(data), metadata\n",
    "\n",
    "    # Download each chunk in parallel and store them\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(lambda item: download_and_process(*item), s3_uris.items()))\n",
    "\n",
    "    print(\"Done downloading\")\n",
    "\n",
    "    data_dict = {name: data for name, data, metadata in results}\n",
    "    metadata = results[0][2]\n",
    "\n",
    "    typed_dict_uint8, typed_dict_int16, typed_dict_int32, typed_dict_float32 = create_typed_dicts(data_dict)\n",
    "\n",
    "    print(\"Running decision tree\")\n",
    "    \n",
    "    # Sum the arrays using LULUCF_fluxes\n",
    "    out_dict_uint32, out_dict_float32 = LULUCF_fluxes_bag(\n",
    "        typed_dict_uint8, typed_dict_int16, typed_dict_float32 \n",
    "    )\n",
    "\n",
    "    out_dict_all_dtypes = {}\n",
    "\n",
    "    # Transfers the dictionaries of numpy arrays for each data type to a new, Pythonic array\n",
    "    for key, value in out_dict_uint32.items():\n",
    "        out_dict_all_dtypes[key] = value\n",
    "\n",
    "    for key, value in out_dict_float32.items():\n",
    "        out_dict_all_dtypes[key] = value\n",
    "\n",
    "    # Clear memory of unneeded arrays\n",
    "    del out_dict_uint32\n",
    "    del out_dict_float32\n",
    "  \n",
    "    # Construct the local output file path with bounding box in the name\n",
    "    local_output_path = f\"/tmp/{tile_id}_{west}_{south}_{east}_{north}.tif\"\n",
    "\n",
    "    print(\"Saving and uploading data\")\n",
    "    \n",
    "    # # Save to a local file\n",
    "    # transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "    # with rasterio.open(local_output_path, \"w\", driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels, count=1, \n",
    "    #                            dtype='float32', crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "    #     dst.write(out_dict_all_dtypes[\"total_C\"][np.newaxis, :, :])\n",
    "    \n",
    "    # # Upload the processed data\n",
    "    # upload_to_s3(local_output_path, s3_uris[agc_2000], bounds, tile_id)\n",
    "\n",
    "    # return\n",
    "    # # return bounds, out_dict_all_dtypes[\"total_C\"].shape\n",
    "\n",
    "# Function to generate bounding boxes within a specified bounding box with a specified chunk size\n",
    "def generate_chunks_within_bounds(west, south, east, north, chunk_size):\n",
    "    chunks = []\n",
    "    lat = south\n",
    "    while lat < north:\n",
    "        lon = west\n",
    "        while lon < east:\n",
    "            chunk_west = lon\n",
    "            chunk_south = lat\n",
    "            chunk_east = min(lon + chunk_size, east)\n",
    "            chunk_north = min(lat + chunk_size, north)\n",
    "            chunks.append((chunk_west, chunk_south, chunk_east, chunk_north))\n",
    "            lon += chunk_size\n",
    "        lat += chunk_size\n",
    "    return chunks\n",
    "\n",
    "# Specify the bounding box and chunk size: west, south, east, north\n",
    "bounding_box = [10, 40, 20, 50]  # 50N_010E\n",
    "chunk_size = 2\n",
    "\n",
    "# bounding_box = [10, 40, 20, 50]  # 50N_010E\n",
    "# bounding_box = [10, 49, 11, 50]\n",
    "# chunk_size = 1  # 1x1 degree chunks\n",
    "\n",
    "# bounding_box = [10, 49.75, 10.25, 50]\n",
    "# chunk_size = 0.25\n",
    "\n",
    "# Generate chunks within the specified bounding box\n",
    "chunks = generate_chunks_within_bounds(*bounding_box, chunk_size)\n",
    "\n",
    "# Define the function to be used with Dask for each chunk\n",
    "def process_chunk(bounds):\n",
    "    return download_process_upload_chunk(bounds)\n",
    "\n",
    "# Create a Dask bag from the list of chunks\n",
    "s3_bag = db.from_sequence(chunks, npartitions=len(chunks))\n",
    "\n",
    "# Apply the process_chunk function to each chunk in parallel\n",
    "processed_chunks = s3_bag.map(process_chunk).compute()\n",
    "\n",
    "# # `processed_chunks` now contains the bounds and the shape of the processed data for each raster\n",
    "# for bounds, shape in processed_chunks:\n",
    "#     print(f\"Bounds: {bounds}, Processed data shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68136cb-1365-4a45-b6c8-7c4466750be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
