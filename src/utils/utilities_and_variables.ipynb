{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507961e7-ae8a-471f-8c1c-5d99a2bf9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import concurrent.futures\n",
    "from osgeo import gdal\n",
    "\n",
    "# dask/parallelization libraries\n",
    "import coiled\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.distributed import print\n",
    "import distributed\n",
    "\n",
    "# scipy basics\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.transform\n",
    "import rasterio.windows\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "from rioxarray.merge import merge_arrays\n",
    "\n",
    "# numba\n",
    "from numba import jit\n",
    "from numba.typed import Dict\n",
    "from numba.core import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d2f34-be9b-49c5-8b20-eb37ae1b668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General paths and constants\n",
    "\n",
    "LC_uri = 's3://gfw2-data/landcover'\n",
    "\n",
    "s3_out_dir = 'climate/AFOLU_flux_model/LULUCF/outputs'\n",
    "\n",
    "IPCC_class_max_val = 6\n",
    "\n",
    "# IPCC codes\n",
    "forest = 1\n",
    "cropland = 2\n",
    "settlement = 3\n",
    "wetland = 4\n",
    "grassland = 5\n",
    "otherland = 6\n",
    "\n",
    "first_year = 2000\n",
    "last_year = 2020\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('gfw2-data')\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "full_raster_dims = 40000\n",
    "\n",
    "interval_years = 5   # number of years in interval. #TODO: calculate programmatically in numba function rather than coded here-- for greater flexibility.\n",
    "\n",
    "sig_height_loss_threshold = 5   # meters\n",
    "\n",
    "biomass_to_carbon = 0.47   # Conversion of biomass to carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41d62f-cb21-46b7-9350-fc5e6cff5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLCLU codes\n",
    "cropland = 244\n",
    "builtup = 250\n",
    "\n",
    "tree_wet_min_height_code = 27\n",
    "tree_wet_max_height_code = 48\n",
    "tree_dry_min_height_code = 127\n",
    "tree_dry_max_height_code = 148\n",
    "\n",
    "tree_threshold = 5   # Height minimum for trees (meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49015c3a-d497-48e8-a782-ff5dab4cb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name patterns\n",
    "\n",
    "### IPCC classes and change\n",
    "IPCC_class_path = \"IPCC_basic_classes\"\n",
    "IPCC_class_pattern = \"IPCC_classes\"\n",
    "IPCC_change_path = \"IPCC_basic_change\"\n",
    "IPCC_change_pattern = \"IPCC_change\"\n",
    "\n",
    "land_state_pattern = \"land_state_node\"\n",
    "agc_dens_pattern = \"AGC_density_MgC_ha\"\n",
    "bgc_dens_pattern = \"BGC_density_MgC_ha\"\n",
    "agc_flux_pattern = \"AGC_flux_MgC_ha\"\n",
    "bgc_flux_pattern = \"BGC_flux_MgC_ha\"\n",
    "\n",
    "land_cover = \"land_cover\"\n",
    "vegetation_height = \"vegetation_height\"\n",
    "\n",
    "agb_2000 = \"agb_2000\"\n",
    "agc_2000 = \"agc_2000\"\n",
    "bgc_2000 = \"bgc_2000\"\n",
    "deadwood_c_2000 = \"deadwood_c_2000\"\n",
    "litter_c_2000 = \"litter_c_2000\"\n",
    "soil_c_2000 = \"soil_c_2000\"\n",
    "\n",
    "r_s_ratio = \"r_s_ratio\"\n",
    "\n",
    "burned_area = \"burned_area\"\n",
    "forest_disturbance = \"forest_disturbance\"\n",
    "\n",
    "planted_forest_type_layer = \"planted_forest_type\"\n",
    "planted_forest_tree_crop_layer = \"planted_forest_tree_crop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09c09b-b2b2-45d0-a27c-f0a11729a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestr():\n",
    "    return time.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "\n",
    "def boundstr(bounds):\n",
    "    bounds_str = \"_\".join([str(round(x)) for x in bounds])\n",
    "    return bounds_str\n",
    "\n",
    "def calc_chunk_length_pixels(bounds):\n",
    "    chunk_length_pixels = int((bounds[3]-bounds[1]) * (40000/10))\n",
    "    return chunk_length_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960e0d5-c59d-4404-8d21-680201123ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the W, S, E, N bounds of a 10x10 degree tile\n",
    "def get_10x10_tile_bounds(tile_id):\n",
    "    \n",
    "    if \"S\" in tile_id:\n",
    "        max_y = -1 * (int(tile_id[:2]))\n",
    "        min_y = -1 * (int(tile_id[:2])+10)\n",
    "    else: \n",
    "        max_y = (int(tile_id[:2]))\n",
    "        min_y = (int(tile_id[:2])-10)\n",
    "\n",
    "    if \"W\" in tile_id:\n",
    "        max_x = -1 * (int(tile_id[4:7])-10)\n",
    "        min_x = -1 * (int(tile_id[4:7]))\n",
    "    else: \n",
    "        max_x = (int(tile_id[4:7])+10)\n",
    "        min_x = (int(tile_id[4:7]))\n",
    "\n",
    "    return min_x, min_y, max_x, max_y      # W, S, E, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fe564-8331-499f-88c5-0d266ceec36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of all chunk boundaries within a bounding box for chunks of a given size\n",
    "def get_chunk_bounds(chunk_params):\n",
    "\n",
    "    min_x = chunk_params[0]\n",
    "    min_y = chunk_params[1]\n",
    "    max_x = chunk_params[2]\n",
    "    max_y = chunk_params[3]\n",
    "    chunk_size = chunk_params[4]\n",
    "    \n",
    "    x, y = (min_x, min_y)\n",
    "    chunks = []\n",
    "\n",
    "    # Polygon Size\n",
    "    while y < max_y:\n",
    "        while x < max_x:\n",
    "            bounds = [\n",
    "                x,\n",
    "                y,\n",
    "                x + chunk_size,\n",
    "                y + chunk_size,\n",
    "            ]\n",
    "            chunks.append(bounds)\n",
    "            x += chunk_size\n",
    "        x = min_x\n",
    "        y += chunk_size\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d7d66-b1e0-44be-b497-c5342eb38b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the encompassing tile_id string in the form YYN/S_XXXE/W based on a coordinate\n",
    "def xy_to_tile_id(top_left_x, top_left_y):\n",
    "\n",
    "    lat_ceil = math.ceil(top_left_y/10.0) * 10\n",
    "    lng_floor = math.floor(top_left_x/10.0) * 10\n",
    "    \n",
    "    lng: str = f\"{str(lng_floor).zfill(3)}E\" if (lng_floor >= 0) else f\"{str(-lng_floor).zfill(3)}W\"\n",
    "    lat: str = f\"{str(lat_ceil).zfill(2)}N\" if (lat_ceil >= 0) else f\"{str(-lat_ceil).zfill(2)}S\"\n",
    "\n",
    "    return f\"{lat}_{lng}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286e357-46f8-4870-8d0d-5899e830a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazily opens tile within provided bounds (i.e. one chunk) and returns as a numpy array\n",
    "# If it can't open the chunk (no data in it), it returns an array of the specified nodata value.  \n",
    "def get_tile_dataset_rio(uri, bounds, chunk_length_pixels, no_data_val):\n",
    "\n",
    "    bounds_str = boundstr(bounds)\n",
    "\n",
    "    # If the uri exists, the relevant window is opened and returned and returned as an array\n",
    "    try:\n",
    "        with rasterio.open(uri) as ds:\n",
    "            window = rasterio.windows.from_bounds(*bounds, ds.transform)\n",
    "            data = ds.read(1, window=window)\n",
    "    \n",
    "    # If the uri does not exist, an array of the correct data type and size is created and returned based on a tile that is known to exist\n",
    "    except:\n",
    "        uri_revised = re.sub(\"[0-9]{2}[A-Z][_][0-9]{3}[A-Z]\", \"00N_110E\", uri)   # Substitutes the tile_id for the tile that doesn't exist with a tile_id that does exist\n",
    "        with rasterio.open(uri_revised) as ds:\n",
    "            data_type = ds.dtypes[0]   # Retrieves the datatype of the tile\n",
    "        data = np.full((chunk_length_pixels, chunk_length_pixels), no_data_val).astype(data_type)  # Array of the right size and datatype\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964ddc1-4e81-45f1-ba86-524f20446b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares list of chunks to download.\n",
    "# Chunks are defined by a bounding box.\n",
    "def prepare_to_download_chunk(bounds, download_dict, no_data_val):\n",
    " \n",
    "    futures = {}\n",
    "    \n",
    "    bounds_str = boundstr(bounds)\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)\n",
    "\n",
    "    # Submit requests to S3 for input chunks but don't actually download them yet. This queueing of the requests before downloading them speeds up the downloading\n",
    "    # Approach is to download all the input chunks up front for every year to make downloading more efficient, even though it means storing more upfront\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        print(f\"Requesting data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "\n",
    "        for key, value in download_dict.items():\n",
    "            futures[executor.submit(get_tile_dataset_rio, value, bounds, chunk_length_pixels, no_data_val)] = key\n",
    "\n",
    "    return futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebaa46-373f-42d3-ab2e-c660e97b96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if tiles exist at all\n",
    "def check_for_tile(download_dict, is_final):\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while i < len(list(download_dict.values())):\n",
    "\n",
    "        s3_key = list(download_dict.values())[i][15:]\n",
    "\n",
    "        # Breaks the loop if the tile exists\n",
    "        try:\n",
    "            s3.head_object(Bucket='gfw2-data', Key=s3_key)\n",
    "            if not is_final:\n",
    "                print(f\"Tile id {list(download_dict.values())[i][-12:-4]} exists. Proceeding.\")\n",
    "            return 1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    print(f\"Tile id {list(download_dict.values())[0][-12:-4]} does not exist. Skipping chunk.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbdbf8-4c4a-4c13-8d2d-36b70e4c7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks whether a chunk has data in it\n",
    "def check_chunk_for_data(layers, item_to_check, bounds_str, tile_id, no_data_val, is_final):\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while i < len(list(layers.values())):\n",
    "\n",
    "        # Checks if all the pixels have the nodata value\n",
    "        min = np.min(list(layers.values())[i])  # Can't use np.all because it doesn't work in chunks that are mostly water; says nodata in chunk even if there is land\n",
    "\n",
    "        # Breaks the loop if there is data in the chunk; don't need to keep checking chunk for data\n",
    "        if min < no_data_val:\n",
    "            if not is_final:\n",
    "                print(f\"Data in chunk {bounds_str}. Proceeding.\")\n",
    "            return 1\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    print(f\"No data in chunk {bounds_str} for any input.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ec6ea-d2a6-4a78-89ee-ec2c2bbdd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves array as a raster locally, then uploads it to s3\n",
    "def save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, output_dict, is_final):\n",
    "\n",
    "    s3_client = boto3.client(\"s3\") # Needs to be in the same function as the upload_file call\n",
    "\n",
    "    transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "\n",
    "    file_info = f'{tile_id}__{bounds_str}'\n",
    "\n",
    "    # For every output file, saves from array to local raster, then to s3.\n",
    "    # Can't save directly to s3, unfortunately, so need to save locally first.\n",
    "    for key, value in output_dict.items():\n",
    "\n",
    "        data_array = value[0]\n",
    "        data_type = value[1]\n",
    "        data_meaning = value[2]\n",
    "        year_out = value[3]\n",
    "\n",
    "        if not is_final:\n",
    "            print(f\"Saving {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "        if is_final:\n",
    "            file_name = f\"{file_info}__{key}.tif\"\n",
    "        else:\n",
    "            file_name = f\"{file_info}__{key}__{timestr()}.tif\"\n",
    "\n",
    "        with rasterio.open(f\"/tmp/{file_name}\", 'w', driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels, count=1, \n",
    "                           dtype=data_type, crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "            dst.write(data_array, 1)\n",
    "\n",
    "        s3_path = f\"{s3_out_dir}/{data_meaning}/{year_out}/{chunk_length_pixels}_pixels/{time.strftime('%Y%m%d')}\"\n",
    "\n",
    "        if not is_final:\n",
    "            print(f\"Uploading {bounds_str} in {tile_id} for {year_out} to {s3_path}: {timestr()}\")\n",
    "\n",
    "        s3_client.upload_file(f\"/tmp/{file_name}\", \"gfw2-data\", Key=f\"{s3_path}/{file_name}\")\n",
    "\n",
    "        # Deletes the local raster\n",
    "        os.remove(f\"/tmp/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed478c6f-2004-4bb8-b03e-6c0345e95b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists rasters in an s3 folder and returns their names as a list\n",
    "def list_rasters_in_folder(full_in_folder):\n",
    "\n",
    "    cmd = ['aws', 's3', 'ls', full_in_folder]\n",
    "    s3_contents_bytes = subprocess.check_output(cmd)\n",
    "\n",
    "    # Converts subprocess results to useful string\n",
    "    s3_contents_str = s3_contents_bytes.decode('utf-8')\n",
    "    s3_contents_list = s3_contents_str.splitlines()\n",
    "    rasters = [line.split()[-1] for line in s3_contents_list]\n",
    "    rasters = [i for i in rasters if \"tif\" in i]\n",
    "\n",
    "    return rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756ce3e-13de-4648-acb4-3a8331fc947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads a shapefile to s3\n",
    "def upload_shp(full_in_folder, in_folder, shp):\n",
    "\n",
    "    print(f\"Uploading to {full_in_folder}{shp}: {timestr()}\")\n",
    "\n",
    "    shp_pattern = shp[:-4]\n",
    "\n",
    "    s3_client = boto3.client(\"s3\")  # Needs to be in the same function as the upload_file call\n",
    "    s3_client.upload_file(f\"/tmp/{shp}\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp}\")\n",
    "    s3_client.upload_file(f\"/tmp/{shp_pattern}.dbf\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp_pattern}.dbf\")\n",
    "    s3_client.upload_file(f\"/tmp/{shp_pattern}.prj\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp_pattern}.prj\")\n",
    "    s3_client.upload_file(f\"/tmp/{shp_pattern}.shx\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp_pattern}.shx\")\n",
    "\n",
    "    os.remove(f\"/tmp/{shp}\")\n",
    "    os.remove(f\"/tmp/{shp_pattern}.dbf\")\n",
    "    os.remove(f\"/tmp/{shp_pattern}.prj\")\n",
    "    os.remove(f\"/tmp/{shp_pattern}.shx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e9847-57ff-4a16-8398-040434e65f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a shapefile of the footprints of rasters in a folder, for checking geographical completeness of rasters\n",
    "def make_tile_footprint_shp(input_dict):\n",
    "\n",
    "    in_folder = list(input_dict.keys())[0]\n",
    "    pattern = list(input_dict.values())[0]\n",
    "\n",
    "    # Task properties\n",
    "    print(f\"Making tile index shapefile for: {in_folder}: {timestr()}\")\n",
    "\n",
    "    # Folder including s3 key\n",
    "    s3_in_folder = f's3://{in_folder}'\n",
    "    vsis3_in_folder = f'/vsis3/{in_folder}'\n",
    "\n",
    "    # List of all the filenames in the folder\n",
    "    filenames = list_rasters_in_folder(s3_in_folder)\n",
    "\n",
    "    # List of the tile paths in the folder\n",
    "    tile_paths = []\n",
    "    tile_paths = [vsis3_in_folder + filename for filename in filenames]\n",
    "\n",
    "    file_paths = 's3_paths.txt'\n",
    "\n",
    "    with open(f\"/tmp/{file_paths}\", 'w') as file:\n",
    "        for item in tile_paths:\n",
    "            file.write(item + '\\n')\n",
    "\n",
    "    # Output shapefile name\n",
    "    shp = f\"raster_footprints_{pattern}.shp\"\n",
    "\n",
    "    cmd = [\"gdaltindex\", \"-t_srs\", \"EPSG:4326\", f\"/tmp/{shp}\", \"--optfile\", f\"/tmp/{file_paths}\"]\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "    # Uploads shapefile to s3\n",
    "    upload_shp(s3_in_folder, in_folder, shp)\n",
    "\n",
    "    return(f\"Completed: {timestr()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76b6cb-43c8-4ab2-b896-32547c99adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an xarray data array locally as a raster and then uploads it to s3\n",
    "def save_and_upload_raster_10x10(**kwargs):\n",
    "\n",
    "    s3_client = boto3.client(\"s3\") # Needs to be in the same function as the upload_file call\n",
    "\n",
    "    data_array = kwargs['data']   # The data being saved\n",
    "    out_file_name = kwargs['out_file_name']   # The output file name\n",
    "    out_folder = kwargs['out_folder']   # The output folder\n",
    "\n",
    "    print(f\"Saving {out_file_name} locally\")\n",
    "\n",
    "    profile_kwargs = {'compress': 'lzw'}   # Adds attribute to compress the output raster \n",
    "    # data_array.rio.to_raster(f\"{out_file_name}\", **profile_kwargs)\n",
    "    data_array.rio.to_raster(f\"/tmp/{out_file_name}\", **profile_kwargs)\n",
    "\n",
    "    print(f\"Saving {out_file_name} to {out_folder[10:]}{out_file_name}\")\n",
    "\n",
    "    s3_client.upload_file(f\"/tmp/{out_file_name}\", \"gfw2-data\", Key=f\"{out_folder[10:]}{out_file_name}\")\n",
    "\n",
    "    # Deletes the local raster\n",
    "    os.remove(f\"/tmp/{out_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c3c02-797c-4262-826b-3bb5624ed29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of 2x2 deg tiles to aggregate into 10x10 deg tiles, where the list is a list of dictionaries of the form \n",
    "# [{'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_000E__IPCC_classes_2020.tif', 0]}, \n",
    "# {'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_010E__IPCC_classes_2020.tif', 0]}, ... ]\n",
    "def create_list_for_aggregation(s3_in_folder_dicts):\n",
    "\n",
    "    s3_in_folders = [list(item.keys())[0] for item in s3_in_folder_dicts]\n",
    "    no_data_values = [list(item.values())[0] for item in s3_in_folder_dicts]\n",
    "\n",
    "    list_of_s3_name_dicts_total = []   # Final list of dictionaries of s3 paths and output aggregated 10x10 rasters\n",
    "    \n",
    "    # Iterates through all the desired s3 folders\n",
    "    for s3_in_folder, no_data_value in zip(s3_in_folders, no_data_values):\n",
    "    \n",
    "        simple_file_names = []   # List of output aggregatd 10x10 rasters\n",
    "    \n",
    "        # Raw filenames in a folder, e.g., ['00N_000E__6_-2_8_0__IPCC_classes_2020.tif', '00N_000E__6_-4_8_-2__IPCC_classes_2020.tif',...]\n",
    "        filenames = list_rasters_in_folder(f\"s3://{s3_in_folder}\")\n",
    "    \n",
    "        # Iterates through all the files in a folder and converts them to the output names. \n",
    "        # Essentially [tile_id]__[pattern].tif. Drops the chunk bounds from the middle.\n",
    "        for filename in filenames:\n",
    "        \n",
    "            result = filename[:10] + filename[filename.rfind(\"__\") + len(\"__\"):]   # Extracts the relevant parts of the raw file names\n",
    "            simple_file_names.append(result)   # New list of simplified file names used for 10x10 degree outputs\n",
    "    \n",
    "        # Removes duplicate simplified file names.\n",
    "        # There are duplicates because each 10x10 output raster has many constituent chunks, each of which have the same aggregated, final name\n",
    "        # e.g., ['00N_000E__IPCC_classes_2020.tif', '00N_010E__IPCC_classes_2020.tif', ...]\n",
    "        simple_file_names = np.unique(simple_file_names).tolist()\n",
    "\n",
    "        # Makes nested lists of the file names and no data values inside the list of all file names, \n",
    "        # e.g., [['00N_000E__IPCC_classes_2020.tif', 0], ['00N_010E__IPCC_classes_2020.tif', 0], ... ]\n",
    "        simple_file_names_and_no_data = [[item, no_data_value] for item in simple_file_names]\n",
    "    \n",
    "        # Makes a list of dictionaries, where the key is the input s3 path and the value is the output aggregated name\n",
    "        # e.g., [{'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_000E__IPCC_classes_2020.tif', 0]}, \n",
    "        # {'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_010E__IPCC_classes_2020.tif', 0]}, ... ]\n",
    "        list_of_s3_name_dicts = [{key: value} for value in simple_file_names_and_no_data for key in [s3_in_folder]]\n",
    "    \n",
    "        # Adds the dictionary of s3 paths and output names for this folder to the list for all folders\n",
    "        list_of_s3_name_dicts_total.append(list_of_s3_name_dicts)\n",
    "    \n",
    "    # Output of above is a nested list, where each input folder is its own inner list. Need to flatten to a list.\n",
    "    # e.g., [{'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_000E__IPCC_classes_2020.tif', 0]},\n",
    "    # {'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_010E__IPCC_classes_2020.tif', 0]}, ... ]\n",
    "    list_of_s3_name_dicts_total = flatten_list(list_of_s3_name_dicts_total)\n",
    "    \n",
    "    print(f\"There are {len(list_of_s3_name_dicts_total)} chunks to process in {len(s3_in_folders)} input folders.\")\n",
    "\n",
    "    return list_of_s3_name_dicts_total\n",
    "\n",
    "# Flattens a nested list\n",
    "def flatten_list(nested_list):\n",
    "    return [x for xs in nested_list for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5036a75-178b-4822-b36c-6031895c69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges rasters that are <10x10 degrees into 10x10 degree rasters in the standard grid. Not currently using.\n",
    "# Approach is to open rasters, convert to xarray, merge as xarray datarrays, then save as rasters locally and upload to s3.\n",
    "def merge_small_tiles_xarray(s3_name_dict):\n",
    "\n",
    "    in_folder = list(s3_name_dict.keys())[0]   # The input s3 folder for the small rasters\n",
    "    out_file_name = list(s3_name_dict.values())[0]   # The output file name for the combined rasters\n",
    "\n",
    "    s3_in_folder = f's3://{in_folder}'   # The input s3 folder with s3:// prepended\n",
    "    vsis3_in_folder = f'/vsis3/{in_folder}'   # The input s3 folder with /vsis3/ prepended\n",
    "\n",
    "    # Lists all the rasters in the specified s3 folder\n",
    "    filenames = list_rasters_in_folder(s3_in_folder)   \n",
    "\n",
    "    # Gets the tile_id from the output file name in the standard format\n",
    "    tile_id = out_file_name[:8]\n",
    "\n",
    "    # Limits the input rasters to the specified tile_id (the relevant 10x10 area)\n",
    "    filenames_in_focus_area = [i for i in filenames if tile_id in i]\n",
    "    \n",
    "    # Lists the tile paths for the relevant rasters\n",
    "    tile_paths = []\n",
    "    tile_paths = [s3_in_folder + filename for filename in filenames_in_focus_area]\n",
    "\n",
    "    print(f\"Opening small rasters in {tile_id} in {s3_in_folder}\")\n",
    "\n",
    "    # Opens the relevant rasters in a list of xarray data arrays\n",
    "    small_rasters = [rioxarray.open_rasterio(tile_path, chunks=True) for tile_path in tile_paths]\n",
    "\n",
    "    print(f\"Merging {tile_id} in {s3_in_folder}\")\n",
    "\n",
    "    nodata_value = 255\n",
    "\n",
    "    min_x, min_y, max_x, max_y = get_10x10_tile_bounds(tile_id)   # The bounding box for the output 10x10 deg tile\n",
    "\n",
    "    # Merges the relevant small data arrays in the list\n",
    "    # https://corteva.github.io/rioxarray/stable/examples/merge.html\n",
    "    merged = merge_arrays(small_rasters, bounds=(min_x, min_y, max_x, max_y), nodata=nodata_value)  # Bounds of the output image (left, bottom, right, top)) \n",
    "\n",
    "    # Names the output folder. Same as the input folder but with the dimensions in pixels replaced\n",
    "    out_folder = re.sub(r'\\d+_pixels', f'{full_raster_dims}_pixels', in_folder)\n",
    "\n",
    "    # Saves the merged xarray data array locally and then to s3 \n",
    "    save_and_upload_raster_10x10(data=merged, out_file_name=out_file_name, out_folder=out_folder)\n",
    "\n",
    "    del merged\n",
    "\n",
    "    return f\"success for {s3_name_dict}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b0777-2db3-49b7-ba70-5d05b138d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges rasters that are <10x10 degrees into 10x10 degree rasters in the standard grid.\n",
    "# Approach is to merge rasters with gdal.Warp and then upload them to s3.\n",
    "def merge_small_tiles_gdal(s3_name_no_data_dict):\n",
    "    \n",
    "    in_folder = list(s3_name_no_data_dict.keys())[0]   # The input s3 folder for the small rasters\n",
    "    out_file_name_no_data = list(s3_name_no_data_dict.values())[0]   # The output file name for the combined rasters and their no data value\n",
    "    out_file_name = out_file_name_no_data[0]    # The output file name\n",
    "    no_data = out_file_name_no_data[1]    # The output no data value. Not currently using but it's available.\n",
    "\n",
    "    s3_in_folder = f's3://{in_folder}'   # The input s3 folder with s3:// prepended\n",
    "    vsis3_in_folder = f'/vsis3/{in_folder}'   # The input s3 folder with /vsis3/ prepended\n",
    "\n",
    "    # Lists all the rasters in the specified s3 folder\n",
    "    filenames = list_rasters_in_folder(s3_in_folder)   \n",
    "\n",
    "    # Gets the tile_id from the output file name in the standard format\n",
    "    tile_id = out_file_name[:8]\n",
    "\n",
    "    # Limits the input rasters to the specified tile_id (the relevant 10x10 area)\n",
    "    filenames_in_focus_area = [i for i in filenames if tile_id in i]\n",
    "    \n",
    "    # Lists the tile paths for the relevant rasters\n",
    "    tile_paths = []\n",
    "    tile_paths = [vsis3_in_folder + filename for filename in filenames_in_focus_area]\n",
    "\n",
    "    print(f\"Merging small rasters in {tile_id} in {vsis3_in_folder}\")\n",
    "\n",
    "    # Names the output folder. Same as the input folder but with the dimensions in pixels replaced\n",
    "    out_folder = re.sub(r'\\d+_pixels', f'{full_raster_dims}_pixels', in_folder[10:])   # [10:] to remove the gfw2-data/ at the front\n",
    "\n",
    "    min_x, min_y, max_x, max_y = get_10x10_tile_bounds(tile_id)\n",
    "\n",
    "    output_extent = [min_x, min_y, max_x, max_y]  # Specify the extent in the order [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    warp_options = gdal.WarpOptions(outputBounds=output_extent, creationOptions=[\"COMPRESS=LZW\"])\n",
    "    # warp_options = gdal.WarpOptions(outputBounds=output_extent, creationOptions=[\"COMPRESS=LZW\"], dstNodata=no_data)\n",
    "\n",
    "    # Merges all output small rasters with the options above\n",
    "    gdal.Warp(f\"/tmp/{out_file_name}\", tile_paths, options=warp_options)\n",
    "\n",
    "    s3_client = boto3.client(\"s3\") # Needs to be in the same function as the upload_file call\n",
    "\n",
    "    print(f\"Saving {out_file_name} to s3: {out_folder}{out_file_name}\")\n",
    "    \n",
    "    s3_client.upload_file(f\"/tmp/{out_file_name}\", \"gfw2-data\", Key=f\"{out_folder}{out_file_name}\")\n",
    "\n",
    "    # Deletes the local raster\n",
    "    os.remove(f\"/tmp/{out_file_name}\")\n",
    "\n",
    "    return f\"success for {s3_name_no_data_dict}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
