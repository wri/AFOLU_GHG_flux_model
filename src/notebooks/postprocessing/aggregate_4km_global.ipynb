{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e4454b-de3a-4dbc-be16-a5e7560fb734",
   "metadata": {},
   "source": [
    "Haven't tested this one yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834e60c-05ef-4b77-b783-cfd7d9d96d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "import rioxarray\n",
    "import rasterio\n",
    "from rasterio.merge import merge as merge_arrays\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import atexit\n",
    "import xarray as xr\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# AWS S3 setup\n",
    "s3_bucket = \"gfw2-data\"\n",
    "local_temp_dir = \"/tmp/aggregated\"\n",
    "\n",
    "# Global variables for Dask cluster and client\n",
    "cluster = None\n",
    "client = None\n",
    "\n",
    "def s3_file_exists(bucket, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        logging.info(f\"File exists: s3://{bucket}/{key}\")\n",
    "        return True\n",
    "    except:\n",
    "        logging.info(f\"File does not exist: s3://{bucket}/{key}\")\n",
    "        return False\n",
    "\n",
    "def list_s3_files(bucket, prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    keys = []\n",
    "    try:\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                keys.append(obj['Key'])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing files in s3://{bucket}/{prefix}: {e}\")\n",
    "    return keys\n",
    "\n",
    "def aggregate_tile(tile_path, output_path):\n",
    "    with rasterio.open(tile_path) as src:\n",
    "        data = src.read(1, masked=True)\n",
    "        transform = src.transform\n",
    "        profile = src.profile\n",
    "\n",
    "        # Resample to 4km resolution\n",
    "        scale_factor = 30 / 4000  # 30m to 4km\n",
    "        new_height = int(data.shape[0] * scale_factor)\n",
    "        new_width = int(data.shape[1] * scale_factor)\n",
    "        resampled_data = src.read(\n",
    "            out_shape=(1, new_height, new_width),\n",
    "            resampling=rasterio.enums.Resampling.average\n",
    "        )\n",
    "\n",
    "        # Update profile\n",
    "        profile.update({\n",
    "            \"height\": new_height,\n",
    "            \"width\": new_width,\n",
    "            \"transform\": rasterio.Affine(\n",
    "                transform.a / scale_factor,\n",
    "                transform.b,\n",
    "                transform.c,\n",
    "                transform.d,\n",
    "                transform.e / scale_factor,\n",
    "                transform.f\n",
    "            )\n",
    "        })\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(resampled_data, 1)\n",
    "\n",
    "def merge_global_tiles(tiles, output_path):\n",
    "    rasters = [rasterio.open(tile) for tile in tiles]\n",
    "    merged, out_transform = merge_arrays(rasters)\n",
    "\n",
    "    out_meta = rasters[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"height\": merged.shape[1],\n",
    "        \"width\": merged.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "\n",
    "    with rasterio.open(output_path, 'w', **out_meta) as dst:\n",
    "        dst.write(merged[0], 1)  # Write the first band\n",
    "\n",
    "def cleanup():\n",
    "    global client, cluster\n",
    "    if client:\n",
    "        client.close()\n",
    "    if cluster:\n",
    "        cluster.close()\n",
    "\n",
    "def process_and_aggregate(tile_id, input_prefix, temp_prefix, output_prefix):\n",
    "    s3_client = boto3.client('s3')\n",
    "    input_file = f'{input_prefix}/{tile_id}.tif'\n",
    "    local_input_path = os.path.join(local_temp_dir, f'{tile_id}.tif')\n",
    "    temp_output_file = f'{temp_prefix}/{tile_id}_4km.tif'\n",
    "    local_temp_output_path = os.path.join(local_temp_dir, f'{tile_id}_4km.tif')\n",
    "\n",
    "    if not os.path.exists(local_temp_dir):\n",
    "        os.makedirs(local_temp_dir)\n",
    "\n",
    "    s3_client.download_file(s3_bucket, input_file, local_input_path)\n",
    "    aggregate_tile(local_input_path, local_temp_output_path)\n",
    "    s3_client.upload_file(local_temp_output_path, s3_bucket, temp_output_file)\n",
    "\n",
    "    os.remove(local_input_path)\n",
    "    os.remove(local_temp_output_path)\n",
    "\n",
    "def main(input_prefix, temp_prefix, output_prefix, tile_ids=None):\n",
    "    global cluster, client\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "    atexit.register(cleanup)\n",
    "\n",
    "    try:\n",
    "        available_tile_ids = list_s3_files(s3_bucket, input_prefix)\n",
    "        available_tile_ids = [os.path.basename(path).replace('.tif', '') for path in available_tile_ids]\n",
    "\n",
    "        if tile_ids:\n",
    "            # Filter the available tile IDs to only include those in the provided list\n",
    "            tile_ids_to_process = [tile_id for tile_id in tile_ids if tile_id in available_tile_ids]\n",
    "        else:\n",
    "            tile_ids_to_process = available_tile_ids\n",
    "\n",
    "        for tile_id in tile_ids_to_process:\n",
    "            dask_tile = dask.delayed(process_and_aggregate)(tile_id, input_prefix, temp_prefix, output_prefix)\n",
    "            with ProgressBar():\n",
    "                dask.compute(dask_tile)\n",
    "\n",
    "        # Merge all the 4km tiles into one global raster\n",
    "        aggregated_tiles = list_s3_files(s3_bucket, temp_prefix)\n",
    "        aggregated_tiles = [f's3://{s3_bucket}/{tile}' for tile in aggregated_tiles]\n",
    "        \n",
    "        if aggregated_tiles:\n",
    "            local_global_path = os.path.join(local_temp_dir, 'global_4km.tif')\n",
    "            merge_global_tiles(aggregated_tiles, local_global_path)\n",
    "            s3_global_output = f'{output_prefix}/global_4km.tif'\n",
    "            s3_client.upload_file(local_global_path, s3_bucket, s3_global_output)\n",
    "            os.remove(local_global_path)\n",
    "            logging.info(f\"Uploaded global raster to s3://{s3_bucket}/{s3_global_output}\")\n",
    "    finally:\n",
    "        print(\"exit\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_prefix = 'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/10x10_degrees/'\n",
    "    temp_prefix = 'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/4km_temp/'\n",
    "    output_prefix = 'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/global/'\n",
    "\n",
    "    tile_ids = ['50N_070E', '70N_040E', '00N_150E', '10N_130E']  # Replace with the list of tile IDs you want to process\n",
    "    main(input_prefix, temp_prefix, output_prefix, tile_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17df234-c501-4f3b-a975-be5864c7c7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
