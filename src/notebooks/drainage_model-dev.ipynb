{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1bacb8-44f6-4e99-b764-0f3acbcb0223",
   "metadata": {},
   "source": [
    "<font size=\"6\">Run the organic soils part of the AFOLU model</font> \n",
    "\n",
    "<font size=\"4\">Must be run using the utilities_and_variables.ipynb kernel</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c1514-9a1e-4c33-a5c1-3448e47b4870",
   "metadata": {},
   "source": [
    "This current version is working (locally) some of the time and causing the kernel to die other times. It also works with coiled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70eae4d0-b084-4fcb-acdd-18e96ccdb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "496ab79b-6979-47f4-bf73-a7a443443cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c2142ea-6f28-40d7-9819-f9ab50d4f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, output_dict, is_final):\n",
    "    s3_client = boto3.client(\"s3\")  # Needs to be in the same function as the upload_file call\n",
    "\n",
    "    transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "\n",
    "    file_info = f'{tile_id}__{bounds_str}'\n",
    "\n",
    "    # For every output file, saves from array to local raster, then to s3.\n",
    "    # Can't save directly to s3, unfortunately, so need to save locally first.\n",
    "    for key, value in output_dict.items():\n",
    "        logging.info(f\"Processing output for key: {key}\")\n",
    "        try:\n",
    "            data_array = value[0]\n",
    "            data_type = value[1]\n",
    "            data_meaning = value[2]\n",
    "            year_out = value[3]\n",
    "\n",
    "            logging.info(f\"Data type: {data_type}, Data meaning: {data_meaning}, Year out: {year_out}\")\n",
    "\n",
    "            if not is_final:\n",
    "                logging.info(f\"Saving {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "            if is_final:\n",
    "                file_name = f\"{file_info}__{key}.tif\"\n",
    "            else:\n",
    "                file_name = f\"{file_info}__{key}__{timestr()}.tif\"\n",
    "\n",
    "            with rasterio.open(f\"/tmp/{file_name}\", 'w', driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels, count=1, \n",
    "                               dtype=data_type, crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "                dst.write(data_array, 1)\n",
    "\n",
    "            s3_path = f\"{s3_out_dir}/{data_meaning}/{year_out}/{chunk_length_pixels}_pixels/{time.strftime('%Y%m%d')}\"\n",
    "            logging.info(f\"Saving output to {s3_path}...\")\n",
    "\n",
    "            if not is_final:\n",
    "                logging.info(f\"Uploading {bounds_str} in {tile_id} for {year_out} to {s3_path}: {timestr()}\")\n",
    "\n",
    "            s3_client.upload_file(f\"/tmp/{file_name}\", \"gfw2-data\", Key=f\"{s3_path}/{file_name}\")\n",
    "\n",
    "            # Deletes the local raster\n",
    "            os.remove(f\"/tmp/{file_name}\")\n",
    "\n",
    "            logging.info(f\"Successfully processed and uploaded {file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing key {key} with value {value}: {str(e)}\")\n",
    "\n",
    "    logging.info(f\"Completed processing for chunk {bounds_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c5a5106-0d0f-4431-a9e3-15453428c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_upload_drainage(bounds, is_final):\n",
    "    bounds_str = boundstr(bounds)  # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])  # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)  # Chunk length in pixels (as opposed to decimal degrees)\n",
    "\n",
    "    no_data_val = 255\n",
    "    logging.info(f\"Processing tile {tile_id} with bounds {bounds_str}\")\n",
    "\n",
    "    try:\n",
    "        # Dictionary of downloaded layers\n",
    "        download_dict = {}\n",
    "        layers = {}\n",
    "\n",
    "        download_dict = {\n",
    "            f\"{land_cover}_2020\": f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/40000_pixels/20240205/{tile_id}__IPCC_classes_2020.tif\",\n",
    "            planted_forest_type_layer: f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/plantation_type/SDPTv2/20230911/{tile_id}_plantation_type_oilpalm_woodfiber_other.tif\",\n",
    "            planted_forest_tree_crop_layer: f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/plantation_simpleType__planted_forest_tree_crop/SDPTv2/20230911/{tile_id}.tif\",\n",
    "            \"peat\": f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/peatlands/processed/20230315/{tile_id}_peat_mask_processed.tif\",\n",
    "            \"dadap\": f\"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_{tile_id}.tif\",\n",
    "            \"engert\": f\"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_{tile_id}.tif\",\n",
    "            \"grip\": f\"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/grip_density/grip_density_{tile_id}.tif\",\n",
    "            \"osm_roads\": f\"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/grip_density/grip_density_{tile_id}.tif\",\n",
    "            \"osm_canals\": f\"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/grip_density/grip_density_{tile_id}.tif\",\n",
    "        }\n",
    "\n",
    "        # Checks whether tile exists at all. Doesn't try to download chunk if the tile doesn't exist.\n",
    "        tile_exists = check_for_tile(download_dict, is_final)\n",
    "\n",
    "        if tile_exists == 0:\n",
    "            logging.info(f\"Tile {tile_id} does not exist. Skipping.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Tile {tile_id} exists. Proceeding with downloading data.\")\n",
    "        futures = prepare_to_download_chunk(bounds, download_dict, no_data_val)\n",
    "\n",
    "        if not is_final:\n",
    "            logging.info(f\"Waiting for requests for data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "\n",
    "        # Waits for requests to come back with data from S3\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            layer = futures[future]\n",
    "            layers[layer] = future.result()\n",
    "            logging.info(f\"Downloaded data for layer: {layer}\")\n",
    "\n",
    "        data_in_chunk = check_chunk_for_data(layers, f\"{land_cover}_\", bounds_str, tile_id, no_data_val, is_final)\n",
    "\n",
    "        if data_in_chunk == 0:\n",
    "            logging.info(f\"No data in chunk {bounds_str}. Skipping.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Data present in chunk {bounds_str}. Proceeding with processing.\")\n",
    "\n",
    "        # Initializes empty dictionaries for each type\n",
    "        uint8_dict_layers = {}\n",
    "        int16_dict_layers = {}\n",
    "        float32_dict_layers = {}\n",
    "\n",
    "        for key, array in layers.items():\n",
    "            logging.info(f\"Processing layer {key} with dtype {array.dtype}\")\n",
    "            if array.dtype == np.uint8:\n",
    "                uint8_dict_layers[key] = array\n",
    "            elif array.dtype == np.int16:\n",
    "                int16_dict_layers[key] = array\n",
    "            elif array.dtype == np.float32:\n",
    "                float32_dict_layers[key] = array\n",
    "            else:\n",
    "                raise TypeError(f\"{key} dtype not in list\")\n",
    "\n",
    "        peat_block = uint8_dict_layers[\"peat\"]\n",
    "        land_cover_block = uint8_dict_layers[f\"{land_cover}_2020\"]\n",
    "        planted_forest_type_block = uint8_dict_layers[planted_forest_type_layer]\n",
    "        planted_forest_tree_crop_block = uint8_dict_layers[planted_forest_tree_crop_layer]\n",
    "        dadap_block = float32_dict_layers[\"dadap\"]\n",
    "        osm_roads_block = float32_dict_layers[\"osm_roads\"]\n",
    "        osm_canals_block = float32_dict_layers[\"osm_canals\"]\n",
    "        engert_block = float32_dict_layers[\"engert\"]\n",
    "        grip_block = float32_dict_layers[\"grip\"]\n",
    "\n",
    "        logging.info(f\"Creating drainage map in {bounds_str} in {tile_id}: {timestr()}\")\n",
    "        soil_block, state_block = process_soil(\n",
    "            peat_block, land_cover_block, planted_forest_type_block, dadap_block, osm_roads_block, osm_canals_block, engert_block, grip_block\n",
    "        )\n",
    "\n",
    "        out_dict_uint32 = {\n",
    "            \"soil\": soil_block,\n",
    "            \"state\": state_block\n",
    "        }\n",
    "\n",
    "        out_dict_all_dtypes = {}\n",
    "\n",
    "        for key, value in out_dict_uint32.items():\n",
    "            data_type = value.dtype.name\n",
    "            out_pattern = key\n",
    "            year = 2020  # Hardcoded example year, change as needed\n",
    "            out_dict_all_dtypes[key] = [value, data_type, out_pattern, f'{year-5}_{year}']\n",
    "\n",
    "        logging.info(f\"Saving and uploading rasters for chunk {bounds_str}.\")\n",
    "        save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, out_dict_all_dtypes, is_final)\n",
    "\n",
    "        del out_dict_all_dtypes\n",
    "\n",
    "        logging.info(f\"Completed processing for chunk {bounds_str}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed processing for {bounds_str}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0eccfbf-92c8-4bdd-948b-6cc27b3c8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def accrete_node(combo, new):\n",
    "    combo = combo*10 + new\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ff3623d-5993-4798-ac5b-f21d48b0e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def process_soil(peat_block, land_cover_block, planted_forest_type_block, dadap_block, osm_roads_block, osm_canals_block, engert_block, grip_block):\n",
    "    rows, cols = peat_block.shape\n",
    "\n",
    "    soil_block = np.empty((rows, cols), dtype=np.uint32)\n",
    "    state_block = np.empty((rows, cols), dtype=np.uint32)\n",
    "\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            peat = peat_block[row, col]\n",
    "            land_cover = land_cover_block[row, col]\n",
    "            planted_forest_type = planted_forest_type_block[row, col]\n",
    "            dadap = dadap_block[row, col]\n",
    "            osm_roads = osm_roads_block[row, col]\n",
    "            osm_canals = osm_canals_block[row, col]\n",
    "            engert = engert_block[row, col]\n",
    "            grip = grip_block[row, col]\n",
    "\n",
    "            node = 0\n",
    "\n",
    "            if peat == 1:\n",
    "                soil_block[row, col] = 1  # 'organic'\n",
    "                if dadap > 0 or osm_roads > 0 or osm_canals > 0 or engert > 0 or grip > 0:\n",
    "                    node = accrete_node(node, 1)\n",
    "                    state_block[row, col] = 1  # 'drained'\n",
    "                elif land_cover == 6 or land_cover == 5:\n",
    "                    node = accrete_node(node, 3)\n",
    "                    state_block[row, col] = 1  # 'drained'\n",
    "                else:\n",
    "                    node = accrete_node(node, 4)\n",
    "                    state_block[row, col] = 2  # 'undrained'\n",
    "            else:\n",
    "                soil_block[row, col] = 2  # 'inorganic'\n",
    "                state_block[row, col] = 0  # Omitted from analysis\n",
    "\n",
    "    return soil_block, state_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ea4ee-0318-425a-bfc2-8142b9ff6d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:37:06,243 - INFO - Processing tile 00N_110E with bounds 113_-3_113_-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:37:06,542 - INFO - Tile id 00N_110E exists. Proceeding.\n",
      "2024-05-29 15:37:06,544 - INFO - Tile 00N_110E exists. Proceeding with downloading data.\n",
      "2024-05-29 15:37:06,575 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,590 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,593 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,595 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,601 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,614 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,626 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,637 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-29 15:37:06,642 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting data in chunk 113_-3_113_-2 in 00N_110E: 20240529_15_37_06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:37:31,350 - INFO - Waiting for requests for data in chunk 113_-3_113_-2 in 00N_110E: 20240529_15_37_31\n",
      "2024-05-29 15:37:31,352 - INFO - Downloaded data for layer: grip\n",
      "2024-05-29 15:37:31,352 - INFO - Downloaded data for layer: osm_canals\n",
      "2024-05-29 15:37:31,353 - INFO - Downloaded data for layer: osm_roads\n",
      "2024-05-29 15:37:31,353 - INFO - Downloaded data for layer: planted_forest_tree_crop\n",
      "2024-05-29 15:37:31,354 - INFO - Downloaded data for layer: planted_forest_type\n",
      "2024-05-29 15:37:31,354 - INFO - Downloaded data for layer: land_cover_2020\n",
      "2024-05-29 15:37:31,355 - INFO - Downloaded data for layer: peat\n",
      "2024-05-29 15:37:31,355 - INFO - Downloaded data for layer: engert\n",
      "2024-05-29 15:37:31,355 - INFO - Downloaded data for layer: dadap\n",
      "2024-05-29 15:37:31,356 - INFO - Checking chunk for data in tile 00N_110E with bounds 113_-3_113_-2\n",
      "2024-05-29 15:37:31,356 - INFO - Checking layer grip with type <class 'numpy.ndarray'>\n",
      "2024-05-29 15:37:31,357 - INFO - Data in chunk 113_-3_113_-2. Proceeding.\n",
      "2024-05-29 15:37:31,357 - INFO - Data present in chunk 113_-3_113_-2. Proceeding with processing.\n",
      "2024-05-29 15:37:31,358 - INFO - Processing layer grip with dtype float32\n",
      "2024-05-29 15:37:31,358 - INFO - Processing layer osm_canals with dtype float32\n",
      "2024-05-29 15:37:31,358 - INFO - Processing layer osm_roads with dtype float32\n",
      "2024-05-29 15:37:31,359 - INFO - Processing layer planted_forest_tree_crop with dtype uint8\n",
      "2024-05-29 15:37:31,359 - INFO - Processing layer planted_forest_type with dtype uint8\n",
      "2024-05-29 15:37:31,360 - INFO - Processing layer land_cover_2020 with dtype uint8\n",
      "2024-05-29 15:37:31,360 - INFO - Processing layer peat with dtype uint8\n",
      "2024-05-29 15:37:31,361 - INFO - Processing layer engert with dtype float32\n",
      "2024-05-29 15:37:31,361 - INFO - Processing layer dadap with dtype float32\n",
      "2024-05-29 15:37:31,362 - INFO - Creating drainage map in 113_-3_113_-2 in 00N_110E: 20240529_15_37_31\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Create LULUCF flux and carbon stock 2x2 deg rasters \n",
    "\n",
    "## Area to analyze\n",
    "## chunk_params arguments: W, S, E, N, chunk size (degrees)\n",
    "# chunk_params = [-180, -60, 180, 80, 2]  # entire world\n",
    "# chunk_params = [-10, 40, 20, 70, 1]    # 30x30 deg (70N_010W), 900 chunks\n",
    "\n",
    "# chunk_params = [-10, 60, 0, 70, 1]    # 10x10 deg (70N_010W), 100 chunks\n",
    "# chunk_params = [-10, 65, -5, 70, 1]    # 5x5 deg (70N_010W), 25 chunks\n",
    "# chunk_params = [-10, 68, -8, 70, 1]    # 2x2 deg (70N_010W), 4 chunks\n",
    "# chunk_params = [-10, 69, -9, 70, 1]    # 1x1 deg (70N_010W), 1 chunk\n",
    "\n",
    "# chunk_params = [10, 40, 20, 50, 2]    # 10x10 deg (50N_010E), 25 chunks\n",
    "# chunk_params = [10, 40, 20, 50, 10]    # 10x10 deg (50N_010E), 1 chunk\n",
    "# chunk_params = [10, 46, 14, 50, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [110, -10, 114, -6, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [10, 48, 12, 50, 1]   # 2x2 deg, 4 chunks\n",
    "# chunk_params = [10, 49, 11, 50, 1]   # 1x1 deg, 1 chunk\n",
    "# chunk_params = [10, 49, 11, 50, 0.5] # 1x1 deg, 4 chunks\n",
    "# chunk_params = [10, 49.5, 10.5, 50, 0.25] # 0.5x0.5 deg, 4 chunks\n",
    "# chunk_params = [10, 42, 11, 43, 0.5] # 1x1 deg, 4 chunks (some GLCLU code=254 for ocean and some land, so data should be output)\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data, no fire)\n",
    "#chunk_params = [15, 41.75, 15.25, 42, 0.25] # 0.25x0.25 deg, 1 chunk (has data with fire)\n",
    "\n",
    "# # Range of no-data cases for testing\n",
    "# chunk_params = [20, 69.75, 20.25, 70, 0.25] # 0.25x0.25 deg, 1 chunk (tile exists for GLCLU but not all other inputs, e.g., fire)\n",
    "# chunk_params = [110, -10, 120, 0, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [110, -20, 120, -10, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [0, 79.75, 0.25, 80, 0.25] # 0.25x0.25 deg, 1 chunk (no 80N_000E tile-- no data)\n",
    "# chunk_params = [112, -12, 116, -8, 2]   # 2x2 deg, 1 chunk (bottom of Java, has data but mostly ocean)\n",
    "# chunk_params = [10.875, 41.75, 11, 42, 0.25] # 0.25x0.25 deg, 1 chunk (entirely GLCLU code=255 for ocean, so no actual data-- nothing should be be output)\n",
    "# chunk_params = [-10, 21.75, -9.75, 22, 0.25] # 0.25x0.25 deg, 1 chunk (has data but entirely desert (fully GLCLU code=0))\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data)\n",
    "\n",
    "chunk_params = [112.75, -2.75, 113.0, -2.5, 0.25] # 1 chunk, has data. Southern Borneo\n",
    "\n",
    "# Makes list of chunks to analyze\n",
    "chunks = get_chunk_bounds(chunk_params)  \n",
    "print(\"Processing\", len(chunks), \"chunks\")\n",
    "# print(chunks)\n",
    "\n",
    "# Determines if the output file names for final versions of outputs should be used\n",
    "is_final = False\n",
    "if len(chunks) > 90:\n",
    "    is_final = True\n",
    "    print(\"Running as final model.\")\n",
    "\n",
    "# Creates list of tasks to run (1 task = 1 chunk for all years)\n",
    "delayed_result = [dask.delayed(calculate_and_upload_drainage)(chunk, is_final) for chunk in chunks]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593927e-7de6-43c3-abee-4883e44f2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c484e7-76b0-4704-ab1f-5dfc2b422ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes - when running without client - get all of the log messages, but dies a lot of the time\n",
    "# when running with local client - no logging; 2024-05-29 14:40:01,855 - distributed.nanny - WARNING - Restarting worker\n",
    "# when running with coiled, still dies some of the time, logging sometimes and not others "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
