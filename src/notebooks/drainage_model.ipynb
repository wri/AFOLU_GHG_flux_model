{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70eae4d0-b084-4fcb-acdd-18e96ccdb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "496ab79b-6979-47f4-bf73-a7a443443cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "847dec8a-eb9f-497f-be0c-47270b54dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculcate_and_upload_drainage(bounds, is_final):\n",
    "    bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])    # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees\n",
    "\n",
    "    no_data_val = 255\n",
    "    logging.info(f\"Processing tile {tile_id} with bounds {bounds_str}\")\n",
    "\n",
    "    try: \n",
    "        # Dictionary of downloaded layers\n",
    "        download_dict = {}\n",
    "        layers = {}\n",
    "    \n",
    "        download_dict = {\n",
    "            # f\"{land_cover}_2000\": f\"{LC_uri}/composite/2000/raw/{tile_id}.tif\",\n",
    "            # f\"{land_cover}_2005\": f\"{LC_uri}/composite/2005/raw/{tile_id}.tif\",\n",
    "            # f\"{land_cover}_2010\": f\"{LC_uri}/composite/2010/raw/{tile_id}.tif\",\n",
    "            # f\"{land_cover}_2015\": f\"{LC_uri}/composite/2015/raw/{tile_id}.tif\"\n",
    "            f\"{land_cover}_2020\": f\"s3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/40000_pixels/20240205/{tile_id}__IPCC_classes_2020.tif\",\n",
    "            planted_forest_type_layer: f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/plantation_type/SDPTv2/20230911/{tile_id}_plantation_type_oilpalm_woodfiber_other.tif\", # Originally from gfw-data-lake, so it's in 400x400 windows\n",
    "            planted_forest_tree_crop_layer: f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/plantation_simpleType__planted_forest_tree_crop/SDPTv2/20230911/{tile_id}.tif\",  # Originally from gfw-data-lake, so it's in 400x400 windows\n",
    "            peat: f\"s3://gfw2-data/climate/carbon_model/other_emissions_inputs/peatlands/processed/20230315/{tile_id}_peat_mask_processed.tif\",\n",
    "            dadap: f\"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_{tile_id}.tif\",\n",
    "            ecozone: f\"s3://gfw2-data/fao_ecozones/v2000/raster/epsg-4326/10/40000/class/gdal-geotiff/{tile_id}.tif\",   # Originally from gfw-data-lake, so it's in 400x400 windows \n",
    "\n",
    "        }\n",
    "    \n",
    "        # Checks whether tile exists at all. Doesn't try to download chunk if the tile doesn't exist.\n",
    "        tile_exists = check_for_tile(download_dict, is_final)\n",
    "    \n",
    "        if tile_exists == 0:\n",
    "            return\n",
    "    \n",
    "        # Note: If running in a local Dask cluster, prints to console may be duplicated. Doesn't happen with a Coiled cluster of the same size (1 worker).\n",
    "        # Seems to be a problem with local Dask getting overwhelmed by so many futures being created and downloaded from s3. \n",
    "        futures = prepare_to_download_chunk(bounds, download_dict, no_data_val)\n",
    "    \n",
    "        # print(futures)\n",
    "    \n",
    "        if not is_final:\n",
    "            print(f\"Waiting for requests for data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "        \n",
    "        # Waits for requests to come back with data from S3\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            layer = futures[future]\n",
    "            layers[layer] = future.result()\n",
    "    \n",
    "        print(layers)\n",
    "        # print(layers[soil_c_2000].dtype)\n",
    "        \n",
    "        # Checks chunk for data. Skips the chunk if it has no data in it.\n",
    "        # will update this line to the peat version \n",
    "        data_in_chunk = check_chunk_for_data(layers, f\"{land_cover}_\", bounds_str, tile_id, no_data_val, is_final)\n",
    "    \n",
    "        if data_in_chunk == 0:\n",
    "            return\n",
    "    \n",
    "        \n",
    "        ### Part 2: Create a separate dictionary for each chunk datatype so that they can be passed to Numba as separate arguments.\n",
    "        ### Numba functions can accept (and return) dictionaries of arrays as long as each dictionary only has arrays of one data type (e.g., uint8, float32)\n",
    "        ### Note: need to add new code if inputs with other data types are added\n",
    "    \n",
    "        # Initializes empty dictionaries for each type\n",
    "        uint8_dict_layers = {}\n",
    "        int16_dict_layers = {}\n",
    "        float32_dict_layers = {}\n",
    "        \n",
    "        # Iterates through the downloaded chunk dictionary and distributes arrays to a separate dictionary for each data type\n",
    "        for key, array in layers.items():\n",
    "            if array.dtype == np.uint8:\n",
    "                uint8_dict_layers[key] = array\n",
    "            elif array.dtype == np.int16:\n",
    "                int16_dict_layers[key] = array\n",
    "            elif array.dtype == np.float32:\n",
    "                float32_dict_layers[key] = array\n",
    "            else:\n",
    "                raise TypeError(f\"{key} dtype not in list\")\n",
    "    \n",
    "        # print(f\"uint8 datasets: {uint8_dict_layers.keys()}\")\n",
    "        # print(f\"int16 datasets: {int16_dict_layers.keys()}\")\n",
    "        # print(f\"float32 datasets: {float32_dict_layers.keys()}\")\n",
    "        \n",
    "        # Creates numba-compliant typed dict for each type of array\n",
    "        typed_dict_uint8 = Dict.empty(\n",
    "            key_type=types.unicode_type, \n",
    "            value_type=types.Array(types.uint8, 2, 'C')  # Assuming 2D arrays of uint8\n",
    "        )\n",
    "    \n",
    "        typed_dict_int16 = Dict.empty(\n",
    "            key_type=types.unicode_type, \n",
    "            value_type=types.Array(types.int16, 2, 'C')  # Assuming 2D arrays of uint8\n",
    "        )\n",
    "    \n",
    "        typed_dict_float32 = Dict.empty(\n",
    "            key_type=types.unicode_type, \n",
    "            value_type=types.Array(types.float32, 2, 'C')  # Assuming 2D arrays of float32\n",
    "        )\n",
    "    \n",
    "        # Populates the numba-compliant typed dicts\n",
    "        for key, array in uint8_dict_layers.items():\n",
    "            typed_dict_uint8[key] = array\n",
    "    \n",
    "        for key, array in int16_dict_layers.items():\n",
    "            typed_dict_int16[key] = array\n",
    "    \n",
    "        for key, array in float32_dict_layers.items():\n",
    "            typed_dict_float32[key] = array\n",
    "    \n",
    "    \n",
    "        ### Part 3: Creates drainage map \n",
    "       \n",
    "        print(f\"Creating drainage map in {bounds_str} in {tile_id}: {timestr()}\")\n",
    "\n",
    "        ### processing is executed here \n",
    "        out_dict_uint32, out_dict_float32 = process_soil_types(\n",
    "            typed_dict_uint8, typed_dict_int16, typed_dict_float32 \n",
    "        )\n",
    "    \n",
    "        out_dict_all_dtypes = {}\n",
    "    \n",
    "        # Transfers the dictionaries of numpy arrays for each data type to a new, Pythonic array\n",
    "        for key, value in out_dict_uint32.items():\n",
    "            out_dict_all_dtypes[key] = value\n",
    "    \n",
    "        for key, value in out_dict_float32.items():\n",
    "            out_dict_all_dtypes[key] = value\n",
    "    \n",
    "        # Clear memory of unneeded arrays\n",
    "        del out_dict_uint32\n",
    "        del out_dict_float32\n",
    "    \n",
    "        \n",
    "        ### Part 4: Save numpy as rasters and upload to s3\n",
    "    \n",
    "        # Adds metadata used for uploading outputs to s3 to the dictionary\n",
    "        for key, value in out_dict_all_dtypes.items():\n",
    "    \n",
    "            data_type = value.dtype.name\n",
    "            out_pattern = key[:-10]\n",
    "            year = int(key[-4:])\n",
    "    \n",
    "            # Dictionary with metadata for each array\n",
    "            #will need to aler the year - year part \n",
    "            out_dict_all_dtypes[key] = [value, data_type, out_pattern, f'{year-5}_{year}']\n",
    "    \n",
    "        save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, out_dict_all_dtypes, is_final)\n",
    "        \n",
    "        # Clear memory of unneeded arrays\n",
    "        del out_dict_all_dtypes\n",
    "    \n",
    "        return f\"Success for {bounds_str}: {timestr()}\"\n",
    "\n",
    "    except Exception as e:\n",
    "            logging.error(f\"Failed processing for {bounds_str}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ff3623d-5993-4798-ac5b-f21d48b0e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "\n",
    "### this is meant to outline the values of the decision tree. these will be replaced by a dictionary of states and values for each dataset\n",
    "### may add extraction to the decision tree\n",
    "\n",
    "def process_soil_types(in_dict_uint8, in_dict_int16, in_dict_float32):\n",
    "\n",
    "    # Separate dictionaries for output numpy arrays of each datatype, named by output type (e.g., AGC density, BGC removals) and year ([output_type}_[year]).\n",
    "    # This is because a dictionary in a Numba function cannot have arrays with multiple data types, so each dictionary has to store only one data type,\n",
    "    # just like inputs to the function.\n",
    "    out_dict_uint32 = {}\n",
    "    out_dict_float32 = {}\n",
    "\n",
    "    peat = in_dict_uint8[peat]\n",
    "    land_cover = in_dict_uint8[land_cover_2020]\n",
    "    rows, cols = peat.shape\n",
    "    print(rows, cols)\n",
    "    \n",
    " \n",
    "    soil = np.empty((rows, cols), dtype=np.object_)\n",
    "    state = np.empty((rows, cols), dtype=np.object_)\n",
    "\n",
    "    node = 0  # Initialize node for decision tracking\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):        \n",
    "            node = 0  # Initialize node for decision tracking\n",
    "            if peat[row, col] == 1:\n",
    "                soil[row, col] = 'organic'\n",
    "                if (dadap[row, col] > 0): #or (grip[row, col] > 0) or (osm[row, col] > 0) or (congo[row, col] > 0):\n",
    "                    node = accrete_node(node, 1)  # Drained due to water management\n",
    "                    state[row, col] = 'drained'\n",
    "                # elif planted_forest_type[row, col] == 'plantation':\n",
    "                #     node = accrete_node(node, 2)  # Drained due to being plantation\n",
    "                #     state[row, col] = 'drained'\n",
    "                elif land_cover[row, col] == \"settlement\" or land_cover[row, col] == \"cropland\":\n",
    "                    node = accrete_node(node, 3)  # Drained due to land cover type\n",
    "                    state[row, col] = 'drained'\n",
    "                else:\n",
    "                    node = accrete_node(node, 4)  # Undrained\n",
    "                    state[row, col] = 'undrained'\n",
    "            else:\n",
    "                soil[row, col] = 'inorganic'\n",
    "                print('inorganic soil omitted from analysis')\n",
    "                # No need to set a state for inorganic soils, continue to next pixel\n",
    "                continue\n",
    "\n",
    "    return soil, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c86ea4ee-0318-425a-bfc2-8142b9ff6d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 13:00:57,212 - INFO - Processing tile 00N_110E with bounds 116_-6_118_-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 13:00:57,605 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-11 13:00:57,615 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-11 13:00:57,643 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-11 13:00:57,652 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-11 13:00:57,653 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2024-05-11 13:00:57,654 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile id ses_2020 exists. Proceeding.\n",
      "Requesting data in chunk 116_-6_118_-4 in 00N_110E: 20240511_13_00_57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 13:03:11,868 - ERROR - Failed processing for 116_-6_118_-4: ecozone dtype not in list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for requests for data in chunk 116_-6_118_-4 in 00N_110E: 20240511_13_03_11\n",
      "{'peat': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 'land_cover_2020': array([[6, 6, 6, ..., 0, 6, 6],\n",
      "       [6, 6, 6, ..., 0, 6, 6],\n",
      "       [6, 6, 6, ..., 0, 6, 6],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 6, 6, 6],\n",
      "       [6, 6, 6, ..., 6, 6, 6],\n",
      "       [6, 6, 6, ..., 6, 6, 6]], dtype=uint8), 'planted_forest_type': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 'planted_forest_tree_crop': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 'ecozone': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint16), 'dadap': array([[          nan,           nan,           nan, ...,           nan,\n",
      "                  nan,           nan],\n",
      "       [          nan,           nan,           nan, ...,           nan,\n",
      "                  nan,           nan],\n",
      "       [          nan,           nan,           nan, ...,           nan,\n",
      "                  nan,           nan],\n",
      "       ...,\n",
      "       [3.4028235e+38, 3.4028235e+38, 3.4028235e+38, ..., 3.4028235e+38,\n",
      "        3.4028235e+38, 3.4028235e+38],\n",
      "       [3.4028235e+38, 3.4028235e+38, 3.4028235e+38, ..., 3.4028235e+38,\n",
      "        3.4028235e+38, 3.4028235e+38],\n",
      "       [3.4028235e+38, 3.4028235e+38, 3.4028235e+38, ..., 3.4028235e+38,\n",
      "        3.4028235e+38, 3.4028235e+38]], dtype=float32)}\n",
      "Data in chunk 116_-6_118_-4. Proceeding.\n",
      "CPU times: user 4.81 s, sys: 3.41 s, total: 8.22 s\n",
      "Wall time: 2min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Create LULUCF flux and carbon stock 2x2 deg rasters \n",
    "\n",
    "## Area to analyze\n",
    "## chunk_params arguments: W, S, E, N, chunk size (degrees)\n",
    "# chunk_params = [-180, -60, 180, 80, 2]  # entire world\n",
    "# chunk_params = [-10, 40, 20, 70, 1]    # 30x30 deg (70N_010W), 900 chunks\n",
    "\n",
    "# chunk_params = [-10, 60, 0, 70, 1]    # 10x10 deg (70N_010W), 100 chunks\n",
    "# chunk_params = [-10, 65, -5, 70, 1]    # 5x5 deg (70N_010W), 25 chunks\n",
    "# chunk_params = [-10, 68, -8, 70, 1]    # 2x2 deg (70N_010W), 4 chunks\n",
    "# chunk_params = [-10, 69, -9, 70, 1]    # 1x1 deg (70N_010W), 1 chunk\n",
    "\n",
    "# chunk_params = [10, 40, 20, 50, 2]    # 10x10 deg (50N_010E), 25 chunks\n",
    "# chunk_params = [10, 40, 20, 50, 10]    # 10x10 deg (50N_010E), 1 chunk\n",
    "# chunk_params = [10, 46, 14, 50, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [110, -10, 114, -6, 2]   # 4x4 deg, 4 chunks\n",
    "# chunk_params = [10, 48, 12, 50, 1]   # 2x2 deg, 4 chunks\n",
    "# chunk_params = [10, 49, 11, 50, 1]   # 1x1 deg, 1 chunk\n",
    "# chunk_params = [10, 49, 11, 50, 0.5] # 1x1 deg, 4 chunks\n",
    "# chunk_params = [10, 49.5, 10.5, 50, 0.25] # 0.5x0.5 deg, 4 chunks\n",
    "# chunk_params = [10, 42, 11, 43, 0.5] # 1x1 deg, 4 chunks (some GLCLU code=254 for ocean and some land, so data should be output)\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data, no fire)\n",
    "#chunk_params = [15, 41.75, 15.25, 42, 0.25] # 0.25x0.25 deg, 1 chunk (has data with fire)\n",
    "\n",
    "# # Range of no-data cases for testing\n",
    "# chunk_params = [20, 69.75, 20.25, 70, 0.25] # 0.25x0.25 deg, 1 chunk (tile exists for GLCLU but not all other inputs, e.g., fire)\n",
    "# chunk_params = [110, -10, 120, 0, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [110, -20, 120, -10, 2]    # 10x10 deg (00N_110E), 25 chunks (all chunks have land and should be output)\n",
    "# chunk_params = [0, 79.75, 0.25, 80, 0.25] # 0.25x0.25 deg, 1 chunk (no 80N_000E tile-- no data)\n",
    "# chunk_params = [112, -12, 116, -8, 2]   # 2x2 deg, 1 chunk (bottom of Java, has data but mostly ocean)\n",
    "# chunk_params = [10.875, 41.75, 11, 42, 0.25] # 0.25x0.25 deg, 1 chunk (entirely GLCLU code=255 for ocean, so no actual data-- nothing should be be output)\n",
    "# chunk_params = [-10, 21.75, -9.75, 22, 0.25] # 0.25x0.25 deg, 1 chunk (has data but entirely desert (fully GLCLU code=0))\n",
    "# chunk_params = [10, 49.75, 10.25, 50, 0.25] # 0.25x0.25 deg, 1 chunk (has data)\n",
    "\n",
    "chunk_params = [116, -6, 118, -4, 2] # 1 chunk, has data\n",
    "\n",
    "# Makes list of chunks to analyze\n",
    "chunks = get_chunk_bounds(chunk_params)  \n",
    "print(\"Processing\", len(chunks), \"chunks\")\n",
    "# print(chunks)\n",
    "\n",
    "# Determines if the output file names for final versions of outputs should be used\n",
    "is_final = False\n",
    "if len(chunks) > 90:\n",
    "    is_final = True\n",
    "    print(\"Running as final model.\")\n",
    "\n",
    "# Creates list of tasks to run (1 task = 1 chunk for all years)\n",
    "delayed_result = [dask.delayed(calculcate_and_upload_drainage)(chunk, is_final) for chunk in chunks]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8126ac-c1fc-4221-bd43-4a494f45846d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
