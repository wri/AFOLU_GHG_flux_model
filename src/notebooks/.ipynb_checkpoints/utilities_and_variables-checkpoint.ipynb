{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b03b028-4f73-430d-99ee-2f0c3abdfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import time\n",
    "import math\n",
    "import ctypes\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# dask/parallelization libraries\n",
    "import coiled\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.distributed import print as dask_print\n",
    "import distributed\n",
    "\n",
    "# scipy basics\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.transform\n",
    "import rasterio.windows\n",
    "#import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "from rioxarray.merge import merge_arrays\n",
    "\n",
    "from numba import jit\n",
    "import concurrent.futures\n",
    "\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185f97d9-ee21-42ad-9e49-097878c8639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General paths and constants\n",
    "\n",
    "s3_out_dir = 'climate/AFOLU_flux_model/organic_soils/outputs'\n",
    "\n",
    "IPCC_class_max_val = 6\n",
    "\n",
    "# IPCC codes\n",
    "forest = 1\n",
    "cropland = 2\n",
    "settlement = 3\n",
    "wetland = 4\n",
    "grassland = 5\n",
    "otherland = 6\n",
    "\n",
    "first_year = 2000\n",
    "last_year = 2020\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('gfw2-data')\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "full_raster_dims = 40000\n",
    "\n",
    "interval_years = 5   # number of years in interval. #TODO: calculate programmatically in numba function rather than coded here-- for greater flexibility.\n",
    "\n",
    "sig_height_loss_threshold = 5   # meters\n",
    "\n",
    "biomass_to_carbon = 0.47   # Conversion of biomass to carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6fdfaf-0068-4e95-a450-b26dd8cff5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name patterns\n",
    "\n",
    "### IPCC classes and change\n",
    "IPCC_class_path = \"IPCC_basic_classes\"\n",
    "IPCC_class_pattern = \"IPCC_classes\"\n",
    "\n",
    "land_cover = \"land_cover\"\n",
    "\n",
    "planted_forest_type_layer = \"planted_forest_type\"\n",
    "planted_forest_tree_crop_layer = \"planted_forest_tree_crop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58988e0-ec13-4ffc-91ad-a2b9c524d449",
   "metadata": {},
   "source": [
    "<font size=\"6\">General functions</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa51beb-6d2b-4db2-a374-db2dd3c176cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestr():\n",
    "    return time.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "\n",
    "def boundstr(bounds):\n",
    "    bounds_str = \"_\".join([str(round(x)) for x in bounds])\n",
    "    return bounds_str\n",
    "\n",
    "def calc_chunk_length_pixels(bounds):\n",
    "    chunk_length_pixels = int((bounds[3]-bounds[1]) * (40000/10))\n",
    "    return chunk_length_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599e9dd3-60ea-4b42-9760-51721f20ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the W, S, E, N bounds of a 10x10 degree tile\n",
    "def get_10x10_tile_bounds(tile_id):\n",
    "    \n",
    "    if \"S\" in tile_id:\n",
    "        max_y = -1 * (int(tile_id[:2]))\n",
    "        min_y = -1 * (int(tile_id[:2])+10)\n",
    "    else: \n",
    "        max_y = (int(tile_id[:2]))\n",
    "        min_y = (int(tile_id[:2])-10)\n",
    "\n",
    "    if \"W\" in tile_id:\n",
    "        max_x = -1 * (int(tile_id[4:7])-10)\n",
    "        min_x = -1 * (int(tile_id[4:7]))\n",
    "    else: \n",
    "        max_x = (int(tile_id[4:7])+10)\n",
    "        min_x = (int(tile_id[4:7]))\n",
    "\n",
    "    return min_x, min_y, max_x, max_y      # W, S, E, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a7c613-1283-4ccf-abf7-bd908315945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of all chunk boundaries within a bounding box for chunks of a given size\n",
    "def get_chunk_bounds(chunk_params):\n",
    "\n",
    "    min_x = chunk_params[0]\n",
    "    min_y = chunk_params[1]\n",
    "    max_x = chunk_params[2]\n",
    "    max_y = chunk_params[3]\n",
    "    chunk_size = chunk_params[4]\n",
    "    \n",
    "    x, y = (min_x, min_y)\n",
    "    chunks = []\n",
    "\n",
    "    # Polygon Size\n",
    "    while y < max_y:\n",
    "        while x < max_x:\n",
    "            bounds = [\n",
    "                x,\n",
    "                y,\n",
    "                x + chunk_size,\n",
    "                y + chunk_size,\n",
    "            ]\n",
    "            chunks.append(bounds)\n",
    "            x += chunk_size\n",
    "        x = min_x\n",
    "        y += chunk_size\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8e89ff-01ff-4446-83db-39c3910d0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the encompassing tile_id string in the form YYN/S_XXXE/W based on a coordinate\n",
    "def xy_to_tile_id(top_left_x, top_left_y):\n",
    "\n",
    "    lat_ceil = math.ceil(top_left_y/10.0) * 10\n",
    "    lng_floor = math.floor(top_left_x/10.0) * 10\n",
    "    \n",
    "    lng: str = f\"{str(lng_floor).zfill(3)}E\" if (lng_floor >= 0) else f\"{str(-lng_floor).zfill(3)}W\"\n",
    "    lat: str = f\"{str(lat_ceil).zfill(2)}N\" if (lat_ceil >= 0) else f\"{str(-lat_ceil).zfill(2)}S\"\n",
    "\n",
    "    return f\"{lat}_{lng}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09504ef-06ec-444d-8ea3-bd15154c6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazily opens tile within provided bounds (i.e. one chunk) and returns as a numpy array\n",
    "# If it can't open the chunk (no data in it), it returns an array of the specified nodata value.\n",
    "# TODO: It would be better if this just didn't return any array at all if there's no chunk. Returning an array of nodata is pretty inefficient.  \n",
    "def get_tile_dataset_rio(uri, bounds, chunk_length_pixels, no_data_val):\n",
    "\n",
    "    bounds_str = boundstr(bounds)\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(uri) as ds:\n",
    "            window = rasterio.windows.from_bounds(*bounds, ds.transform)\n",
    "            data = ds.read(1, window=window)\n",
    "    except:\n",
    "        data = np.full((chunk_length_pixels, chunk_length_pixels), no_data_val)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13dfd7a3-1173-4a04-88dd-6d847ccb9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares list of chunks to download.\n",
    "# Chunks are defined by a bounding box.\n",
    "def prepare_to_download_chunk(bounds, download_dict, no_data_val):\n",
    " \n",
    "    futures = {}\n",
    "\n",
    "    bounds_str = boundstr(bounds)\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)\n",
    "\n",
    "    # Submit requests to S3 for input chunks but don't actually download them yet. This queueing of the requests before downloading them speeds up the downloading\n",
    "    # Approach is to download all the input chunks up front for every year to make downloading more efficient, even though it means storing more upfront\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        dask_print(f\"Requesting data in chunk {bounds_str} in {tile_id}: {timestr()}\")\n",
    "\n",
    "        for key, value in download_dict.items():\n",
    "            futures[executor.submit(get_tile_dataset_rio, value, bounds, chunk_length_pixels, no_data_val)] = key\n",
    "\n",
    "    return futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c26e191-cadf-47f1-8135-436b319081d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if tiles exist at all\n",
    "def check_for_tile(download_dict):\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while i < len(list(download_dict.values())):\n",
    "\n",
    "        s3_key = list(download_dict.values())[i][15:]\n",
    "\n",
    "        # Breaks the loop if the tile exists\n",
    "        try:\n",
    "            s3.head_object(Bucket='gfw2-data', Key=s3_key)\n",
    "            dask_print(f\"Tile id {list(download_dict.values())[i][-12:-4]} exists. Proceeding.\")\n",
    "            return 1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    dask_print(f\"Tile id {list(download_dict.values())[0][-12:-4]} does not exist. Skipping chunk.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "118219c7-4151-42c8-a044-30a1f1709039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks whether a chunk has data in it\n",
    "def check_chunk_for_data(layers, item_to_check, bounds_str, tile_id, no_data_val):\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while i < len(list(layers.values())):\n",
    "\n",
    "        # Checks if all the pixels have the nodata value\n",
    "        min = np.min(list(layers.values())[i])  # Can't use np.all because it doesn't work in chunks that are mostly water; says nodata in chunk even if there is land\n",
    "\n",
    "        # Breaks the loop if there is data in the chunk\n",
    "        if min < no_data_val:\n",
    "            dask_print(f\"Data in chunk {bounds_str}. Proceeding.\")\n",
    "            return 1\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    dask_print(f\"No data in chunk {bounds_str} for any input.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a63159-8e5e-4787-9fb8-0b696de77492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saves array as a raster locally, then uploads it to s3\n",
    "# def save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, output_dict, is_final):\n",
    "\n",
    "#     s3_client = boto3.client(\"s3\") # Needs to be in the same function as the upload_file call\n",
    "\n",
    "#     transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "\n",
    "#     file_info = f'{tile_id}__{bounds_str}'\n",
    "\n",
    "#     # For every output file, saves from array to local raster, then to s3.\n",
    "#     # Can't save directly to s3, unfortunately, so need to save locally first.\n",
    "#     for key, value in output_dict.items():\n",
    "\n",
    "#         data_array = value[0]\n",
    "#         data_meaning = value[2]\n",
    "#         year_out = value[3]\n",
    "\n",
    "#         array_dtype = data_array.dtype\n",
    "\n",
    "#         if not is_final:\n",
    "#             dask_print(f\"Saving {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "#         if is_final:\n",
    "#             file_name = f\"{file_info}__{key}.tif\"\n",
    "#         else:\n",
    "#             file_name = f\"{file_info}__{key}__{timestr()}.tif\"\n",
    "\n",
    "#         with rasterio.open(f\"/tmp/{file_name}\", 'w', driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels, count=1, dtype='uint8', crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "#             dst.write(data_array.astype(rasterio.uint8), 1)\n",
    "\n",
    "#         s3_path = f\"{s3_out_dir}/{data_meaning}/{year_out}/{chunk_length_pixels}_pixels/{time.strftime('%Y%m%d')}\"\n",
    "\n",
    "#         if not is_final:\n",
    "#             dask_print(f\"Uploading {bounds_str} in {tile_id} for {year_out} to {s3_path}: {timestr()}\")\n",
    "\n",
    "#         s3_client.upload_file(f\"/tmp/{file_name}\", \"gfw2-data\", Key=f\"{s3_path}/{file_name}\")\n",
    "\n",
    "#         # Deletes the local raster\n",
    "#         os.remove(f\"/tmp/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6760924c-160e-44bc-9898-62b127c99495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, bounds_str, output_dict, is_final, output_dir):\n",
    "    s3_client = boto3.client(\"s3\")  # Needs to be in the same function as the upload_file call\n",
    "\n",
    "    transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "\n",
    "    file_info = f'{tile_id}__{bounds_str}'\n",
    "\n",
    "    # For every output file, saves from array to local raster, then to s3.\n",
    "    for key, value in output_dict.items():\n",
    "        data_array = value[0]\n",
    "        data_meaning = value[2]\n",
    "        year_out = value[3]\n",
    "\n",
    "        if not is_final:\n",
    "            dask_print(f\"Saving {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "        file_name = f\"{file_info}__{key}.tif\" if is_final else f\"{file_info}__{key}__{timestr()}.tif\"\n",
    "\n",
    "        # Define path where the files will be saved on S3\n",
    "        s3_path = f\"{output_dir}/{data_meaning}/{year_out}/{chunk_length_pixels}_pixels/{time.strftime('%Y%m%d')}\"\n",
    "\n",
    "        with rasterio.open(f\"/tmp/{file_name}\", 'w', driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels,\n",
    "                           count=1, dtype=data_array.dtype, crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "            dst.write(data_array, 1)\n",
    "\n",
    "        if not is_final:\n",
    "            dask_print(f\"Uploading {bounds_str} in {tile_id} for {year_out} to {s3_path}: {timestr()}\")\n",
    "\n",
    "        s3_client.upload_file(f\"/tmp/{file_name}\", \"gfw2-data\", f\"{s3_path}/{file_name}\")\n",
    "\n",
    "        # Deletes the local raster\n",
    "        os.remove(f\"/tmp/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d4e5c0-48e6-4169-834e-e1fac16f8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists rasters in an s3 folder and returns their names as a list\n",
    "def list_rasters_in_folder(full_in_folder):\n",
    "\n",
    "    cmd = ['aws', 's3', 'ls', full_in_folder]\n",
    "    s3_contents_bytes = subprocess.check_output(cmd)\n",
    "\n",
    "    # Converts subprocess results to useful string\n",
    "    s3_contents_str = s3_contents_bytes.decode('utf-8')\n",
    "    s3_contents_list = s3_contents_str.splitlines()\n",
    "    rasters = [line.split()[-1] for line in s3_contents_list]\n",
    "    rasters = [i for i in rasters if \"tif\" in i]\n",
    "\n",
    "    return rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ddc2d7-2427-4f47-8e16-ed01c507f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads a shapefile to s3\n",
    "def upload_shp(full_in_folder, in_folder, shp):\n",
    "\n",
    "    dask_print(f\"Uploading to {full_in_folder}{shp}: {timestr()}\")\n",
    "\n",
    "    shp_pattern = shp[:-4]\n",
    "\n",
    "    s3_client = boto3.client(\"s3\")  # Needs to be in the same function as the upload_file call\n",
    "    s3_client.upload_file(f\"/tmp/{shp}\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp}\")\n",
    "    s3_client.upload_file(f\"/tmp/{shp_pattern}.dbf\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp_pattern}.dbf\")\n",
    "    s3_client.upload_file(f\"/tmp/{shp_pattern}.prj\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp_pattern}.prj\")\n",
    "    s3_client.upload_file(f\"/tmp/{shp_pattern}.shx\", \"gfw2-data\", Key=f\"{in_folder[10:]}{shp_pattern}.shx\")\n",
    "\n",
    "    os.remove(f\"/tmp/{shp}\")\n",
    "    os.remove(f\"/tmp/{shp_pattern}.dbf\")\n",
    "    os.remove(f\"/tmp/{shp_pattern}.prj\")\n",
    "    os.remove(f\"/tmp/{shp_pattern}.shx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b56bce-7231-4a98-8bda-be30e47e25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a shapefile of the footprints of rasters in a folder, for checking geographical completeness of rasters\n",
    "def make_tile_footprint_shp(input_dict):\n",
    "\n",
    "    in_folder = list(input_dict.keys())[0]\n",
    "    pattern = list(input_dict.values())[0]\n",
    "\n",
    "    # Task properties\n",
    "    dask_print(f\"Making tile index shapefile for: {in_folder}: {timestr()}\")\n",
    "\n",
    "    # Folder including s3 key\n",
    "    s3_in_folder = f's3://{in_folder}'\n",
    "    vsis3_in_folder = f'/vsis3/{in_folder}'\n",
    "\n",
    "    # List of all the filenames in the folder\n",
    "    filenames = list_rasters_in_folder(s3_in_folder)\n",
    "\n",
    "    # List of the tile paths in the folder\n",
    "    tile_paths = []\n",
    "    tile_paths = [vsis3_in_folder + filename for filename in filenames]\n",
    "\n",
    "    file_paths = 's3_paths.txt'\n",
    "\n",
    "    with open(f\"/tmp/{file_paths}\", 'w') as file:\n",
    "        for item in tile_paths:\n",
    "            file.write(item + '\\n')\n",
    "\n",
    "    # Output shapefile name\n",
    "    shp = f\"raster_footprints_{pattern}.shp\"\n",
    "\n",
    "    cmd = [\"gdaltindex\", \"-t_srs\", \"EPSG:4326\", f\"/tmp/{shp}\", \"--optfile\", f\"/tmp/{file_paths}\"]\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "    # Uploads shapefile to s3\n",
    "    upload_shp(s3_in_folder, in_folder, shp)\n",
    "\n",
    "    return(f\"Completed: {timestr()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a2ae67e-adbe-465d-8d97-2bf00fe9b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an xarray data array locally as a raster and then uploads it to s3\n",
    "def save_and_upload_raster_10x10(**kwargs):\n",
    "\n",
    "    s3_client = boto3.client(\"s3\") # Needs to be in the same function as the upload_file call\n",
    "\n",
    "    data_array = kwargs['data']   # The data being saved\n",
    "    out_file_name = kwargs['out_file_name']   # The output file name\n",
    "    out_folder = kwargs['out_folder']   # The output folder\n",
    "\n",
    "    dask_print(f\"Saving {out_file_name} locally\")\n",
    "\n",
    "    profile_kwargs = {'compress': 'lzw'}   # Adds attribute to compress the output raster \n",
    "    # data_array.rio.to_raster(f\"{out_file_name}\", **profile_kwargs)\n",
    "    data_array.rio.to_raster(f\"/tmp/{out_file_name}\", **profile_kwargs)\n",
    "\n",
    "    dask_print(f\"Saving {out_file_name} to {out_folder[10:]}{out_file_name}\")\n",
    "\n",
    "    s3_client.upload_file(f\"/tmp/{out_file_name}\", \"gfw2-data\", Key=f\"{out_folder[10:]}{out_file_name}\")\n",
    "\n",
    "    # Deletes the local raster\n",
    "    os.remove(f\"/tmp/{out_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12628166-1647-41be-87af-35e873510f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of tiles to aggregate, where the list is a list of dictionaries of the form \n",
    "# [{'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_000E__IPCC_classes_2020.tif', 0]}, \n",
    "# {'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_010E__IPCC_classes_2020.tif', 0]}, ... ]\n",
    "def create_list_for_aggregation(s3_in_folder_dicts):\n",
    "\n",
    "    s3_in_folders = [list(item.keys())[0] for item in s3_in_folder_dicts]\n",
    "    no_data_values = [list(item.values())[0] for item in s3_in_folder_dicts]\n",
    "\n",
    "    list_of_s3_name_dicts_total = []   # Final list of dictionaries of s3 paths and output aggregated 10x10 rasters\n",
    "    \n",
    "    # Iterates through all the desired s3 folders\n",
    "    for s3_in_folder, no_data_value in zip(s3_in_folders, no_data_values):\n",
    "    \n",
    "        simple_file_names = []   # List of output aggregatd 10x10 rasters\n",
    "    \n",
    "        # Raw filenames in a folder, e.g., ['00N_000E__6_-2_8_0__IPCC_classes_2020.tif', '00N_000E__6_-4_8_-2__IPCC_classes_2020.tif',...]\n",
    "        filenames = list_rasters_in_folder(f\"s3://{s3_in_folder}\")\n",
    "    \n",
    "        # Iterates through all the files in a folder and converts them to the output names. \n",
    "        # Essentially [tile_id]__[pattern].tif. Drops the chunk bounds from the middle.\n",
    "        for filename in filenames:\n",
    "        \n",
    "            result = filename[:10] + filename[filename.rfind(\"__\") + len(\"__\"):]   # Extracts the relevant parts of the raw file names\n",
    "            simple_file_names.append(result)   # New list of simplified file names used for 10x10 degree outputs\n",
    "    \n",
    "        # Removes duplicate simplified file names.\n",
    "        # There are duplicates because each 10x10 output raster has many constituent chunks, each of which have the same aggregated, final name\n",
    "        # e.g., ['00N_000E__IPCC_classes_2020.tif', '00N_010E__IPCC_classes_2020.tif', ...]\n",
    "        simple_file_names = np.unique(simple_file_names).tolist()\n",
    "\n",
    "        # Makes nested lists of the file names and no data values inside the list of all file names, \n",
    "        # e.g., [['00N_000E__IPCC_classes_2020.tif', 0], ['00N_010E__IPCC_classes_2020.tif', 0], ... ]\n",
    "        simple_file_names_and_no_data = [[item, no_data_value] for item in simple_file_names]\n",
    "    \n",
    "        # Makes a list of dictionaries, where the key is the input s3 path and the value is the output aggregated name\n",
    "        # e.g., [{'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_000E__IPCC_classes_2020.tif', 0]}, \n",
    "        # {'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_010E__IPCC_classes_2020.tif', 0]}, ... ]\n",
    "        list_of_s3_name_dicts = [{key: value} for value in simple_file_names_and_no_data for key in [s3_in_folder]]\n",
    "    \n",
    "        # Adds the dictionary of s3 paths and output names for this folder to the list for all folders\n",
    "        list_of_s3_name_dicts_total.append(list_of_s3_name_dicts)\n",
    "    \n",
    "    # Output of above is a nested list, where each input folder is its own inner list. Need to flatten to a list.\n",
    "    # e.g., [{'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_000E__IPCC_classes_2020.tif', 0]},\n",
    "    # {'gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_classes/2020/8000_pixels/20240205/': ['00N_010E__IPCC_classes_2020.tif', 0]}, ... ]\n",
    "    list_of_s3_name_dicts_total = flatten_list(list_of_s3_name_dicts_total)\n",
    "    \n",
    "    print(f\"There are {len(list_of_s3_name_dicts_total)} chunks to process in {len(s3_in_folders)} input folders.\")\n",
    "\n",
    "    return list_of_s3_name_dicts_total\n",
    "\n",
    "# Flattens a nested list\n",
    "def flatten_list(nested_list):\n",
    "    return [x for xs in nested_list for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c35d26b3-ab0f-4cde-a18b-5e62e169a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges rasters that are <10x10 degrees into 10x10 degree rasters in the standard grid. Not currently using.\n",
    "# Approach is to open rasters, convert to xarray, merge as xarray datarrays, then save as rasters locally and upload to s3.\n",
    "def merge_small_tiles_xarray(s3_name_dict):\n",
    "\n",
    "    in_folder = list(s3_name_dict.keys())[0]   # The input s3 folder for the small rasters\n",
    "    out_file_name = list(s3_name_dict.values())[0]   # The output file name for the combined rasters\n",
    "\n",
    "    s3_in_folder = f's3://{in_folder}'   # The input s3 folder with s3:// prepended\n",
    "    vsis3_in_folder = f'/vsis3/{in_folder}'   # The input s3 folder with /vsis3/ prepended\n",
    "\n",
    "    # Lists all the rasters in the specified s3 folder\n",
    "    filenames = list_rasters_in_folder(s3_in_folder)   \n",
    "\n",
    "    # Gets the tile_id from the output file name in the standard format\n",
    "    tile_id = out_file_name[:8]\n",
    "\n",
    "    # Limits the input rasters to the specified tile_id (the relevant 10x10 area)\n",
    "    filenames_in_focus_area = [i for i in filenames if tile_id in i]\n",
    "    \n",
    "    # Lists the tile paths for the relevant rasters\n",
    "    tile_paths = []\n",
    "    tile_paths = [s3_in_folder + filename for filename in filenames_in_focus_area]\n",
    "\n",
    "    dask_print(f\"Opening small rasters in {tile_id} in {s3_in_folder}\")\n",
    "\n",
    "    # Opens the relevant rasters in a list of xarray data arrays\n",
    "    small_rasters = [rioxarray.open_rasterio(tile_path, chunks=True) for tile_path in tile_paths]\n",
    "\n",
    "    dask_print(f\"Merging {tile_id} in {s3_in_folder}\")\n",
    "\n",
    "    nodata_value = 255\n",
    "\n",
    "    min_x, min_y, max_x, max_y = get_10x10_tile_bounds(tile_id)   # The bounding box for the output 10x10 deg tile\n",
    "\n",
    "    # Merges the relevant small data arrays in the list\n",
    "    # https://corteva.github.io/rioxarray/stable/examples/merge.html\n",
    "    merged = merge_arrays(small_rasters, bounds=(min_x, min_y, max_x, max_y), nodata=nodata_value)  # Bounds of the output image (left, bottom, right, top)) \n",
    "\n",
    "    # Names the output folder. Same as the input folder but with the dimensions in pixels replaced\n",
    "    out_folder = re.sub(r'\\d+_pixels', f'{full_raster_dims}_pixels', in_folder)\n",
    "\n",
    "    # Saves the merged xarray data array locally and then to s3 \n",
    "    save_and_upload_raster_10x10(data=merged, out_file_name=out_file_name, out_folder=out_folder)\n",
    "\n",
    "    del merged\n",
    "\n",
    "    return f\"success for {s3_name_dict}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e615713f-7dcd-4a35-bb29-4efbd4dfa336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges rasters that are <10x10 degrees into 10x10 degree rasters in the standard grid.\n",
    "# Approach is to merge rasters with gdal.Warp and then upload them to s3.\n",
    "def merge_small_tiles_gdal(s3_name_no_data_dict):\n",
    "    \n",
    "    in_folder = list(s3_name_no_data_dict.keys())[0]   # The input s3 folder for the small rasters\n",
    "    out_file_name_no_data = list(s3_name_no_data_dict.values())[0]   # The output file name for the combined rasters and their no data value\n",
    "    out_file_name = out_file_name_no_data[0]    # The output file name\n",
    "    no_data = out_file_name_no_data[1]    # The output no data value. Not currently using but it's available.\n",
    "\n",
    "    s3_in_folder = f's3://{in_folder}'   # The input s3 folder with s3:// prepended\n",
    "    vsis3_in_folder = f'/vsis3/{in_folder}'   # The input s3 folder with /vsis3/ prepended\n",
    "\n",
    "    # Lists all the rasters in the specified s3 folder\n",
    "    filenames = list_rasters_in_folder(s3_in_folder)   \n",
    "\n",
    "    # Gets the tile_id from the output file name in the standard format\n",
    "    tile_id = out_file_name[:8]\n",
    "\n",
    "    # Limits the input rasters to the specified tile_id (the relevant 10x10 area)\n",
    "    filenames_in_focus_area = [i for i in filenames if tile_id in i]\n",
    "    \n",
    "    # Lists the tile paths for the relevant rasters\n",
    "    tile_paths = []\n",
    "    tile_paths = [vsis3_in_folder + filename for filename in filenames_in_focus_area]\n",
    "\n",
    "    dask_print(f\"Merging small rasters in {tile_id} in {vsis3_in_folder}\")\n",
    "\n",
    "    # Names the output folder. Same as the input folder but with the dimensions in pixels replaced\n",
    "    out_folder = re.sub(r'\\d+_pixels', f'{full_raster_dims}_pixels', in_folder[10:])   # [10:] to remove the gfw2-data/ at the front\n",
    "\n",
    "    min_x, min_y, max_x, max_y = get_10x10_tile_bounds(tile_id)\n",
    "\n",
    "    output_extent = [min_x, min_y, max_x, max_y]  # Specify the extent in the order [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    warp_options = gdal.WarpOptions(outputBounds=output_extent, creationOptions=[\"COMPRESS=LZW\"])\n",
    "    # warp_options = gdal.WarpOptions(outputBounds=output_extent, creationOptions=[\"COMPRESS=LZW\"], dstNodata=no_data)\n",
    "\n",
    "    # Merges all output small rasters with the options above\n",
    "    gdal.Warp(f\"/tmp/{out_file_name}\", tile_paths, options=warp_options)\n",
    "\n",
    "    s3_client = boto3.client(\"s3\") # Needs to be in the same function as the upload_file call\n",
    "\n",
    "    dask_print(f\"Saving {out_file_name} to s3: {out_folder}{out_file_name}\")\n",
    "    \n",
    "    s3_client.upload_file(f\"/tmp/{out_file_name}\", \"gfw2-data\", Key=f\"{out_folder}{out_file_name}\")\n",
    "\n",
    "    # Deletes the local raster\n",
    "    os.remove(f\"/tmp/{out_file_name}\")\n",
    "\n",
    "    return f\"success for {s3_name_no_data_dict}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e67726a-a758-4d83-a82d-9b958ce245d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_id(tile):\n",
    "    tile_id = tile.replace('.tif', '')\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a6b289c-9eeb-405a-9d97-912f23158ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_raster(file_path, expected_crs):\n",
    "    with rioxarray.open_rasterio(file_path) as raster:\n",
    "        assert raster.rio.count() > 0, \"Raster file is empty after writing\"\n",
    "        assert raster.rio.crs == expected_crs, \"Raster file CRS does not match expected CRS\"\n",
    "        print(f\"Verification successful for {file_path}. Data and projection are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec02f83-bb0e-4d34-b44e-c72909d3a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_dataset(uri, tile, name, template=None):\n",
    "    try:\n",
    "        return rioxarray.open_rasterio(uri + tile, chunks=4000, default_name=name).squeeze(\"band\")\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        if template is not None:\n",
    "            return xr.zeros_like(template)\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d351527-8723-4252-bd65-5cef3cd429e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(uri, name, template=None):\n",
    "    try:\n",
    "        return rioxarray.open_rasterio(uri, chunks=4000, default_name=name).squeeze(\"band\")\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        if template is not None:\n",
    "            return xr.zeros_like(template)\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429e000-0cbd-47d6-b3a8-98d3a465aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def accrete_node(combo, new):\n",
    "    combo = combo*10 + new\n",
    "    return combo\n",
    "\n",
    "# accrete_node(1, 1)\n",
    "# accrete_node(13, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0db6a475-b681-4f08-a6e5-09a3d8a4f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Function to track the number of land use changes per pixel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
