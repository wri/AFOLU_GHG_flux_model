{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863578b1-33f7-4b09-9f94-70aca384a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# dask/parallelization libraries\n",
    "import coiled\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.distributed import print as dask_print\n",
    "import dask.config\n",
    "import distributed\n",
    "\n",
    "# scipy basics\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import rasterio.transform\n",
    "import rasterio.windows\n",
    "import rioxarray\n",
    "from rioxarray.merge import merge_arrays\n",
    "\n",
    "from numba import jit\n",
    "import concurrent.futures\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "import math\n",
    "import ctypes\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac756f2-deb1-4977-a306-d8dde7dcba79",
   "metadata": {},
   "source": [
    "<font size=\"6\">Making cloud and local clusters</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc533d0-5c8b-4205-ad55-483320511596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cluster\n",
    "coiled_cluster = coiled.Cluster(\n",
    "    n_workers=20,\n",
    "    use_best_zone=True, \n",
    "    compute_purchase_option=\"spot_with_fallback\",\n",
    "    idle_timeout=\"10 minutes\",\n",
    "    region=\"us-east-1\",\n",
    "    name=\"AFOLU_flux_model\", \n",
    "    account='jterry64', # Necessary to use the AWS environment that Justin set up in Coiled\n",
    "    worker_memory = \"32GiB\" \n",
    ")\n",
    "\n",
    "# Coiled cluster (cloud run)\n",
    "coiled_client = coiled_cluster.get_client()\n",
    "coiled_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def9e14-a4f2-42b0-8523-b7c9e8a82cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cluster\n",
    "coiled_cluster = coiled.Cluster(\n",
    "    n_workers=1,\n",
    "    use_best_zone=True, \n",
    "    compute_purchase_option=\"spot_with_fallback\",\n",
    "    idle_timeout=\"10 minutes\",\n",
    "    region=\"us-east-1\",\n",
    "    name=\"AFOLU_flux_model\", \n",
    "    account='jterry64', # Necessary to use the AWS environment that Justin set up in Coiled\n",
    "    worker_memory = \"64GiB\" \n",
    ")\n",
    "\n",
    "# Coiled cluster (cloud run)\n",
    "coiled_client = coiled_cluster.get_client()\n",
    "coiled_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e009b-89d0-4811-b9c1-71bbdc5370b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local single-process cluster (local run). Will run .compute() on just one process, not a whole cluster.\n",
    "local_client = Client(processes=False)\n",
    "local_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8d577-20a7-407c-b443-36b265bcb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_client = Client()\n",
    "local_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5a10e-be93-444c-9a90-5d61d3cdd278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local cluster with multiple workers\n",
    "local_cluster = LocalCluster()  \n",
    "local_client = Client(local_cluster)\n",
    "local_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d658bf-e757-4846-94a7-83db0c1af4d9",
   "metadata": {},
   "source": [
    "<font size=\"6\">Shutting down cloud and local clusters</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df695f3-723c-4723-9cf0-2dd35193ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coiled_cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9db6f6-af00-4942-b473-309396f32ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12443761-7252-496c-bfb3-0d6058639d3c",
   "metadata": {},
   "source": [
    "<font size=\"6\">Scripts</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54acff-1de6-45ff-8904-54c7516e67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_folder = 'landcover/composite/2020/raw/'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('gfw2-data')\n",
    "\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43188cb7-2a82-4eac-a6dd-3790c4c48f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in list(range(2005, 2021, 5)):\n",
    "\n",
    "    small_raster_paths = []\n",
    "    \n",
    "    change_folder = f\"climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_change/{year-5}_{year}/2000_pixels/{time.strftime('%Y%m%d')}/\"\n",
    "   \n",
    "    for object_summary in my_bucket.objects.filter(Prefix=change_folder):\n",
    "        small_raster_paths.append(object_summary.key)\n",
    "    \n",
    "    # print(small_raster_paths)\n",
    "    \n",
    "    tile_string = \"50N_010E\"\n",
    "    \n",
    "    small_raster_paths = [i for i in small_raster_paths if tile_string in i]\n",
    "    \n",
    "    # print(small_raster_paths)\n",
    "    \n",
    "    small_raster_paths = ['s3://gfw2-data/' + path for path in small_raster_paths]\n",
    "    \n",
    "    print(small_raster_paths)\n",
    "\n",
    "    small_rasters = [rioxarray.open_rasterio(path, chunks=True) for path in small_raster_paths]\n",
    "\n",
    "    merged = merge_arrays(small_rasters)  # https://corteva.github.io/rioxarray/stable/examples/merge.html\n",
    "\n",
    "    out_file = f'merged_change_{year-5}_{year}.tif'\n",
    "\n",
    "    merged.rio.to_raster(f'/tmp/{out_file}')\n",
    "\n",
    "    s3_client.upload_file(f'/tmp/{out_file}', \"gfw2-data\", \n",
    "                          Key=f\"climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_change/{year-5}_{year}/40000_pixels/{time.strftime('%Y%m%d')}/{out_file}\")\n",
    "\n",
    "    os.remove(f'/tmp/{out_file}')\n",
    "\n",
    "    del merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f4c96-7268-4ad6-8978-b8437aab91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_upload(bounds, chunk_length_pixels, tile_id, bounds_str, output_dict):\n",
    "\n",
    "    transform = rasterio.transform.from_bounds(*bounds, width=chunk_length_pixels, height=chunk_length_pixels)\n",
    "\n",
    "    file_info = f'{tile_id}__{bounds_str}'\n",
    "\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    # For every output file, saves from array to local raster, then to s3.\n",
    "    # Can't save directly to s3, unfortunately, so need to save locally first.\n",
    "    for key, value in output_dict.items():\n",
    "\n",
    "        data_meaning = value[2]\n",
    "        year_out = value[3]\n",
    "\n",
    "        dask_print(f\"Saving {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "        file_name = f\"{file_info}__{key}__{timestr()}\"\n",
    "\n",
    "        with rasterio.open(f\"/tmp/{file_name}.tif\", 'w', driver='GTiff', width=chunk_length_pixels, height=chunk_length_pixels, count=1, dtype='uint8', crs='EPSG:4326', transform=transform, compress='lzw', blockxsize=400, blockysize=400) as dst:\n",
    "            dst.write(value[0].astype(rasterio.uint8), 1)\n",
    "\n",
    "        dask_print(f\"Uploading {bounds_str} in {tile_id} for {year_out}: {timestr()}\")\n",
    "\n",
    "        s3_client.upload_file(f\"/tmp/{file_name}.tif\", \"gfw2-data\", Key=f\"{s3_out_dir}/{data_meaning}/{year_out}/40000_pixels/{time.strftime('%Y%m%d')}/{file_name}.tif\")\n",
    "\n",
    "        # Deletes the local raster. It won't be used again.\n",
    "        os.remove(f\"/tmp/{file_name}.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b61101-0f7f-4bad-8b35-e7889799b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_tiles(year):\n",
    "\n",
    "    small_raster_paths = []\n",
    "    \n",
    "    change_folder = f\"climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_change/{year-5}_{year}/2000_pixels/{time.strftime('%Y%m%d')}/\"\n",
    "\n",
    "    cmd = ['aws', 's3', 'ls', f's3://gfw2-data/{change_folder}']\n",
    "\n",
    "    s3_contents_bytes = subprocess.check_output(cmd)\n",
    "    s3_contents_str = s3_contents_bytes.decode('utf-8')\n",
    "    s3_contents_list = s3_contents_str.splitlines()\n",
    "    filenames = [line.split()[-1] for line in s3_contents_list]\n",
    "    \n",
    "    small_raster_paths = [f's3://gfw2-data/{change_folder}' + filename for filename in filenames]\n",
    "\n",
    "    # dask_print(small_raster_paths)\n",
    "    \n",
    "    tile_string = \"50N_010E\"\n",
    "    \n",
    "    small_raster_paths = [i for i in small_raster_paths if tile_string in i]\n",
    "    \n",
    "    # print(small_raster_paths)\n",
    "\n",
    "    small_rasters = [rioxarray.open_rasterio(path, chunks=True) for path in small_raster_paths]\n",
    "\n",
    "    dask_print(f\"Merging {year-5}_{year} to s3\")\n",
    "\n",
    "    merged = merge_arrays(small_rasters)  # https://corteva.github.io/rioxarray/stable/examples/merge.html\n",
    "\n",
    "    out_file = f'merged_change_{year-5}_{year}.tif'\n",
    "\n",
    "    dask_print(f\"Saving locally {year-5}_{year} to s3\")\n",
    "\n",
    "    merged.rio.to_raster(f'/tmp/{out_file}')\n",
    "\n",
    "    # IPCC_change_dict[f\"IPCC_change_{year-5}_{year}merged\"] = [merged, \"uint8\", \"IPCC_basic_change\", f'{year-5}_{year}']  \n",
    "\n",
    "    # save_and_upload(bounds, chunk_length_pixels, tile_id, bounds_str, IPCC_change_dict)\n",
    "\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    path_and_file = f\"climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_change/{year-5}_{year}/40000_pixels/{time.strftime('%Y%m%d')}/{out_file}\"\n",
    "\n",
    "    dask_print(f\"Uploading {year-5}_{year} to s3\")\n",
    "\n",
    "    s3_client.upload_file(f'/tmp/{out_file}', \"gfw2-data\", \n",
    "                          Key = f'{path_and_file}')\n",
    "\n",
    "    dask_print(f\"Done uploading {year-5}_{year} to s3\")\n",
    "\n",
    "    os.remove(f'/tmp/{out_file}')\n",
    "\n",
    "    del merged\n",
    "\n",
    "    return f\"success for {year}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d3c3c-0cdc-4007-8697-1e4f9d46cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2005, 2021, 5))\n",
    "\n",
    "delayed_result = [dask.delayed(merge_small_tiles)(year) for year in years]\n",
    "\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fef8ac-9ecf-4745-9638-c5d53c1e541d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e1c7f-7311-4f24-83a3-e586f1b00818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa2b02-2547-4afd-9926-a973c2f2c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed, compute\n",
    "import subprocess\n",
    "\n",
    "# Define your processing function\n",
    "def process_item(year):\n",
    "\n",
    "    dask_print(year)\n",
    "\n",
    "    small_raster_paths = []\n",
    "    \n",
    "    change_folder = f\"climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_change/{year-5}_{year}/2000_pixels/{time.strftime('%Y%m%d')}/\"\n",
    "\n",
    "    # cmd = ['aws', 's3', 'ls', 's3://gfw2-data/climate/AFOLU_flux_model/LULUCF/outputs/IPCC_basic_change/2000_2005/2000_pixels/20240126/']\n",
    "    cmd = ['aws', 's3', 'ls', f's3://gfw2-data/{change_folder}']\n",
    "\n",
    "    s3_contents_bytes = subprocess.check_output(cmd)\n",
    "    s3_contents_str = s3_contents_bytes.decode('utf-8')\n",
    "    s3_contents_list = s3_contents_str.splitlines()\n",
    "    filenames = [line.split()[-1] for line in s3_contents_list]\n",
    "    \n",
    "    # dask_print(s3_contents_bytes)\n",
    "    # dask_print(s3_contents_str)\n",
    "    # dask_print(s3_contents_list)\n",
    "    # dask_print(filenames)\n",
    "\n",
    "    small_raster_paths = [f's3://gfw2-data/{change_folder}' + filename for filename in filenames]\n",
    "\n",
    "    dask_print(small_raster_paths)\n",
    "    \n",
    "   \n",
    "    # for object_summary in my_bucket.objects.filter(Prefix=change_folder):\n",
    "    #     dask_print(items)\n",
    "    #     small_raster_paths.append(object_summary.key)\n",
    "\n",
    "    # small_raster_paths = list_files(change_folder)\n",
    "    \n",
    "    # Example processing\n",
    "    return year * 2\n",
    "\n",
    "\n",
    "\n",
    "# List of items to process\n",
    "your_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "your_list = list(range(2005, 2021, 5))\n",
    "\n",
    "# Create delayed tasks\n",
    "delayed_tasks = [delayed(process_item)(year) for year in years]\n",
    "\n",
    "# Compute results in parallel\n",
    "results = compute(*delayed_tasks)\n",
    "\n",
    "# Convert tuple of results to a list\n",
    "results_list = list(results)\n",
    "\n",
    "print(results_list)  # Output: [2, 4, 6, 8, 10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
