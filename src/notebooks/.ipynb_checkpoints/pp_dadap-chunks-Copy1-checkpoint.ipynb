{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6f4aa5-ba8c-4151-bfd0-821211766aba",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Load dadap density data, resample to correct grid, and reclassify to binary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "813967a9-0f3b-4b87-9735-cc6d90ef6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask\n",
    "import boto3\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8f6faf8-1247-440b-9176-2ac564525480",
   "metadata": {},
   "outputs": [],
   "source": [
    "peatlands_uri = \"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/raw/GFW_Global_Peatlands/\"\n",
    "dadap_uri = \"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/raw/Dadap_SEA_Drainage/canal_length_data/canal_length_1km.tif\"\n",
    "s3_base_dir = \"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/\"\n",
    "dadap_pattern = \"dadap_density\"\n",
    "\n",
    "processed_dir = \"s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed\"\n",
    "output_dir = os.path.join(processed_dir,dadap_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "450f2bdd-9be1-4ebc-b1ff-da91b8471c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dadap density raster...\n",
      "Loading template raster for CRS and resolution reference...\n",
      "Reprojecting and resampling dadap density raster...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dadap_density():\n",
    "    print(\"Loading dadap density raster...\")\n",
    "    dadap_density = get_dataset(dadap_uri, \"dadap_density\", template=None)\n",
    "\n",
    "    # Load a template raster from the Hansen dataset for reference\n",
    "    print(\"Loading template raster for CRS and resolution reference...\")\n",
    "    template_raster = get_tile_dataset(peatlands_uri, \"00N_110E.tif\", \"peatlands\", template=None)\n",
    "\n",
    "    # Reproject and resample dadap_density to match template\n",
    "    print(\"Reprojecting and resampling dadap density raster...\")\n",
    "    dadap_density = dadap_density.rio.reproject_match(template_raster)\n",
    "\n",
    "    return dadap_density\n",
    "\n",
    "dadap_density = preprocess_dadap_density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc92c675-a2a9-4b7e-97fb-f41c8e8c3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_and_save_dadap_density(bounds, is_final):\n",
    "    xmin, ymin, xmax, ymax = bounds\n",
    "    bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])    # tile_id in YYN/S_XXXE/W\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees)    \n",
    "    \n",
    "    #xmin, ymin, xmax, ymax = bounds \n",
    "    local_path = \"/tmp\"  # Temporary local path for saving the file\n",
    "    output_filename = f\"{tile_id}_{dadap_pattern}.tif\" if is_final else f\"{tile_id}_{dadap_pattern}_{time.strftime('%Y%m%d%H%M%S')}.tif\"\n",
    "    local_file_path = os.path.join(local_path, output_filename)\n",
    "    output_s3_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    # Clip the raster\n",
    "    clipped = dadap_density.rio.clip_box(minx=xmin, miny=ymin, maxx=xmax, maxy=ymax)\n",
    "    \n",
    "    # Check if there is any non-nodata value in the clipped raster\n",
    "    if np.any(clipped.data != clipped.rio.nodata):\n",
    "        # Save the raster locally\n",
    "        clipped.rio.to_raster(local_file_path)\n",
    "        print(f\"Saved locally: {local_file_path}\")\n",
    "\n",
    "        # Upload to S3\n",
    "        try:\n",
    "            s3 = boto3.client('s3')\n",
    "            s3.upload_file(local_file_path, \"gfw2-data\", output_s3_path.lstrip('/'))\n",
    "            print(f\"Successfully uploaded {output_filename} to S3 at {output_s3_path}\")\n",
    "        except NoCredentialsError:\n",
    "            print(\"Credentials not available for AWS S3.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload to S3: {str(e)}\")\n",
    "\n",
    "        # Optionally, remove the local file after upload\n",
    "        os.remove(local_file_path)\n",
    "        print(f\"Deleted local file: {local_file_path}\")\n",
    "    else:\n",
    "        print(f\"No valid data in chunk {bounds} for tile {tile_id}. Skipping this chunk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f2f87-d68c-4762-b3ee-481b69eb2037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 25 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eglen_wri/anaconda3/envs/dask_notebook/lib/python3.10/site-packages/rioxarray/raster_writer.py:130: UserWarning: The nodata value (3.402823466e+38) has been automatically changed to (3.4028234663852886e+38) to match the dtype of the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid data in chunk [112, -10, 114, -8] for tile 00N_110E. Skipping this chunk.\n",
      "No valid data in chunk [114, -8, 116, -6] for tile 00N_110E. Skipping this chunk.\n",
      "No valid data in chunk [118, -8, 120, -6] for tile 00N_110E. Skipping this chunk.\n",
      "No valid data in chunk [116, -10, 118, -8] for tile 00N_110E. Skipping this chunk.\n",
      "No valid data in chunk [116, -8, 118, -6] for tile 00N_110E. Skipping this chunk.\n",
      "No valid data in chunk [112, -8, 114, -6] for tile 00N_110E. Skipping this chunk.\n",
      "No valid data in chunk [110, -8, 112, -6] for tile 00N_110E. Skipping this chunk.\n",
      "No valid data in chunk [118, -10, 120, -8] for tile 00N_110E. Skipping this chunk.\n"
     ]
    }
   ],
   "source": [
    "chunk_params = [110, -10, 120, 0, 2]  # Customize as needed\n",
    "#chunk_params = [110, -6, 112, -4, 1]\n",
    "\n",
    "# Makes list of chunks to analyze\n",
    "chunks = get_chunk_bounds(chunk_params)  \n",
    "print(\"Processing\", len(chunks), \"chunks\")\n",
    "\n",
    "# Determines if the output file names for final versions of outputs should be used\n",
    "is_final = False\n",
    "if len(chunks) > 30:\n",
    "    is_final = True\n",
    "    print(\"Running as final model.\")\n",
    "\n",
    "# Creates list of tasks to run (1 task = 1 chunk for all years)\n",
    "delayed_result = [dask.delayed(clip_and_save_dadap_density)(chunk, is_final) for chunk in chunks]\n",
    "\n",
    "# Actually runs analysis\n",
    "results = dask.compute(*delayed_result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b83583-8839-4a97-bdfb-f6906862a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert processing to Dask delayed tasks\n",
    "# tasks = [dask.delayed(process_tiles)(dadap_density, output_dir, [tile]) for tile in tiles]\n",
    "# results = dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da399b72-1316-4229-a340-974b518c6966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
