{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e36d71-c1b5-49ce-ac96-e442b07e0f6a",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to process the Dadap Canals density raster into 10x10 degree tiles and to set up a template for processing other input datasets. The code needs to check the CRS, projection, and cell size of the dataset and correct if necessary. The code also needs to export chunks of the input dataset to s3, which can later be merged into 10x10 degree tiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "813967a9-0f3b-4b87-9735-cc6d90ef6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask\n",
    "import boto3\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "\n",
    "# scipy basics\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.transform\n",
    "import rasterio.windows\n",
    "from rasterio.windows import from_bounds\n",
    "#import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "from rioxarray.merge import merge_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8f6faf8-1247-440b-9176-2ac564525480",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_uri = \"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/raw/GFW_Global_Peatlands/00N_110E.tif\"\n",
    "dadap_uri = \"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/raw/Dadap_SEA_Drainage/canal_length_data/canal_length_1km.tif\"\n",
    "s3_base_dir = \"s3://gfw2-data/climate/AFOLU_flux_model/organic_soils/\"\n",
    "dadap_pattern = \"dadap_density\"\n",
    "\n",
    "processed_dir = \"s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed\"\n",
    "output_dir = os.path.join(processed_dir,dadap_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9a99df3-d5c0-4494-b253-eee4d7842992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nodata_value(uri):\n",
    "#     with rasterio.open(uri) as src:\n",
    "#         return src.nodatavals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "450f2bdd-9be1-4ebc-b1ff-da91b8471c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_dadap_density():\n",
    "#     print(\"Loading dadap density raster...\")\n",
    "#     dadap_density = get_dataset(dadap_uri, \"dadap_density\", template=None)\n",
    "\n",
    "#     # Load a template raster from the Hansen dataset for reference\n",
    "#     print(\"Loading template raster for CRS and resolution reference...\")\n",
    "#     template_raster = get_tile_dataset(peatlands_uri, \"00N_110E.tif\", \"peatlands\", template=None)\n",
    "\n",
    "#     # Reproject and resample dadap_density to match template\n",
    "#     print(\"Reprojecting and resampling dadap density raster...\")\n",
    "#     dadap_density = dadap_density.rio.reproject_match(template_raster)\n",
    "\n",
    "#     return dadap_density\n",
    "\n",
    "# dadap_density = preprocess_dadap_density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc92c675-a2a9-4b7e-97fb-f41c8e8c3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clip_and_save_dadap_density(bounds, is_final):\n",
    "#     xmin, ymin, xmax, ymax = bounds\n",
    "#     bounds_str = boundstr(bounds)    # String form of chunk bounds\n",
    "#     tile_id = xy_to_tile_id(bounds[0], bounds[3])    # tile_id in YYN/S_XXXE/W\n",
    "#     chunk_length_pixels = calc_chunk_length_pixels(bounds)   # Chunk length in pixels (as opposed to decimal degrees)    \n",
    "    \n",
    "#     #xmin, ymin, xmax, ymax = bounds \n",
    "#     local_path = \"/tmp\"  # Temporary local path for saving the file\n",
    "#     output_filename = f\"{tile_id}_{dadap_pattern}.tif\" if is_final else f\"{tile_id}_{dadap_pattern}_{time.strftime('%Y%m%d%H%M%S')}.tif\"\n",
    "#     local_file_path = os.path.join(local_path, output_filename)\n",
    "#     output_s3_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "#     # Clip the raster \n",
    "#     # I am not clear on whether I actually need to clip the raster \n",
    "#     clipped = dadap_density.rio.clip_box(minx=xmin, miny=ymin, maxx=xmax, maxy=ymax)\n",
    "    \n",
    "#     # Check if there is any non-nodata value in the clipped raster\n",
    "#     if np.any(clipped.data != clipped.rio.nodata):\n",
    "#         # Save the raster locally\n",
    "#         clipped.rio.to_raster(local_file_path)\n",
    "#         print(f\"Saved locally: {local_file_path}\")\n",
    "\n",
    "#         # Upload to S3\n",
    "#         try:\n",
    "#             s3 = boto3.client('s3')\n",
    "#             s3.upload_file(local_file_path, \"gfw2-data\", output_s3_path.lstrip('/'))\n",
    "#             print(f\"Successfully uploaded {output_filename} to S3 at {output_s3_path}\")\n",
    "#         except NoCredentialsError:\n",
    "#             print(\"Credentials not available for AWS S3.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to upload to S3: {str(e)}\")\n",
    "\n",
    "#         # Optionally, remove the local file after upload\n",
    "#         os.remove(local_file_path)\n",
    "#         print(f\"Deleted local file: {local_file_path}\")\n",
    "#     else:\n",
    "#         print(f\"No valid data in chunk {bounds} for tile {tile_id}. Skipping this chunk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30b92832-71d2-4c94-8b30-ab57d468e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_dataset_rio(uri, bounds, chunk_length_pixels):\n",
    "    bounds_str = boundstr(bounds)\n",
    "    try:\n",
    "        with rasterio.open(uri) as ds:\n",
    "            no_data_val = ds.nodatavals[0] if ds.nodatavals[0] is not None else -9999\n",
    "            window = from_bounds(*bounds, ds.transform)\n",
    "            data = ds.read(1, window=window)\n",
    "            if data.size == 0:  # Skip chunks with no data\n",
    "                print(f\"No data in chunk {bounds_str}, skipping.\")\n",
    "                return None\n",
    "\n",
    "            transform = ds.window_transform(window)\n",
    "            data_array = xr.DataArray(data, dims=[\"y\", \"x\"], coords={\n",
    "                \"x\": np.linspace(transform.c, transform.c + transform.a * (data.shape[1] - 1), num=data.shape[1]),\n",
    "                \"y\": np.linspace(transform.f, transform.f + transform.e * (data.shape[0] - 1), num=data.shape[0])\n",
    "            })\n",
    "            data_array.rio.write_crs(ds.crs, inplace=True)\n",
    "            data_array.rio.write_nodata(no_data_val, inplace=True)\n",
    "            return data_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data for bounds {bounds_str}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5900b792-62df-4e5a-ab0c-40a1df5cb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dadap_chunk(bounds, dadap_uri, template_uri, output_dir, is_final):\n",
    "    print(f\"Processing chunk with bounds: {bounds}\")\n",
    "    chunk_length_pixels = calc_chunk_length_pixels(bounds)\n",
    "    tile_id = xy_to_tile_id(bounds[0], bounds[3])\n",
    "\n",
    "    dadap_data = get_tile_dataset_rio(dadap_uri, bounds, chunk_length_pixels)\n",
    "    if dadap_data is None:\n",
    "        return f\"Skipped chunk {bounds} due to no data\"\n",
    "\n",
    "    template_data = get_tile_dataset_rio(template_uri, bounds, chunk_length_pixels)\n",
    "    if template_data is None:\n",
    "        return f\"Skipped chunk {bounds} due to no template data\"\n",
    "\n",
    "    dadap_data_matched = dadap_data.rio.reproject_match(template_data)\n",
    "    save_and_upload_small_raster_set(bounds, chunk_length_pixels, tile_id, boundstr(bounds), {\n",
    "        \"dadap_chunk\": [dadap_data_matched, 'float32', 'dadap_density', time.strftime('%Y')]\n",
    "    }, is_final, output_dir)\n",
    "    return f\"Processed and uploaded chunk {bounds}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c59f2f87-d68c-4762-b3ee-481b69eb2037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 25 chunks\n",
      "Processing chunk with bounds: [110, -2, 112, 0]\n",
      "Processing chunk with bounds: [118, -2, 120, 0]\n",
      "Processing chunk with bounds: [114, -2, 116, 0]\n",
      "Processing chunk with bounds: [110, -10, 112, -8]\n",
      "Processing chunk with bounds: [116, -2, 118, 0]\n",
      "Processing chunk with bounds: [112, -8, 114, -6]\n",
      "Processing chunk with bounds: [118, -4, 120, -2]\n",
      "Processing chunk with bounds: [114, -10, 116, -8]\n",
      "Processing chunk with bounds: [112, -10, 114, -8]\n",
      "Processing chunk with bounds: [116, -4, 118, -2]\n",
      "Processing chunk with bounds: [118, -10, 120, -8]\n",
      "Processing chunk with bounds: [110, -4, 112, -2]\n",
      "No data in chunk 112_-8_114_-6, skipping.No data in chunk 118_-10_120_-8, skipping.\n",
      "Processing chunk with bounds: [114, -8, 116, -6]No data in chunk 110_-10_112_-8, skipping.\n",
      "No data in chunk 112_-10_114_-8, skipping.\n",
      "No data in chunk 114_-10_116_-8, skipping.\n",
      "\n",
      "\n",
      "Processing chunk with bounds: [110, -6, 112, -4]\n",
      "Processing chunk with bounds: [114, -4, 116, -2]\n",
      "Processing chunk with bounds: [110, -8, 112, -6]\n",
      "No data in chunk 114_-8_116_-6, skipping.\n",
      "Processing chunk with bounds: [114, -6, 116, -4]\n",
      "Processing chunk with bounds: [118, -8, 120, -6]\n",
      "No data in chunk 110_-8_112_-6, skipping.\n",
      "Processing chunk with bounds: [112, -2, 114, 0]\n",
      "No data in chunk 118_-8_120_-6, skipping.\n",
      "Processing chunk with bounds: [116, -10, 118, -8]\n",
      "Saving 118_-2_120_0 in 00N_110E for 2024: 20240506_11_07_48\n",
      "Saving 118_-4_120_-2 in 00N_110E for 2024: 20240506_11_07_48\n",
      "Uploading 118_-2_120_0 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_07_51\n",
      "No data in chunk 116_-10_118_-8, skipping.\n",
      "Uploading 118_-4_120_-2 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_07_52\n",
      "Processing chunk with bounds: [112, -4, 114, -2]\n",
      "Saving 110_-4_112_-2 in 00N_110E for 2024: 20240506_11_07_52\n",
      "Saving 114_-2_116_0 in 00N_110E for 2024: 20240506_11_07_52\n",
      "Saving 116_-2_118_0 in 00N_110E for 2024: 20240506_11_07_52\n",
      "Saving 116_-4_118_-2 in 00N_110E for 2024: 20240506_11_07_52\n",
      "Saving 110_-2_112_0 in 00N_110E for 2024: 20240506_11_07_52\n",
      "Uploading 110_-4_112_-2 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_07_55\n",
      "Uploading 116_-4_118_-2 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_07_57\n",
      "Uploading 114_-2_116_0 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_07_57\n",
      "Uploading 116_-2_118_0 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_07_57\n",
      "Uploading 110_-2_112_0 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_07_57\n",
      "Processing chunk with bounds: [116, -6, 118, -4]\n",
      "Saving 114_-6_116_-4 in 00N_110E for 2024: 20240506_11_07_59\n",
      "Saving 110_-6_112_-4 in 00N_110E for 2024: 20240506_11_07_59\n",
      "Processing chunk with bounds: [112, -6, 114, -4]\n",
      "Uploading 114_-6_116_-4 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_02\n",
      "Uploading 110_-6_112_-4 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_02\n",
      "Saving 114_-4_116_-2 in 00N_110E for 2024: 20240506_11_08_03\n",
      "Saving 112_-4_114_-2 in 00N_110E for 2024: 20240506_11_08_03\n",
      "Saving 112_-2_114_0 in 00N_110E for 2024: 20240506_11_08_03\n",
      "Uploading 114_-4_116_-2 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_07\n",
      "Uploading 112_-2_114_0 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_07\n",
      "Uploading 112_-4_114_-2 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_07\n",
      "Saving 116_-6_118_-4 in 00N_110E for 2024: 20240506_11_08_08\n",
      "Saving 112_-6_114_-4 in 00N_110E for 2024: 20240506_11_08_08\n",
      "Processing chunk with bounds: [116, -8, 118, -6]\n",
      "Uploading 112_-6_114_-4 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_11\n",
      "Uploading 116_-6_118_-4 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_11\n",
      "Processing chunk with bounds: [118, -6, 120, -4]\n",
      "No data in chunk 116_-8_118_-6, skipping.\n",
      "Saving 118_-6_120_-4 in 00N_110E for 2024: 20240506_11_08_13\n",
      "Uploading 118_-6_120_-4 in 00N_110E for 2024 to s3:/gfw2-data/climate/AFOLU_flux_model/organic_soils/inputs/processed/dadap_density/dadap_density/2024/8000_pixels/20240506: 20240506_11_08_14\n",
      "('Skipped chunk [110, -10, 112, -8] due to no data', 'Skipped chunk [112, -10, 114, -8] due to no data', 'Skipped chunk [114, -10, 116, -8] due to no data', 'Skipped chunk [116, -10, 118, -8] due to no data', 'Skipped chunk [118, -10, 120, -8] due to no data', 'Skipped chunk [110, -8, 112, -6] due to no data', 'Skipped chunk [112, -8, 114, -6] due to no data', 'Skipped chunk [114, -8, 116, -6] due to no data', 'Skipped chunk [116, -8, 118, -6] due to no data', 'Skipped chunk [118, -8, 120, -6] due to no data', 'Processed and uploaded chunk [110, -6, 112, -4]', 'Processed and uploaded chunk [112, -6, 114, -4]', 'Processed and uploaded chunk [114, -6, 116, -4]', 'Processed and uploaded chunk [116, -6, 118, -4]', 'Processed and uploaded chunk [118, -6, 120, -4]', 'Processed and uploaded chunk [110, -4, 112, -2]', 'Processed and uploaded chunk [112, -4, 114, -2]', 'Processed and uploaded chunk [114, -4, 116, -2]', 'Processed and uploaded chunk [116, -4, 118, -2]', 'Processed and uploaded chunk [118, -4, 120, -2]', 'Processed and uploaded chunk [110, -2, 112, 0]', 'Processed and uploaded chunk [112, -2, 114, 0]', 'Processed and uploaded chunk [114, -2, 116, 0]', 'Processed and uploaded chunk [116, -2, 118, 0]', 'Processed and uploaded chunk [118, -2, 120, 0]')\n"
     ]
    }
   ],
   "source": [
    "# Makes list of chunks to analyze\n",
    "chunk_params = [110, -10, 120, 0, 2]\n",
    "chunks = get_chunk_bounds(chunk_params)  \n",
    "print(\"Processing\", len(chunks), \"chunks\")\n",
    "\n",
    "is_final = len(chunks) > 30\n",
    "if is_final:\n",
    "    print(\"Running as final model.\")\n",
    "\n",
    "# Correct the function name and parameters\n",
    "delayed_result = [dask.delayed(process_dadap_chunk)(chunk, dadap_uri, template_uri, output_dir, is_final) for chunk in chunks]\n",
    "\n",
    "results = dask.compute(*delayed_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17b83583-8839-4a97-bdfb-f6906862a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert processing to Dask delayed tasks\n",
    "# tasks = [dask.delayed(process_tiles)(dadap_density, output_dir, [tile]) for tile in tiles]\n",
    "# results = dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da399b72-1316-4229-a340-974b518c6966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
