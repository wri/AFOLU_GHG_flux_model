{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdc63fcc-d9b3-4b6e-b7db-e8b710179f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this version works on a small set of rasters, but crashes why I try to run on global\n",
    "# import os\n",
    "# import logging\n",
    "# import boto3\n",
    "# import rioxarray\n",
    "# import rasterio\n",
    "# from rasterio.merge import merge as merge_arrays\n",
    "# import dask\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# import atexit\n",
    "\n",
    "# # Setup logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# # AWS S3 setup\n",
    "# s3_bucket = \"gfw2-data\"\n",
    "# local_temp_dir = \"/tmp/merged\"\n",
    "\n",
    "# # Global variables for Dask cluster and client\n",
    "# cluster = None\n",
    "# client = None\n",
    "\n",
    "# def s3_file_exists(bucket, key):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     try:\n",
    "#         s3.head_object(Bucket=bucket, Key=key)\n",
    "#         logging.info(f\"File exists: s3://{bucket}/{key}\")\n",
    "#         return True\n",
    "#     except:\n",
    "#         logging.info(f\"File does not exist: s3://{bucket}/{key}\")\n",
    "#         return False\n",
    "\n",
    "# def list_s3_files(bucket, prefix):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     keys = []\n",
    "#     try:\n",
    "#         paginator = s3.get_paginator('list_objects_v2')\n",
    "#         for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "#             for obj in page.get('Contents', []):\n",
    "#                 keys.append(obj['Key'])\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error listing files in s3://{bucket}/{prefix}: {e}\")\n",
    "#     return keys\n",
    "\n",
    "# def merge_tiles(tile_id, input_prefix, output_prefix, type_indicator):\n",
    "#     small_raster_paths = list_s3_files(s3_bucket, input_prefix)\n",
    "\n",
    "#     small_raster_paths = [path for path in small_raster_paths if tile_id in path]\n",
    "\n",
    "#     small_raster_paths = [f's3://{s3_bucket}/{path}' for path in small_raster_paths]\n",
    "\n",
    "#     if not small_raster_paths:\n",
    "#         logging.info(f\"No small rasters found for tile {tile_id}.\")\n",
    "#         return\n",
    "\n",
    "#     # Open rasters using rasterio directly\n",
    "#     small_rasters = [rasterio.open(path) for path in small_raster_paths]\n",
    "\n",
    "#     merged, out_transform = merge_arrays(small_rasters)\n",
    "\n",
    "#     if not os.path.exists(local_temp_dir):\n",
    "#         os.makedirs(local_temp_dir)\n",
    "\n",
    "#     out_file = f'{tile_id}_{type_indicator}.tif'\n",
    "#     local_output_path = os.path.join(local_temp_dir, out_file)\n",
    "\n",
    "#     # Copy the metadata from one of the source rasters\n",
    "#     out_meta = small_rasters[0].meta.copy()\n",
    "#     out_meta.update({\n",
    "#         \"driver\": \"GTiff\",\n",
    "#         \"height\": merged.shape[1],\n",
    "#         \"width\": merged.shape[2],\n",
    "#         \"transform\": out_transform,\n",
    "#         \"compress\": \"lzw\"\n",
    "#     })\n",
    "\n",
    "#     with rasterio.open(local_output_path, 'w', **out_meta) as dst:\n",
    "#         dst.write(merged[0], 1)  # Write the first band\n",
    "\n",
    "#     s3_client = boto3.client('s3')\n",
    "#     s3_output_path = os.path.join(output_prefix, out_file)\n",
    "#     s3_client.upload_file(local_output_path, s3_bucket, s3_output_path)\n",
    "#     logging.info(f\"Uploaded merged raster to s3://{s3_bucket}/{s3_output_path}\")\n",
    "\n",
    "#     os.remove(local_output_path)\n",
    "\n",
    "# def cleanup():\n",
    "#     global client, cluster\n",
    "#     if client:\n",
    "#         client.close()\n",
    "#     if cluster:\n",
    "#         cluster.close()\n",
    "\n",
    "# def main(input_prefixes, output_prefixes):\n",
    "#     global cluster, client\n",
    "#     cluster = LocalCluster()\n",
    "#     client = Client(cluster)\n",
    "#     atexit.register(cleanup)  # Ensure the cluster is closed when the script exits\n",
    "\n",
    "#     try:\n",
    "#         for input_prefix, output_prefix in zip(input_prefixes, output_prefixes):\n",
    "#             type_indicator = \"soil\" if \"soil\" in input_prefix.split('/') else \"state\"\n",
    "#             # Extract tile IDs from file names\n",
    "#             tile_ids = list(set([\"_\".join(os.path.basename(path).split('_')[:2]) for path in list_s3_files(s3_bucket, input_prefix)]))\n",
    "\n",
    "#             dask_tiles = [dask.delayed(merge_tiles)(tile_id, input_prefix, output_prefix, type_indicator) for tile_id in tile_ids]\n",
    "#             with ProgressBar():\n",
    "#                 dask.compute(*dask_tiles)\n",
    "#     finally:\n",
    "#         cleanup()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_prefixes = [\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/8000_pixels/20240603/',  # Replace with your input prefix for soil\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/state/2020/8000_pixels/20240603/'  # Replace with your input prefix for state\n",
    "#     ]\n",
    "#     output_prefixes = [\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/10x10_degrees/',  # Replace with your desired output prefix for soil\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/state/2020/10x10_degrees/'  # Replace with your desired output prefix for state\n",
    "#     ]\n",
    "#     main(input_prefixes, output_prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c9cef26-9594-4e5f-89ef-d41223584c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this version is working by only doing one tile id!\n",
    "# import os\n",
    "# import logging\n",
    "# import boto3\n",
    "# import rioxarray\n",
    "# import rasterio\n",
    "# from rasterio.merge import merge as merge_arrays\n",
    "# import dask\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# import atexit\n",
    "\n",
    "# # Setup logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# # AWS S3 setup\n",
    "# s3_bucket = \"gfw2-data\"\n",
    "# local_temp_dir = \"/tmp/merged\"\n",
    "\n",
    "# # Global variables for Dask cluster and client\n",
    "# cluster = None\n",
    "# client = None\n",
    "\n",
    "# def s3_file_exists(bucket, key):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     try:\n",
    "#         s3.head_object(Bucket=bucket, Key=key)\n",
    "#         logging.info(f\"File exists: s3://{bucket}/{key}\")\n",
    "#         return True\n",
    "#     except:\n",
    "#         logging.info(f\"File does not exist: s3://{bucket}/{key}\")\n",
    "#         return False\n",
    "\n",
    "# def list_s3_files(bucket, prefix):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     keys = []\n",
    "#     try:\n",
    "#         paginator = s3.get_paginator('list_objects_v2')\n",
    "#         for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "#             for obj in page.get('Contents', []):\n",
    "#                 keys.append(obj['Key'])\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error listing files in s3://{bucket}/{prefix}: {e}\")\n",
    "#     return keys\n",
    "\n",
    "# def merge_tiles(tile_id, input_prefix, output_prefix, type_indicator):\n",
    "#     small_raster_paths = list_s3_files(s3_bucket, input_prefix)\n",
    "#     small_raster_paths = [path for path in small_raster_paths if tile_id in path]\n",
    "#     small_raster_paths = [f's3://{s3_bucket}/{path}' for path in small_raster_paths]\n",
    "\n",
    "#     if not small_raster_paths:\n",
    "#         logging.info(f\"No small rasters found for tile {tile_id}.\")\n",
    "#         return\n",
    "\n",
    "#     # Open rasters using rasterio directly\n",
    "#     small_rasters = [rasterio.open(path) for path in small_raster_paths]\n",
    "\n",
    "#     merged, out_transform = merge_arrays(small_rasters)\n",
    "\n",
    "#     if not os.path.exists(local_temp_dir):\n",
    "#         os.makedirs(local_temp_dir)\n",
    "\n",
    "#     out_file = f'{tile_id}_{type_indicator}.tif'\n",
    "#     local_output_path = os.path.join(local_temp_dir, out_file)\n",
    "\n",
    "#     out_meta = small_rasters[0].meta.copy()\n",
    "#     out_meta.update({\n",
    "#         \"driver\": \"GTiff\",\n",
    "#         \"height\": merged.shape[1],\n",
    "#         \"width\": merged.shape[2],\n",
    "#         \"transform\": out_transform,\n",
    "#         \"compress\": \"lzw\"\n",
    "#     })\n",
    "\n",
    "#     with rasterio.open(local_output_path, 'w', **out_meta) as dst:\n",
    "#         dst.write(merged[0], 1)  # Write the first band\n",
    "\n",
    "#     s3_client = boto3.client('s3')\n",
    "#     s3_output_path = os.path.join(output_prefix, out_file)\n",
    "#     s3_client.upload_file(local_output_path, s3_bucket, s3_output_path)\n",
    "#     logging.info(f\"Uploaded merged raster to s3://{s3_bucket}/{s3_output_path}\")\n",
    "\n",
    "#     os.remove(local_output_path)\n",
    "\n",
    "# def cleanup():\n",
    "#     global client, cluster\n",
    "#     if client:\n",
    "#         client.close()\n",
    "#     if cluster:\n",
    "#         cluster.close()\n",
    "\n",
    "# def get_tile_ids(prefix):\n",
    "#     files = list_s3_files(s3_bucket, prefix)\n",
    "#     tile_ids = list(set([\"_\".join(os.path.basename(path).split('_')[:2]) for path in files]))\n",
    "#     return tile_ids\n",
    "\n",
    "# def process_tile_id(tile_id, input_prefix, output_prefix, type_indicator):\n",
    "#     merge_tiles(tile_id, input_prefix, output_prefix, type_indicator)\n",
    "\n",
    "# def main(input_prefixes, output_prefixes, specific_tile_id=None):\n",
    "#     # global cluster, client\n",
    "#     # cluster = LocalCluster()\n",
    "#     # client = Client(cluster)\n",
    "#     # atexit.register(cleanup)\n",
    "\n",
    "#     try:\n",
    "#         for input_prefix, output_prefix in zip(input_prefixes, output_prefixes):\n",
    "#             type_indicator = \"soil\" if \"soil\" in input_prefix.split('/') else \"state\"\n",
    "#             tile_ids = get_tile_ids(input_prefix)\n",
    "\n",
    "#             if specific_tile_id:\n",
    "#                 tile_ids = [specific_tile_id] if specific_tile_id in tile_ids else []\n",
    "\n",
    "#             for tile_id in tile_ids:\n",
    "#                 dask_tile = dask.delayed(process_tile_id)(tile_id, input_prefix, output_prefix, type_indicator)\n",
    "#                 with ProgressBar():\n",
    "#                     dask.compute(dask_tile)\n",
    "#     finally:\n",
    "#         # cleanup()\n",
    "#         print(exit)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     input_prefixes = [\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/8000_pixels/20240603/',\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/state/2020/8000_pixels/20240603/'\n",
    "#     ]\n",
    "#     output_prefixes = [\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/10x10_degrees/',\n",
    "#         'climate/AFOLU_flux_model/organic_soils/outputs/state/2020/10x10_degrees/'\n",
    "#     ]\n",
    "#     specific_tile_id = '00N_110E'  # Replace with the tile ID you want to test, or set to None to process all tiles\n",
    "#     main(input_prefixes, output_prefixes, specific_tile_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70f1d479-c39d-4056-8e7a-86052497aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 23:52:00,614 - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 04:37:25,803 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "2024-06-04 05:52:01,195 - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "import rioxarray\n",
    "import rasterio\n",
    "from rasterio.merge import merge as merge_arrays\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import atexit\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# AWS S3 setup\n",
    "s3_bucket = \"gfw2-data\"\n",
    "local_temp_dir = \"/tmp/merged\"\n",
    "\n",
    "# Global variables for Dask cluster and client\n",
    "cluster = None\n",
    "client = None\n",
    "\n",
    "def s3_file_exists(bucket, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        logging.info(f\"File exists: s3://{bucket}/{key}\")\n",
    "        return True\n",
    "    except:\n",
    "        logging.info(f\"File does not exist: s3://{bucket}/{key}\")\n",
    "        return False\n",
    "\n",
    "def list_s3_files(bucket, prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    keys = []\n",
    "    try:\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                keys.append(obj['Key'])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing files in s3://{bucket}/{prefix}: {e}\")\n",
    "    return keys\n",
    "\n",
    "def merge_tiles(tile_id, input_prefix, output_prefix, type_indicator):\n",
    "    small_raster_paths = list_s3_files(s3_bucket, input_prefix)\n",
    "    small_raster_paths = [path for path in small_raster_paths if tile_id in path]\n",
    "    small_raster_paths = [f's3://{s3_bucket}/{path}' for path in small_raster_paths]\n",
    "\n",
    "    if not small_raster_paths:\n",
    "        logging.info(f\"No small rasters found for tile {tile_id}.\")\n",
    "        return\n",
    "\n",
    "    # Open rasters using rasterio directly\n",
    "    small_rasters = [rasterio.open(path) for path in small_raster_paths]\n",
    "\n",
    "    merged, out_transform = merge_arrays(small_rasters)\n",
    "\n",
    "    if not os.path.exists(local_temp_dir):\n",
    "        os.makedirs(local_temp_dir)\n",
    "\n",
    "    out_file = f'{tile_id}_{type_indicator}.tif'\n",
    "    local_output_path = os.path.join(local_temp_dir, out_file)\n",
    "\n",
    "    out_meta = small_rasters[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": merged.shape[1],\n",
    "        \"width\": merged.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "\n",
    "    with rasterio.open(local_output_path, 'w', **out_meta) as dst:\n",
    "        dst.write(merged[0], 1)  # Write the first band\n",
    "\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_output_path = os.path.join(output_prefix, out_file)\n",
    "    s3_client.upload_file(local_output_path, s3_bucket, s3_output_path)\n",
    "    logging.info(f\"Uploaded merged raster to s3://{s3_bucket}/{s3_output_path}\")\n",
    "\n",
    "    os.remove(local_output_path)\n",
    "\n",
    "def cleanup():\n",
    "    global client, cluster\n",
    "    if client:\n",
    "        client.close()\n",
    "    if cluster:\n",
    "        cluster.close()\n",
    "\n",
    "def get_tile_ids(prefix):\n",
    "    files = list_s3_files(s3_bucket, prefix)\n",
    "    tile_ids = list(set([\"_\".join(os.path.basename(path).split('_')[:2]) for path in files]))\n",
    "    return tile_ids\n",
    "\n",
    "def process_tile_id(tile_id, input_prefix, output_prefix, type_indicator):\n",
    "    merge_tiles(tile_id, input_prefix, output_prefix, type_indicator)\n",
    "\n",
    "def main(input_prefixes, output_prefixes, tile_ids=None):\n",
    "    # global cluster, client\n",
    "    # cluster = LocalCluster()\n",
    "    # client = Client(cluster)\n",
    "    # atexit.register(cleanup)\n",
    "\n",
    "    try:\n",
    "        for input_prefix, output_prefix in zip(input_prefixes, output_prefixes):\n",
    "            type_indicator = \"soil\" if \"soil\" in input_prefix.split('/') else \"state\"\n",
    "            available_tile_ids = get_tile_ids(input_prefix)\n",
    "\n",
    "            if tile_ids:\n",
    "                # Filter the available tile IDs to only include those in the provided list\n",
    "                tile_ids_to_process = [tile_id for tile_id in tile_ids if tile_id in available_tile_ids]\n",
    "            else:\n",
    "                tile_ids_to_process = available_tile_ids\n",
    "\n",
    "            for tile_id in tile_ids_to_process:\n",
    "                dask_tile = dask.delayed(process_tile_id)(tile_id, input_prefix, output_prefix, type_indicator)\n",
    "                with ProgressBar():\n",
    "                    dask.compute(dask_tile)\n",
    "    finally:\n",
    "        print(\"exit\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    input_prefixes = [\n",
    "        'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/8000_pixels/20240603/',\n",
    "        'climate/AFOLU_flux_model/organic_soils/outputs/state/2020/8000_pixels/20240603/'\n",
    "    ]\n",
    "    output_prefixes = [\n",
    "        'climate/AFOLU_flux_model/organic_soils/outputs/soil/2020/10x10_degrees/',\n",
    "        'climate/AFOLU_flux_model/organic_soils/outputs/state/2020/10x10_degrees/'\n",
    "    ]\n",
    "    tile_ids = ['50N_070E', '70N_040E', '00N_150E', '10N_130E', '20S_140E', '00N_020E', '10S_030E', '10N_080W', '20S_080W', '40N_120E', '70N_140E', '20N_010E', '30N_110W', '40N_120W', '00N_010E', '10N_070W', '10N_000E', '20N_060W', '20S_020E', '20S_010E', '30N_020W', '60N_090W', '40N_070W', '70N_160E', '80N_090E', '60N_100E', '10N_050E', '30S_060W', '40N_060E', '70N_070E', '20N_000E', '70N_050E', '70N_010E', '10N_080E', '60N_150W', '50N_010W', '20S_130E', '30S_070W', '60N_160E', '00N_060W', '20S_050W', '30N_100W', '40N_110W', '20N_120E', '60N_140W', '60N_060E', '70N_140W', '10N_100E', '00N_040W', '10S_060W', '50N_090W', '10S_050W', '60N_030E', '00N_080W', '10N_090W', '70N_120W', '80N_140E', '60N_160W', '10S_050E', '30N_010W', '30S_010E', '40S_070W', '70N_060E', '70N_150E', '60N_080E', '20S_070W', '20N_080E', '30N_060E', '60N_060W', '50N_060W', '70N_080W', '10N_010E', '40N_000E', '70N_090E', '00N_090W', '40N_110E', '20N_110E', '60N_040E', '60N_020E', '70N_160W', '30N_120W', '20N_070W', '60N_010E', '40N_080W', '40N_100W', '00N_130E', '00N_040E', '30N_100E', '20N_080W', '00N_070W', '30S_090W', '70N_180W', '70N_130E', '60N_120E', '70N_170E', '10S_040E', '20N_090E', '70N_030E', '30N_120E', '10S_150E', '50N_130E', '40N_010E', '10N_050W', '10S_100E', '10N_020W', '20N_110W', '10N_070E', '20S_060W', '50N_110E', '60N_000E', '30N_090W', '20N_090W', '50N_000E', '60N_070E', '00N_000E', '10S_160E', '20N_020W', '50N_100E', '60N_080W', '40N_040E', '60N_170E', '70N_080E', '50N_150E', '40N_080E', '10S_080W', '40S_140E', '50N_140E', '50N_090E', '20N_070E', '70N_120E', '30S_020E', '30N_030E', '10S_140E', '80N_120W', '30N_050E', '10S_020E', '40N_010W', '10S_070W', '40N_030E', '20N_100E', '50N_030E', '70N_110E', '70N_090W', '00N_100E', '30S_170E', '50N_050E', '40S_170E', '50N_040E', '10N_120E', '40S_160E', '50N_130W', '20S_160E', '50S_080W', '80N_020E', '00N_160E', '30N_080E', '30S_140E', '50N_080W', '50N_100W', '80N_070E', '00N_110E', '40N_140E', '50N_080E', '60N_110W', '10S_130E', '20N_010W', '70N_150W', '40N_090E', '20N_020E', '60N_050E', '40N_100E', '10N_030E', '20N_040E', '80N_120E', '30N_110E', '70N_020E', '20S_110E', '30S_080W', '00N_140E', '20S_150E', '30S_030E', '40S_080W', '50N_110W', '80N_110E', '70N_010W', '10N_090E', '20N_100W', '30N_080W', '60N_130E', '10N_060W', '10N_010W', '40N_090W', '40N_130E', '10S_170E', '10S_010E', '50N_070W', '30S_150E', '50N_120E', '40N_050E', '00N_050W', '20S_040E', '10N_020E', '20N_030E', '40N_070E', '50N_010E', '40N_130W', '60N_100W', '60N_140E', '10S_040W', '40N_020E', '60N_090E', '00N_120E', '70N_100E', '10N_110E', '50N_120W', '10N_040E', '60N_150E', '60N_180W', '60N_110E', '50S_060W', '80N_060E', '60N_130W', '70N_000E', '80N_140W', '80N_130W', '30N_040E', '50N_020E', '00N_090E', '20N_120W', '60N_070W', '60N_120W', '80N_080E', '30S_130E', '70N_130W', '00N_030E', '30N_070E', '60N_020W', '20S_030E', '30N_090E', '60N_010W', '70N_030W', '70N_110W', '10N_140E', '20S_120E', '30S_110E', '20N_050E', '30S_120E', '60N_170W', '10S_110E', '50S_070W', '70N_020W', '70N_100W', '10S_120E']  # Replace with the list of tile IDs you want to process\n",
    "    main(input_prefixes, output_prefixes, tile_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f655b7c-e398-4f90-806c-2c0f76d320a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e5969-38a7-4d7a-a389-a05af0fe5338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
